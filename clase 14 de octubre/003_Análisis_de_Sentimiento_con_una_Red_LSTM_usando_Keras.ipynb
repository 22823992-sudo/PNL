{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# Análisis de Sentimiento con una Red LSTM usando Keras\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "En esta actividad vas a construir un modelo de red neuronal recurrente (RNN), específicamente una LSTM, usando la API Keras de TensorFlow. El modelo va a leer frases en español y clasificar su sentimiento como positivo o negativo.\n",
        "\n",
        "### ¿Qué vamos a lograr?\n",
        "\n",
        "- Entender cómo las redes recurrentes procesan secuencias de palabras\n",
        "- Implementar una LSTM que recuerda el contexto de la frase\n",
        "- Usar embeddings de palabras para representar el significado\n",
        "- Observar cómo las LSTM superan las limitaciones de bag-of-words\n",
        "\n",
        "### ¿Qué es una LSTM?\n",
        "\n",
        "LSTM (Long Short-Term Memory) es un tipo especial de red neuronal recurrente diseñada para:\n",
        "- **Procesar secuencias**: Lee las palabras en orden, una después de la otra\n",
        "- **Mantener memoria**: Recuerda información importante de palabras anteriores\n",
        "- **Olvidar información irrelevante**: Decide qué información del pasado mantener y qué descartar\n",
        "\n",
        "A diferencia de las MLP que vimos antes, las LSTM **sí consideran el orden** de las palabras, lo cual es fundamental para entender el lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## 1. Preparación del entorno\n",
        "\n",
        "Importamos las librerías necesarias, incluyendo herramientas de Keras para procesamiento de secuencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {
        "id": "cell-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1359fac-7ec8-4025-a0e2-fb2e6b1c81a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow/Keras versión: 3.10.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(f\"TensorFlow/Keras versión: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {
        "id": "cell-3"
      },
      "source": [
        "## 2. Datos de entrenamiento\n",
        "\n",
        "Vamos a usar las mismas frases que en la actividad anterior, pero ahora las vamos a procesar como **secuencias de palabras**, no como bolsa de palabras.\n",
        "\n",
        "Esta diferencia es fundamental: la LSTM va a poder aprovechar el orden en que aparecen las palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {
        "id": "cell-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edeae83-a3ae-4091-b85a-371ade2f1c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 10\n",
            "Balance: 5 positivas, 5 negativas\n",
            "\n",
            "Ejemplos:\n",
            "  1. 'La verdad, este lugar está bárbaro. Muy recomendab...' → Positivo\n",
            "  2. 'Una porquería de servicio, nunca más vuelvo...' → Negativo\n",
            "  3. 'Me encantó la comida, aunque la música estaba muy ...' → Positivo\n"
          ]
        }
      ],
      "source": [
        "frases = [\n",
        "    \"La verdad, este lugar está bárbaro. Muy recomendable\",\n",
        "    \"Una porquería de servicio, nunca más vuelvo\",\n",
        "    \"Me encantó la comida, aunque la música estaba muy fuerte\",\n",
        "    \"El envío fue lento y el producto llegó dañado. Qué desastre\",\n",
        "    \"Todo excelente. Atención de diez\",\n",
        "    \"Qué estafa, me arrepiento de haber comprado\",\n",
        "    \"Muy conforme con el resultado final\",\n",
        "    \"No me gustó para nada la experiencia\",\n",
        "    \"Superó mis expectativas, gracias\",\n",
        "    \"No lo recomiendo, mala calidad\"\n",
        "]\n",
        "\n",
        "etiquetas = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
        "\n",
        "print(f\"Total de frases: {len(frases)}\")\n",
        "print(f\"Balance: {sum(etiquetas)} positivas, {len(etiquetas) - sum(etiquetas)} negativas\\n\")\n",
        "print(\"Ejemplos:\")\n",
        "for i in range(3):\n",
        "    sentimiento = \"Positivo\" if etiquetas[i] == 1 else \"Negativo\"\n",
        "    print(f\"  {i+1}. '{frases[i][:50]}...' → {sentimiento}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {
        "id": "cell-5"
      },
      "source": [
        "## 3. Tokenización y construcción del vocabulario\n",
        "\n",
        "Con Keras, vamos a convertir las frases en secuencias de números, donde cada número representa una palabra del vocabulario.\n",
        "\n",
        "### Diferencia clave con bag-of-words:\n",
        "\n",
        "**Bag-of-words:**\n",
        "- \"Me gusta\" → [1, 0, 1, 0, 0] (solo presencia/ausencia)\n",
        "\n",
        "**Secuencia:**\n",
        "- \"Me gusta\" → [5, 12] (orden preservado, cada palabra tiene un ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {
        "id": "cell-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc1e2d4-2a86-48d7-eff2-777d5fdce141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario: 59 palabras únicas\n",
            "\n",
            "Primeras 15 palabras del vocabulario:\n",
            "  '<OOV>' → 1\n",
            "  'la' → 2\n",
            "  'muy' → 3\n",
            "  'de' → 4\n",
            "  'me' → 5\n",
            "  'el' → 6\n",
            "  'qué' → 7\n",
            "  'no' → 8\n",
            "  'verdad' → 9\n",
            "  'este' → 10\n",
            "  'lugar' → 11\n",
            "  'está' → 12\n",
            "  'bárbaro' → 13\n",
            "  'recomendable' → 14\n",
            "  'una' → 15\n",
            "\n",
            "Ejemplo de conversión:\n",
            "Frase original: 'La verdad, este lugar está bárbaro. Muy recomendable'\n",
            "Secuencia numérica: [2, 9, 10, 11, 12, 13, 3, 14]\n"
          ]
        }
      ],
      "source": [
        "# Tokenización: convierte palabras a números\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(frases)\n",
        "\n",
        "# Mostramos el vocabulario construido\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 por el índice 0\n",
        "print(f\"Tamaño del vocabulario: {vocab_size} palabras únicas\\n\")\n",
        "print(\"Primeras 15 palabras del vocabulario:\")\n",
        "for palabra, idx in list(tokenizer.word_index.items())[:15]:\n",
        "    print(f\"  '{palabra}' → {idx}\")\n",
        "\n",
        "# Convertimos frases a secuencias numéricas\n",
        "secuencias = tokenizer.texts_to_sequences(frases)\n",
        "\n",
        "print(f\"\\nEjemplo de conversión:\")\n",
        "print(f\"Frase original: '{frases[0]}'\")\n",
        "print(f\"Secuencia numérica: {secuencias[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## 4. Padding: estandarizando la longitud de las secuencias\n",
        "\n",
        "Las redes neuronales necesitan entradas de tamaño fijo, pero nuestras frases tienen longitudes diferentes.\n",
        "\n",
        "**Solución: Padding**\n",
        "- Rellenamos las secuencias cortas con ceros al final\n",
        "- Todas las secuencias terminan con la misma longitud\n",
        "\n",
        "Ejemplo:\n",
        "- `[5, 12]` → `[5, 12, 0, 0, 0]` (padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {
        "id": "cell-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5adad8da-e99a-44dd-9307-b2ca7facb886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud de la frase más larga: 11 palabras\n",
            "\n",
            "Forma de X después del padding: (10, 11)\n",
            "  10 frases × 11 posiciones\n",
            "\n",
            "Ejemplo de secuencia con padding:\n",
            "Frase: 'La verdad, este lugar está bárbaro. Muy recomendable'\n",
            "Secuencia: [ 2  9 10 11 12 13  3 14  0  0  0]\n",
            "Nota: Los ceros al final son padding (relleno)\n"
          ]
        }
      ],
      "source": [
        "# Calculamos la longitud máxima\n",
        "maxlen = max(len(seq) for seq in secuencias)\n",
        "print(f\"Longitud de la frase más larga: {maxlen} palabras\\n\")\n",
        "\n",
        "# Aplicamos padding\n",
        "X = pad_sequences(secuencias, maxlen=maxlen, padding='post')\n",
        "y = np.array(etiquetas)\n",
        "\n",
        "print(f\"Forma de X después del padding: {X.shape}\")\n",
        "print(f\"  {X.shape[0]} frases × {X.shape[1]} posiciones\\n\")\n",
        "\n",
        "print(\"Ejemplo de secuencia con padding:\")\n",
        "print(f\"Frase: '{frases[0]}'\")\n",
        "print(f\"Secuencia: {X[0]}\")\n",
        "print(f\"Nota: Los ceros al final son padding (relleno)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## 5. Definición del modelo LSTM\n",
        "\n",
        "Vamos a construir una red con tres componentes clave:\n",
        "\n",
        "### 1. Capa de Embedding\n",
        "Convierte cada palabra (número) en un vector denso de dimensión fija. Estos vectores se aprenden durante el entrenamiento y capturan similitudes semánticas.\n",
        "\n",
        "**Ejemplo conceptual:**\n",
        "- \"excelente\" → [0.8, 0.9, -0.1, 0.7, ...]\n",
        "- \"bueno\" → [0.7, 0.8, -0.2, 0.6, ...] (vector similar)\n",
        "- \"malo\" → [-0.7, -0.8, 0.2, -0.6, ...] (vector opuesto)\n",
        "\n",
        "### 2. Capa LSTM\n",
        "Procesa la secuencia de embeddings manteniendo memoria del contexto.\n",
        "\n",
        "### 3. Capa Dense (salida)\n",
        "Clasifica el sentimiento basándose en la representación aprendida por la LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {
        "id": "cell-10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "0f49f6f4-aecf-44ac-f1c6-c3be196d9507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquitectura del modelo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m)           │           \u001b[38;5;34m944\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │         \u001b[38;5;34m6,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">944</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,249\u001b[0m (28.32 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,249</span> (28.32 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,249\u001b[0m (28.32 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,249</span> (28.32 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parámetros totales: 7,249\n"
          ]
        }
      ],
      "source": [
        "# Parámetros del modelo\n",
        "embedding_dim = 16  # Dimensión de los vectores de embeddings\n",
        "lstm_units = 32     # Número de unidades en la capa LSTM\n",
        "\n",
        "# Construcción del modelo\n",
        "modelo = Sequential([\n",
        "    # Capa 1: Embedding - convierte palabras a vectores densos\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
        "\n",
        "    # Capa 2: LSTM - procesa la secuencia con memoria\n",
        "    LSTM(units=lstm_units),\n",
        "\n",
        "    # Capa 3: Dense - clasificación final\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilación del modelo\n",
        "modelo.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Construir el modelo explícitamente\n",
        "modelo.build(input_shape=X.shape)\n",
        "\n",
        "print(\"Arquitectura del modelo:\")\n",
        "modelo.summary()\n",
        "\n",
        "print(f\"\\nParámetros totales: {modelo.count_params():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## 6. Entrenamiento\n",
        "\n",
        "Entrenamos el modelo por varias épocas. La LSTM va a aprender:\n",
        "- Qué palabras son importantes para el sentimiento\n",
        "- Cómo el orden afecta el significado\n",
        "- Qué patrones secuenciales indican positivo o negativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e0e3a9-d689-4b95-a652-f6f93ba54b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "INICIANDO ENTRENAMIENTO\n",
            "============================================================\n",
            "Épocas: 20\n",
            "Batch size: 2\n",
            "\n",
            "Epoch 1/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0280\n",
            "Epoch 2/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0267\n",
            "Epoch 3/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0242\n",
            "Epoch 4/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0218\n",
            "Epoch 5/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0204 \n",
            "Epoch 6/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0196\n",
            "Epoch 7/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0188\n",
            "Epoch 8/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0178\n",
            "Epoch 9/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0172\n",
            "Epoch 10/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0160\n",
            "Epoch 11/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0153\n",
            "Epoch 12/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0144\n",
            "Epoch 13/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0136\n",
            "Epoch 14/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0132\n",
            "Epoch 15/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0126\n",
            "Epoch 16/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0122\n",
            "Epoch 17/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0115\n",
            "Epoch 18/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0111\n",
            "Epoch 19/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0106\n",
            "Epoch 20/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0102\n",
            "Epoch 21/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0099\n",
            "Epoch 22/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0098\n",
            "Epoch 23/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0095\n",
            "Epoch 24/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0088\n",
            "Epoch 25/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0084\n",
            "Epoch 26/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0083\n",
            "Epoch 27/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0081\n",
            "Epoch 28/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0081\n",
            "Epoch 29/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0074\n",
            "Epoch 30/30\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0074\n",
            "\n",
            "============================================================\n",
            "ENTRENAMIENTO FINALIZADO\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"INICIANDO ENTRENAMIENTO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Épocas: 20\")\n",
        "print(f\"Batch size: 2\\n\")\n",
        "\n",
        "# Entrenamiento\n",
        "history = modelo.fit(\n",
        "    X, y,\n",
        "    epochs=30,\n",
        "    batch_size=4,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENTRENAMIENTO FINALIZADO\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## 7. Análisis del entrenamiento\n",
        "\n",
        "Observá cómo evolucionan la pérdida (loss) y la precisión (accuracy) durante el entrenamiento.\n",
        "\n",
        "- **Loss decrece**: El modelo comete menos errores\n",
        "- **Accuracy aumenta**: Más predicciones correctas\n",
        "\n",
        "Si la accuracy llega a 1.0, significa que el modelo clasificó perfectamente todos los ejemplos de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {
        "id": "cell-14"
      },
      "source": [
        "## 8. Evaluación con frases nuevas\n",
        "\n",
        "Ahora vamos a probar el modelo con frases que no vio durante el entrenamiento. Esta es la verdadera prueba de si aprendió patrones generalizables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {
        "id": "cell-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716f8c5a-04d2-4f8e-8935-872858691094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EVALUACIÓN EN FRASES NUEVAS\n",
            "============================================================\n",
            "\n",
            "Frase 1: 'Muy buena atención, quedé encantado'\n",
            "  Predicción: Positivo (probabilidad: 0.99)\n",
            "  Confianza: Alta\n",
            "\n",
            "Frase 2: 'Horrible experiencia, no vuelvo más'\n",
            "  Predicción: Negativo (probabilidad: 0.01)\n",
            "  Confianza: Alta\n",
            "\n",
            "Frase 3: 'Todo excelente, gracias por la atención'\n",
            "  Predicción: Positivo (probabilidad: 0.99)\n",
            "  Confianza: Alta\n",
            "\n",
            "Frase 4: 'Me arrepiento completamente, fue un desastre'\n",
            "  Predicción: Negativo (probabilidad: 0.01)\n",
            "  Confianza: Alta\n",
            "\n",
            "Frase 5: 'horrible atención'\n",
            "  Predicción: Positivo (probabilidad: 0.99)\n",
            "  Confianza: Alta\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "frases_nuevas = [\n",
        "    \"Muy buena atención, quedé encantado\",\n",
        "    \"Horrible experiencia, no vuelvo más\",\n",
        "    \"Todo excelente, gracias por la atención\",\n",
        "    \"Me arrepiento completamente, fue un desastre\",\n",
        "    \"horrible atención\"\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EVALUACIÓN EN FRASES NUEVAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tokenizamos y aplicamos padding\n",
        "secuencias_nuevas = tokenizer.texts_to_sequences(frases_nuevas)\n",
        "X_nuevo = pad_sequences(secuencias_nuevas, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Predicción\n",
        "predicciones = modelo.predict(X_nuevo, verbose=0)\n",
        "\n",
        "# Mostrar resultados\n",
        "for i, (frase, pred) in enumerate(zip(frases_nuevas, predicciones), 1):\n",
        "    probabilidad = pred[0]\n",
        "    clase = \"Positivo\" if probabilidad >= 0.5 else \"Negativo\"\n",
        "\n",
        "    print(f\"\\nFrase {i}: '{frase}'\")\n",
        "    print(f\"  Predicción: {clase} (probabilidad: {probabilidad:.2f})\")\n",
        "\n",
        "    # Indicador de confianza\n",
        "    if probabilidad >= 0.8 or probabilidad <= 0.2:\n",
        "        print(f\"  Confianza: Alta\")\n",
        "    elif probabilidad >= 0.6 or probabilidad <= 0.4:\n",
        "        print(f\"  Confianza: Media\")\n",
        "    else:\n",
        "        print(f\"  Confianza: Baja (ambiguo)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {
        "id": "cell-16"
      },
      "source": [
        "## 9. Comparación con enfoques anteriores\n",
        "\n",
        "Recapitulemos lo que mejoramos con cada modelo:\n",
        "\n",
        "### Perceptrón simple:\n",
        "- ✗ No considera orden de palabras\n",
        "- ✗ Representación binaria (0/1)\n",
        "- ✗ Modelo lineal\n",
        "- ✓ Muy simple de entender\n",
        "\n",
        "### MLP (Red Multicapa):\n",
        "- ✗ No considera orden de palabras\n",
        "- ✗ Representación binaria (0/1)\n",
        "- ✓ Puede aprender patrones no lineales\n",
        "- ✓ Mejor capacidad de generalización\n",
        "\n",
        "### LSTM:\n",
        "- ✓ **Considera el orden de las palabras**\n",
        "- ✓ **Embeddings aprendidos** (vectores densos)\n",
        "- ✓ **Memoria del contexto** (puede recordar palabras anteriores)\n",
        "- ✓ Puede aprender patrones no lineales complejos\n",
        "\n",
        "La LSTM es un avance significativo porque finalmente podemos procesar el lenguaje como una secuencia, no como una bolsa desordenada de palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## 10. Reflexión final\n",
        "\n",
        "### ¿Qué aprendimos?\n",
        "\n",
        "1. **Procesamiento de secuencias**: Las LSTM pueden leer frases palabra por palabra, manteniendo memoria del contexto.\n",
        "\n",
        "2. **Embeddings de palabras**: En lugar de vectores binarios (0/1), cada palabra se representa con un vector denso que captura su significado.\n",
        "\n",
        "3. **Orden importa**: \"No me gusta\" y \"Me gusta, no\" ahora se procesan diferente (antes eran idénticos con bag-of-words).\n",
        "\n",
        "4. **Tokenización automática**: Keras construye el vocabulario automáticamente y maneja palabras desconocidas con `<OOV>`.\n",
        "\n",
        "### Ventajas sobre MLP con bag-of-words:\n",
        "\n",
        "- Captura el orden de las palabras\n",
        "- Aprende representaciones semánticas (embeddings)\n",
        "- Puede detectar patrones secuenciales\n",
        "- Mejor manejo de frases largas\n",
        "\n",
        "### Limitaciones que aún persisten:\n",
        "\n",
        "1. **Procesamiento secuencial**: La LSTM lee de izquierda a derecha, puede \"olvidar\" información del principio en frases muy largas\n",
        "\n",
        "2. **No puede mirar hacia adelante**: Al procesar una palabra, no sabe qué viene después\n",
        "\n",
        "3. **Dataset pequeño**: Con solo 10 ejemplos, los embeddings no se entrenan bien\n",
        "\n",
        "4. **Vocabulario limitado**: Solo conoce las palabras que aparecieron en el entrenamiento\n",
        "\n",
        "### ¿Qué sigue?\n",
        "\n",
        "En la próxima actividad vamos a ver cómo los **modelos preentrenados** como BETO (BERT en español) resuelven muchas de estas limitaciones:\n",
        "\n",
        "- Ya fueron entrenados con millones de textos\n",
        "- Tienen embeddings muy ricos\n",
        "- Usan arquitectura Transformer (no secuencial, con **atención**)\n",
        "- Pueden hacer análisis de sentimiento sin necesidad de entrenar desde cero\n",
        "\n",
        "Esto nos va a llevar al concepto de **transfer learning**, que revolucionó el NLP y es la base de los LLMs modernos como GPT."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}