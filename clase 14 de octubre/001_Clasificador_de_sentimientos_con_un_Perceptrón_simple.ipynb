{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# Perceptrón para Análisis de Sentimiento en Español\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "En esta actividad vas a implementar desde cero un modelo de Perceptrón en Python usando solo NumPy. Vamos a entrenarlo con frases breves escritas en español rioplatense para que aprenda a reconocer si una frase tiene un sentimiento positivo o negativo.\n",
        "\n",
        "El objetivo es entender cómo funciona una neurona artificial básica, cómo se ajustan sus pesos durante el aprendizaje, y cómo puede hacer predicciones en base a un conjunto pequeño de frases.\n",
        "\n",
        "### ¿Qué es un perceptrón?\n",
        "\n",
        "El perceptrón es el modelo más simple de neurona artificial. Fue propuesto en 1958 por Frank Rosenblatt y representa la base fundamental de las redes neuronales modernas.\n",
        "\n",
        "**Funcionamiento básico:**\n",
        "1. Recibe múltiples entradas (features)\n",
        "2. Multiplica cada entrada por un peso\n",
        "3. Suma todos los resultados y agrega un sesgo (bias)\n",
        "4. Aplica una función de activación para decidir la salida\n",
        "\n",
        "Es un modelo lineal que solo puede aprender patrones que sean linealmente separables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## 1. Datos de entrenamiento\n",
        "\n",
        "Vamos a usar un pequeño conjunto de frases etiquetadas como positivas (1) o negativas (0). Las frases son simples como las que podríamos encontrar en una conversación cotidiana en Argentina.\n",
        "\n",
        "También definimos un vocabulario básico de palabras clave que aparecen con frecuencia y que nos pueden ayudar a inferir el sentimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-2",
        "outputId": "a877c305-1193-494b-d52f-78453e05f500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 6\n",
            "Vocabulario: 10 palabras clave\n",
            "\n",
            "Primera frase: 'Amo el verano en Buenos Aires' → Sentimiento: Positivo\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Frases con su etiqueta de sentimiento (1 = positivo, 0 = negativo)\n",
        "frases = [\n",
        "    \"Amo el verano en Buenos Aires\",\n",
        "    \"No me gusta el tráfico matutino\",\n",
        "    \"Este asado está espectacular\",\n",
        "    \"Qué bajón, perdí el colectivo\",\n",
        "    \"Me encanta salir los domingos\",\n",
        "    \"Detesto el calor húmedo\"\n",
        "]\n",
        "\n",
        "etiquetas = np.array([1, 0, 1, 0, 1, 0])  # Etiquetas correspondientes\n",
        "\n",
        "# Vocabulario manual con palabras claves de carga emocional\n",
        "vocabulario = [\"amo\", \"no\", \"gusta\", \"asado\", \"espectacular\", \"bajón\", \"perdí\", \"detesto\", \"calor\", \"húmedo\"]\n",
        "\n",
        "print(f\"Total de frases: {len(frases)}\")\n",
        "print(f\"Vocabulario: {len(vocabulario)} palabras clave\")\n",
        "print(f\"\\nPrimera frase: '{frases[0]}' → Sentimiento: {'Positivo' if etiquetas[0] == 1 else 'Negativo'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {
        "id": "cell-3"
      },
      "source": [
        "## 2. Representación numérica: Vectorización de frases\n",
        "\n",
        "Las redes neuronales no pueden procesar texto directamente. Necesitamos convertir cada frase en un vector numérico.\n",
        "\n",
        "### Bag of Words (Bolsa de Palabras)\n",
        "\n",
        "Vamos a usar una representación simple llamada **bag of words**: para cada frase, creamos un vector binario que indica si cada palabra del vocabulario aparece (1) o no (0) en la frase.\n",
        "\n",
        "**Ejemplo:**\n",
        "- Vocabulario: [\"amo\", \"no\", \"gusta\", \"asado\"]\n",
        "- Frase: \"Amo el asado\"\n",
        "- Vector: [1, 0, 0, 1] (tiene \"amo\" y \"asado\", no tiene \"no\" ni \"gusta\")\n",
        "\n",
        "**Limitación importante:** Este método no captura el orden de las palabras ni el contexto. \"No me gusta\" y \"me gusta\" tendrían vectores muy similares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-4",
        "outputId": "2f2826ae-1648-4e68-adac-1e34f089a90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de features (X):\n",
            "Forma: (6, 10) (6 frases × 10 features)\n",
            "\n",
            "Vectores generados:\n",
            "Frase 1: [1 0 0 0 0 0 0 0 0 0] → Sentimiento: Positivo\n",
            "Frase 2: [0 1 1 0 0 0 0 0 0 0] → Sentimiento: Negativo\n",
            "Frase 3: [0 0 0 1 1 0 0 0 0 0] → Sentimiento: Positivo\n",
            "Frase 4: [0 0 0 0 0 0 1 0 0 0] → Sentimiento: Negativo\n",
            "Frase 5: [0 0 0 0 0 0 0 0 0 0] → Sentimiento: Positivo\n",
            "Frase 6: [0 0 0 0 0 0 0 1 1 1] → Sentimiento: Negativo\n"
          ]
        }
      ],
      "source": [
        "def vectorizar(frase, vocabulario):\n",
        "    \"\"\"\n",
        "    Convierte una frase en un vector binario según el vocabulario.\n",
        "\n",
        "    Args:\n",
        "        frase: String con la frase a vectorizar\n",
        "        vocabulario: Lista de palabras clave\n",
        "\n",
        "    Returns:\n",
        "        Array de numpy con 1s y 0s\n",
        "    \"\"\"\n",
        "    tokens = frase.lower().split()\n",
        "    return np.array([1 if palabra in tokens else 0 for palabra in vocabulario])\n",
        "\n",
        "# Aplicamos la función a todas las frases\n",
        "X = np.array([vectorizar(frase, vocabulario) for frase in frases])\n",
        "\n",
        "print(\"Matriz de features (X):\")\n",
        "print(f\"Forma: {X.shape} (6 frases × 10 features)\\n\")\n",
        "print(\"Vectores generados:\")\n",
        "for i, (frase, vector) in enumerate(zip(frases, X)):\n",
        "    print(f\"Frase {i+1}: {vector} → Sentimiento: {'Positivo' if etiquetas[i] == 1 else 'Negativo'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {
        "id": "cell-5"
      },
      "source": [
        "## 3. Definición del modelo: el Perceptrón\n",
        "\n",
        "Un perceptrón es una función matemática que:\n",
        "\n",
        "1. **Multiplica** cada entrada por un peso (weight)\n",
        "2. **Suma** los resultados y agrega un sesgo (bias)\n",
        "3. **Aplica** una función de activación para decidir si \"dispara\" o no\n",
        "\n",
        "### Fórmula matemática:\n",
        "\n",
        "```\n",
        "z = (x₁ × w₁) + (x₂ × w₂) + ... + (xₙ × wₙ) + bias\n",
        "salida = función_activación(z)\n",
        "```\n",
        "\n",
        "### Función de activación escalón:\n",
        "\n",
        "```\n",
        "Si z > 0 → salida = 1 (positivo)\n",
        "Si z ≤ 0 → salida = 0 (negativo)\n",
        "```\n",
        "\n",
        "Vamos a inicializar los pesos aleatoriamente y entrenar el modelo para que aprenda de los errores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-6",
        "outputId": "d61a6bd2-f96a-4f8e-8317-83e89cdd37f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos iniciales (aleatorios):\n",
            "  w[amo] = 0.497\n",
            "  w[no] = -0.138\n",
            "  w[gusta] = 0.648\n",
            "  w[asado] = 1.523\n",
            "  w[espectacular] = -0.234\n",
            "  w[bajón] = -0.234\n",
            "  w[perdí] = 1.579\n",
            "  w[detesto] = 0.767\n",
            "  w[calor] = -0.469\n",
            "  w[húmedo] = 0.543\n",
            "\n",
            "Bias inicial: 0.0\n",
            "\n",
            "Funciones definidas: activar() y predecir()\n"
          ]
        }
      ],
      "source": [
        "# Inicializamos pesos y bias con valores aleatorios pequeños\n",
        "np.random.seed(42)  # Para reproducibilidad\n",
        "pesos = np.random.randn(len(vocabulario))\n",
        "bias = 0.0\n",
        "\n",
        "print(\"Pesos iniciales (aleatorios):\")\n",
        "for i, (palabra, peso) in enumerate(zip(vocabulario, pesos)):\n",
        "    print(f\"  w[{palabra}] = {peso:.3f}\")\n",
        "print(f\"\\nBias inicial: {bias}\")\n",
        "\n",
        "def activar(suma):\n",
        "    \"\"\"\n",
        "    Función de activación escalón (step function).\n",
        "    Devuelve 1 si la suma es positiva, 0 en caso contrario.\n",
        "    \"\"\"\n",
        "    return 1 if suma > 0 else 0\n",
        "\n",
        "def predecir(x):\n",
        "    \"\"\"\n",
        "    Hace una predicción para un vector de entrada x.\n",
        "\n",
        "    Args:\n",
        "        x: Vector de features (array de numpy)\n",
        "\n",
        "    Returns:\n",
        "        Predicción: 0 o 1\n",
        "    \"\"\"\n",
        "    suma = np.dot(x, pesos) + bias\n",
        "    return activar(suma)\n",
        "\n",
        "print(\"\\nFunciones definidas: activar() y predecir()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## 4. Entrenamiento del modelo\n",
        "\n",
        "Ahora vamos a entrenar el perceptrón ajustando los pesos según los errores que comete.\n",
        "\n",
        "### Regla de aprendizaje del perceptrón:\n",
        "\n",
        "Para cada ejemplo:\n",
        "1. Calcular la predicción\n",
        "2. Calcular el error: `error = etiqueta_real - predicción`\n",
        "3. Si hay error (≠ 0), ajustar los pesos:\n",
        "   - `peso_nuevo = peso_viejo + (tasa_aprendizaje × error × entrada)`\n",
        "   - `bias_nuevo = bias_viejo + (tasa_aprendizaje × error)`\n",
        "\n",
        "### Parámetros de entrenamiento:\n",
        "\n",
        "- **Tasa de aprendizaje**: Controla qué tan grande es cada ajuste (0.1 es un valor común)\n",
        "- **Épocas**: Cantidad de veces que recorremos todo el dataset\n",
        "\n",
        "El entrenamiento se detiene cuando el modelo clasifica correctamente todos los ejemplos o llega al máximo de épocas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-8",
        "outputId": "6a41c0e9-0089-47fb-cb5d-1a4ebf6782dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "INICIANDO ENTRENAMIENTO\n",
            "============================================================\n",
            "Tasa de aprendizaje: 0.1\n",
            "Épocas máximas: 50\n",
            "Ejemplos de entrenamiento: 6\n",
            "\n",
            "Época  1: Errores = 1\n",
            "Época  2: Errores = 2\n",
            "Época  3: Errores = 2\n",
            "Época  4: Errores = 1\n",
            "Época  5: Errores = 2\n",
            "Época  6: Errores = 2\n",
            "Época  7: Errores = 0\n",
            "\n",
            "Convergencia alcanzada en época 7\n",
            "\n",
            "============================================================\n",
            "ENTRENAMIENTO FINALIZADO\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Parámetros de entrenamiento\n",
        "tasa_aprendizaje = 0.1\n",
        "epocas = 50\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"INICIANDO ENTRENAMIENTO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Tasa de aprendizaje: {tasa_aprendizaje}\")\n",
        "print(f\"Épocas máximas: {epocas}\")\n",
        "print(f\"Ejemplos de entrenamiento: {len(X)}\\n\")\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    errores = 0\n",
        "\n",
        "    # Recorremos cada ejemplo\n",
        "    for i in range(len(X)):\n",
        "        x_i = X[i]\n",
        "        y_real = etiquetas[i]\n",
        "        y_pred = predecir(x_i)\n",
        "        error = y_real - y_pred\n",
        "\n",
        "        # Si hay error, ajustamos pesos y bias\n",
        "        if error != 0:\n",
        "            pesos += tasa_aprendizaje * error * x_i\n",
        "            bias += tasa_aprendizaje * error\n",
        "            errores += 1\n",
        "\n",
        "    print(f\"Época {epoca + 1:2d}: Errores = {errores}\")\n",
        "\n",
        "    # Si no hay errores, el modelo convergió\n",
        "    if errores == 0:\n",
        "        print(f\"\\nConvergencia alcanzada en época {epoca + 1}\")\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENTRENAMIENTO FINALIZADO\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## 5. Análisis de los pesos aprendidos\n",
        "\n",
        "Después del entrenamiento, los pesos nos indican qué tan importante es cada palabra para determinar el sentimiento.\n",
        "\n",
        "- **Pesos positivos**: Palabras asociadas a sentimiento positivo\n",
        "- **Pesos negativos**: Palabras asociadas a sentimiento negativo\n",
        "- **Pesos cercanos a 0**: Palabras poco informativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-10",
        "outputId": "39ba0220-b3c0-4cae-f564-fc31b06ce2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos aprendidos después del entrenamiento:\n",
            "\n",
            "Palabra               Peso Interpretación\n",
            "--------------------------------------------------\n",
            "asado                1.523  → Positivo\n",
            "amo                  0.497  → Positivo\n",
            "detesto              0.367  → Positivo\n",
            "gusta                0.248  → Positivo\n",
            "húmedo               0.143  → Positivo\n",
            "perdí               -0.121  → Negativo\n",
            "bajón               -0.234  → Negativo\n",
            "espectacular        -0.234  → Negativo\n",
            "no                  -0.538  → Negativo\n",
            "calor               -0.869  → Negativo\n",
            "\n",
            "Bias final: 0.100\n",
            "\n",
            "El bias representa el umbral base del modelo.\n"
          ]
        }
      ],
      "source": [
        "print(\"Pesos aprendidos después del entrenamiento:\\n\")\n",
        "print(f\"{'Palabra':<15} {'Peso':>10} {'Interpretación'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for palabra, peso in sorted(zip(vocabulario, pesos), key=lambda x: x[1], reverse=True):\n",
        "    if peso > 0.1:\n",
        "        interpretacion = \"→ Positivo\"\n",
        "    elif peso < -0.1:\n",
        "        interpretacion = \"→ Negativo\"\n",
        "    else:\n",
        "        interpretacion = \"→ Neutral\"\n",
        "\n",
        "    print(f\"{palabra:<15} {peso:>10.3f}  {interpretacion}\")\n",
        "\n",
        "print(f\"\\nBias final: {bias:.3f}\")\n",
        "print(\"\\nEl bias representa el umbral base del modelo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## 6. Prueba con nuevas frases\n",
        "\n",
        "Ahora vamos a ver cómo se comporta nuestro perceptrón con frases nuevas que no vio durante el entrenamiento.\n",
        "\n",
        "Esta es la verdadera prueba: ¿puede generalizar lo que aprendió a casos nuevos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-12",
        "outputId": "04d9dd50-3d5c-4166-bd31-40334e13c6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PREDICCIONES EN FRASES NUEVAS\n",
            "============================================================\n",
            "\n",
            "Frase 1: 'No aguanto este calor'\n",
            "  Vector: [0 1 0 0 0 0 0 0 1 0]\n",
            "  Predicción: Negativo\n",
            "  Palabras clave detectadas: no, calor\n",
            "\n",
            "Frase 2: 'Qué hermoso día para pasear'\n",
            "  Vector: [0 0 0 0 0 0 0 0 0 0]\n",
            "  Predicción: Positivo\n",
            "  No se detectaron palabras del vocabulario\n",
            "\n",
            "Frase 3: 'Detesto levantarme temprano'\n",
            "  Vector: [0 0 0 0 0 0 0 1 0 0]\n",
            "  Predicción: Positivo\n",
            "  Palabras clave detectadas: detesto\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Frases nuevas para testeo\n",
        "frases_prueba = [\n",
        "    \"No aguanto este calor\",\n",
        "    \"Qué hermoso día para pasear\",\n",
        "    \"Detesto levantarme temprano\"\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PREDICCIONES EN FRASES NUEVAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Vectorizamos las frases nuevas\n",
        "X_prueba = np.array([vectorizar(frase, vocabulario) for frase in frases_prueba])\n",
        "predicciones = [predecir(x) for x in X_prueba]\n",
        "\n",
        "# Mostramos los resultados\n",
        "for i, (frase, pred, vector) in enumerate(zip(frases_prueba, predicciones, X_prueba), 1):\n",
        "    resultado = \"Positivo\" if pred == 1 else \"Negativo\"\n",
        "    print(f\"\\nFrase {i}: '{frase}'\")\n",
        "    print(f\"  Vector: {vector}\")\n",
        "    print(f\"  Predicción: {resultado}\")\n",
        "\n",
        "    # Mostramos qué palabras del vocabulario detectó\n",
        "    palabras_detectadas = [vocabulario[j] for j in range(len(vocabulario)) if vector[j] == 1]\n",
        "    if palabras_detectadas:\n",
        "        print(f\"  Palabras clave detectadas: {', '.join(palabras_detectadas)}\")\n",
        "    else:\n",
        "        print(f\"  No se detectaron palabras del vocabulario\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## 7. Reflexión final\n",
        "\n",
        "### ¿Qué aprendimos?\n",
        "\n",
        "1. **Funcionamiento de una neurona artificial básica**: El perceptrón es el bloque fundamental de las redes neuronales. Aprendimos cómo combina entradas ponderadas y aplica una función de activación.\n",
        "\n",
        "2. **Proceso de entrenamiento**: Vimos cómo un modelo aprende ajustando sus pesos iterativamente basándose en los errores que comete. Este principio se extiende a redes neuronales más complejas.\n",
        "\n",
        "3. **Representación de texto**: Usamos bag-of-words, una técnica simple pero efectiva para convertir texto en números que las máquinas pueden procesar.\n",
        "\n",
        "### Limitaciones observadas\n",
        "\n",
        "1. **No considera el orden**: \"No me gusta\" vs \"Me gusta, no\" se representan igual\n",
        "2. **Vocabulario limitado**: Solo conoce las palabras que definimos manualmente\n",
        "3. **Modelo lineal**: Solo puede aprender patrones linealmente separables\n",
        "4. **Sin contexto**: No entiende sarcasmo, ironía o matices del lenguaje\n",
        "5. **Dataset pequeño**: Con solo 6 ejemplos, la generalización es limitada\n",
        "\n",
        "### ¿Qué sigue?\n",
        "\n",
        "En el próximo laboratorio, vamos a ver cómo las **redes neuronales multicapa** (MLP) pueden capturar patrones más complejos usando múltiples perceptrones organizados en capas. Esto nos va a permitir:\n",
        "\n",
        "- Aprender representaciones no lineales\n",
        "- Capturar interacciones entre features\n",
        "- Mejorar la capacidad de generalización\n",
        "\n",
        "Más adelante veremos cómo las **redes recurrentes** (LSTM) pueden procesar el orden de las palabras y, finalmente, cómo los **Transformers** revolucionaron el procesamiento de lenguaje natural."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}