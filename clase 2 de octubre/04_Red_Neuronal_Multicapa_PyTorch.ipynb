{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0ts5be-b31n"
      },
      "source": [
        "# Red Neuronal Multicapa (MLP) con PyTorch\n",
        "\n",
        "**Materiales desarrollados por Matías Barreto, 2025**\n",
        "\n",
        "**Tecnicatura en Ciencia de Datos - IFTS**\n",
        "\n",
        "**Asignatura:** Procesamiento de Lenguaje Natural\n",
        "\n",
        "---\n",
        "\n",
        "## Introducción\n",
        "\n",
        "En el notebook anterior implementamos un perceptrón simple desde cero y descubrimos sus limitaciones: solo puede resolver problemas linealmente separables. Ahora damos el salto hacia las **redes neuronales multicapa** (MLP - Multilayer Perceptron), que superan estas restricciones.\n",
        "\n",
        "Además, dejaremos de programar todo manualmente y usaremos **PyTorch**, el framework de deep learning más usado en investigación y cada vez más en la industria. PyTorch es la base de HuggingFace Transformers, que usaremos en las próximas semanas.\n",
        "\n",
        "### ¿Qué cambia con capas ocultas?\n",
        "\n",
        "Agregar capas intermedias entre entrada y salida permite:\n",
        "1. **Resolver problemas no lineales** (como XOR)\n",
        "2. **Aprender representaciones jerárquicas** (features abstractas)\n",
        "3. **Capturar interacciones complejas** entre palabras\n",
        "4. **Aproximar funciones arbitrarias** (teorema de aproximación universal)\n",
        "\n",
        "### Objetivos de aprendizaje\n",
        "\n",
        "1. Comprender la arquitectura de redes feedforward multicapa\n",
        "2. Dominar los fundamentos de PyTorch: tensores, autograd, nn.Module\n",
        "3. Implementar una red MLP para clasificación de texto\n",
        "4. Usar funciones de activación modernas (ReLU, Sigmoid)\n",
        "5. Entrenar con backpropagation automática\n",
        "6. Comparar rendimiento con el perceptrón simple\n",
        "\n",
        "### ¿Por qué PyTorch?\n",
        "\n",
        "**Ventajas:**\n",
        "- Autograd: Calcula gradientes automáticamente\n",
        "- Sintaxis pythónica e intuitiva\n",
        "- Debugging fácil (ejecución eager)\n",
        "- Ecosistema rico (HuggingFace, torchvision, etc.)\n",
        "- GPU acceleration out-of-the-box\n",
        "- Estándar en investigación académica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1xChsXkb31u"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Instalación e Importación de PyTorch\n",
        "\n",
        "En Google Colab, PyTorch ya viene instalado. Para instalación local, visitar: https://pytorch.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww77K-M_b31w",
        "outputId": "5b6c5572-46c8-465a-8010-760dbb0c2591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch versión: 2.8.0+cu126\n",
            "CUDA disponible: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Si necesitás instalar PyTorch (descomentá la línea siguiente)\n",
        "# !pip install torch\n",
        "\n",
        "# PyTorch: Framework de deep learning\n",
        "import torch\n",
        "# torch.nn: Módulo con capas y modelos de redes neuronales\n",
        "import torch.nn as nn\n",
        "# torch.optim: Optimizadores (SGD, Adam, etc.)\n",
        "import torch.optim as optim\n",
        "\n",
        "# NumPy: Para operaciones numéricas complementarias\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib: Para visualizaciones\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fijamos semillas para reproducibilidad\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch versión: {torch.__version__}\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Ejecutando en CPU (suficiente para este notebook)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8cKXZob31z"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Dataset: Análisis de Sentimiento en Español Rioplatense\n",
        "\n",
        "Usamos un corpus más grande que en el perceptrón simple para aprovechar la capacidad de la red multicapa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNueVHjIb31z",
        "outputId": "5f06d060-b661-45e4-d3b0-56fa7e2d378e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus de entrenamiento: 20 frases\n",
            "Distribución: [10 10] (negativas, positivas)\n",
            "\n",
            "Primeras 3 frases positivas:\n",
            "  1. La verdad, este lugar está bárbaro. Muy recomendable.\n",
            "  2. Qué buena onda la atención, volvería sin dudarlo.\n",
            "  3. Me encantó la comida, aunque la música estaba muy fuerte.\n",
            "\n",
            "Primeras 3 frases negativas:\n",
            "  1. Una porquería de servicio, nunca más vuelvo.\n",
            "  2. El envío fue lento y el producto llegó dañado. Qué desastre.\n",
            "  3. Qué estafa, me arrepiento de haber comprado.\n"
          ]
        }
      ],
      "source": [
        "# Corpus ampliado de reseñas en español rioplatense\n",
        "# Etiqueta: 1 = Positivo, 0 = Negativo\n",
        "frases = [\n",
        "    # Positivas\n",
        "    \"La verdad, este lugar está bárbaro. Muy recomendable.\",\n",
        "    \"Qué buena onda la atención, volvería sin dudarlo.\",\n",
        "    \"Me encantó la comida, aunque la música estaba muy fuerte.\",\n",
        "    \"Todo excelente. Atención de diez.\",\n",
        "    \"Muy conforme con el resultado final.\",\n",
        "    \"Superó mis expectativas, gracias.\",\n",
        "    \"El mejor asado que probé en mucho tiempo.\",\n",
        "    \"Excelente relación precio-calidad, muy recomendable.\",\n",
        "    \"La atención fue impecable, muy atentos.\",\n",
        "    \"Me gustó mucho el ambiente tranquilo.\",\n",
        "\n",
        "    # Negativas\n",
        "    \"Una porquería de servicio, nunca más vuelvo.\",\n",
        "    \"El envío fue lento y el producto llegó dañado. Qué desastre.\",\n",
        "    \"Qué estafa, me arrepiento de haber comprado.\",\n",
        "    \"No me gustó para nada la experiencia.\",\n",
        "    \"No lo recomiendo, mala calidad.\",\n",
        "    \"Malísima atención, el mozo tenía mala onda.\",\n",
        "    \"Tardaron dos horas en entregar, llegó todo frío.\",\n",
        "    \"Me cobraron de más y encima se hicieron los giles.\",\n",
        "    \"La carne estaba pasada, casi no se podía comer.\",\n",
        "    \"Pésima experiencia, no vuelvo más.\"\n",
        "]\n",
        "\n",
        "# Etiquetas: 1=positivo, 0=negativo\n",
        "etiquetas = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # 10 positivas\n",
        "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # 10 negativas\n",
        "\n",
        "print(f\"Corpus de entrenamiento: {len(frases)} frases\")\n",
        "print(f\"Distribución: {np.bincount(etiquetas)} (negativas, positivas)\")\n",
        "print(f\"\\nPrimeras 3 frases positivas:\")\n",
        "for i in range(3):\n",
        "    print(f\"  {i+1}. {frases[i]}\")\n",
        "print(f\"\\nPrimeras 3 frases negativas:\")\n",
        "for i in range(10, 13):\n",
        "    print(f\"  {i-9}. {frases[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRjcXZlgb311"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Vocabulario y Vectorización\n",
        "\n",
        "Construimos un vocabulario extendido con palabras clave del corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQh7Ei6ib313",
        "outputId": "b1587765-249d-4b07-b45d-723ec207bab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario: 30 palabras\n",
            "['bárbaro', 'recomendable', 'buena', 'onda', 'encantó', 'excelente', 'conforme', 'superó', 'expectativas', 'mejor', 'impecable', 'gustó', 'tranquilo', 'porquería', 'nunca', 'desastre', 'estafa', 'arrepiento', 'no', 'mala', 'malísima', 'pésima', 'tardaron', 'frío', 'pasada', 'lugar', 'atención', 'comida', 'servicio', 'experiencia']\n"
          ]
        }
      ],
      "source": [
        "# Vocabulario manual con palabras discriminativas\n",
        "vocabulario = [\n",
        "    # Positivas\n",
        "    \"bárbaro\", \"recomendable\", \"buena\", \"onda\", \"encantó\",\n",
        "    \"excelente\", \"conforme\", \"superó\", \"expectativas\", \"mejor\",\n",
        "    \"impecable\", \"gustó\", \"tranquilo\",\n",
        "\n",
        "    # Negativas\n",
        "    \"porquería\", \"nunca\", \"desastre\", \"estafa\", \"arrepiento\",\n",
        "    \"no\", \"mala\", \"malísima\", \"pésima\", \"tardaron\",\n",
        "    \"frío\", \"pasada\",\n",
        "\n",
        "    # Neutras/contextuales\n",
        "    \"lugar\", \"atención\", \"comida\", \"servicio\", \"experiencia\"\n",
        "]\n",
        "\n",
        "print(f\"Vocabulario: {len(vocabulario)} palabras\")\n",
        "print(vocabulario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxokxNNhb315",
        "outputId": "36a264d2-658b-4226-e7a1-35f753673d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matriz de características:\n",
            "Forma X: (20, 30) (muestras x features)\n",
            "Forma y: (20, 1)\n",
            "\n",
            "Primera frase vectorizada:\n",
            "Frase: 'La verdad, este lugar está bárbaro. Muy recomendable.'\n",
            "Vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0.]\n",
            "Palabras detectadas: ['lugar']\n"
          ]
        }
      ],
      "source": [
        "def vectorizar(frase, vocabulario):\n",
        "    \"\"\"\n",
        "    Convierte una frase en vector binario según el vocabulario.\n",
        "\n",
        "    Args:\n",
        "        frase (str): Texto a vectorizar\n",
        "        vocabulario (list): Lista de palabras clave\n",
        "\n",
        "    Returns:\n",
        "        np.array: Vector binario (1 si palabra presente, 0 si no)\n",
        "    \"\"\"\n",
        "    tokens = frase.lower().split()\n",
        "    return np.array([1 if palabra in tokens else 0 for palabra in vocabulario], dtype=np.float32)\n",
        "\n",
        "\n",
        "# Vectorizamos todas las frases\n",
        "X_np = np.array([vectorizar(frase, vocabulario) for frase in frases], dtype=np.float32)\n",
        "y_np = etiquetas.astype(np.float32).reshape(-1, 1)  # Shape (n, 1) para BCELoss\n",
        "\n",
        "print(f\"\\nMatriz de características:\")\n",
        "print(f\"Forma X: {X_np.shape} (muestras x features)\")\n",
        "print(f\"Forma y: {y_np.shape}\")\n",
        "print(f\"\\nPrimera frase vectorizada:\")\n",
        "print(f\"Frase: '{frases[0]}'\")\n",
        "print(f\"Vector: {X_np[0]}\")\n",
        "palabras_presentes = [vocabulario[i] for i in range(len(vocabulario)) if X_np[0][i] == 1]\n",
        "print(f\"Palabras detectadas: {palabras_presentes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adSxpdhob317"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Introducción a Tensores en PyTorch\n",
        "\n",
        "Los **tensores** son el equivalente de NumPy arrays en PyTorch, pero con capacidades adicionales:\n",
        "- Pueden ejecutarse en GPU\n",
        "- Registran operaciones para backpropagation automática (autograd)\n",
        "- Integración nativa con redes neuronales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi-QE60-b318",
        "outputId": "4bd0dc1c-3085-40ef-9b44-3819002e5261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Información de los tensores:\n",
            "======================================================================\n",
            "X.shape: torch.Size([20, 30])\n",
            "X.dtype: torch.float32\n",
            "X.device: cpu  # cpu o cuda\n",
            "X.requires_grad: False  # False porque son datos, no parámetros\n",
            "\n",
            "y.shape: torch.Size([20, 1])\n",
            "y.dtype: torch.float32\n",
            "\n",
            "======================================================================\n",
            "Operaciones con tensores:\n",
            "----------------------------------------------------------------------\n",
            "Media de X: 0.0400\n",
            "Desviación estándar de X: 0.1961\n",
            "Suma de y: 10\n"
          ]
        }
      ],
      "source": [
        "# Convertimos NumPy arrays a tensores de PyTorch\n",
        "# torch.tensor() crea un nuevo tensor copiando los datos\n",
        "X = torch.tensor(X_np)\n",
        "y = torch.tensor(y_np)\n",
        "\n",
        "print(\"Información de los tensores:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"X.shape: {X.shape}\")\n",
        "print(f\"X.dtype: {X.dtype}\")\n",
        "print(f\"X.device: {X.device}  # cpu o cuda\")\n",
        "print(f\"X.requires_grad: {X.requires_grad}  # False porque son datos, no parámetros\")\n",
        "print()\n",
        "print(f\"y.shape: {y.shape}\")\n",
        "print(f\"y.dtype: {y.dtype}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Operaciones con tensores:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Operaciones básicas (similares a NumPy)\n",
        "print(f\"Media de X: {X.mean().item():.4f}\")\n",
        "print(f\"Desviación estándar de X: {X.std().item():.4f}\")\n",
        "print(f\"Suma de y: {y.sum().item():.0f}\")\n",
        "\n",
        "# .item() extrae el valor de un tensor de un solo elemento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0oSEgpkb319"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Arquitectura de una Red Multicapa (MLP)\n",
        "\n",
        "Una red feedforward multicapa consta de:\n",
        "\n",
        "```\n",
        "INPUT → [CAPA OCULTA 1] → [CAPA OCULTA 2] → ... → OUTPUT\n",
        "```\n",
        "\n",
        "### Arquitectura que vamos a implementar:\n",
        "\n",
        "```\n",
        "Input Layer          Hidden Layer        Output Layer\n",
        "(30 features)        (8 neuronas)        (1 neurona)\n",
        "\n",
        "x₁ ─┐\n",
        "x₂ ─┤               h₁ ─┐\n",
        "x₃ ─┤──────────────>h₂ ─┤\n",
        "... │    ReLU        h₃ ─┤──────────────> y\n",
        "x₃₀─┘               h₄ ─┤    Sigmoid\n",
        "                    ... ─┘\n",
        "                    h₈\n",
        "```\n",
        "\n",
        "### Componentes:\n",
        "\n",
        "1. **Capa de entrada**: 30 features (tamaño del vocabulario)\n",
        "2. **Capa oculta**: 8 neuronas con activación ReLU\n",
        "3. **Capa de salida**: 1 neurona con activación Sigmoid (probabilidad 0-1)\n",
        "\n",
        "### ¿Por qué ReLU?\n",
        "\n",
        "**ReLU (Rectified Linear Unit):**\n",
        "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "**Ventajas sobre escalón:**\n",
        "- Diferenciable en casi todos lados (excepto x=0)\n",
        "- No sufre vanishing gradient\n",
        "- Computacionalmente eficiente\n",
        "- Introduce no linealidad necesaria para problemas complejos\n",
        "\n",
        "### ¿Por qué Sigmoid en la salida?\n",
        "\n",
        "**Sigmoid:**\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "**Ventajas:**\n",
        "- Salida en rango [0, 1]: interpretable como probabilidad\n",
        "- Diferenciable en todos lados\n",
        "- Compatible con Binary Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt0GUs6tb32C"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Definición del Modelo con nn.Module\n",
        "\n",
        "En PyTorch, todos los modelos heredan de `nn.Module`. Esto nos da:\n",
        "- Gestión automática de parámetros\n",
        "- Métodos para entrenamiento/evaluación\n",
        "- Compatibilidad con optimizadores\n",
        "- Serialización fácil (guardar/cargar modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnVQG76Kb32C",
        "outputId": "c62070bc-5017-45a6-9ab3-f92d1b3b04e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo MLP creado:\n",
            "======================================================================\n",
            "MLP(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=30, out_features=8, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n",
            "\n",
            "======================================================================\n",
            "Parámetros del modelo:\n",
            "----------------------------------------------------------------------\n",
            "Total de parámetros entrenables: 257\n",
            "  net.0.weight         → shape: [8, 30]              |  240 params\n",
            "  net.0.bias           → shape: [8]                  |    8 params\n",
            "  net.2.weight         → shape: [1, 8]               |    8 params\n",
            "  net.2.bias           → shape: [1]                  |    1 params\n",
            "\n",
            "Nota: Los parámetros se inicializan aleatoriamente.\n",
            "El entrenamiento ajustará estos valores para minimizar el error.\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Perceptrón Multicapa para clasificación binaria de sentimientos.\n",
        "\n",
        "    Arquitectura:\n",
        "        Input (input_size) → Hidden (hidden_size) → Output (1)\n",
        "                              ReLU                   Sigmoid\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"\n",
        "        Constructor del modelo.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Número de features de entrada\n",
        "            hidden_size (int): Número de neuronas en la capa oculta\n",
        "        \"\"\"\n",
        "        # Llamamos al constructor de la clase padre (nn.Module)\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        # Definimos las capas de la red usando nn.Sequential\n",
        "        # Sequential encadena capas: la salida de una es entrada de la siguiente\n",
        "        self.net = nn.Sequential(\n",
        "            # Capa 1: Linear (fully connected) de input_size a hidden_size\n",
        "            # Hace: y = Wx + b, donde W shape (hidden_size, input_size)\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "\n",
        "            # Activación ReLU: max(0, x)\n",
        "            # Introduce no linealidad, permite aprender patrones complejos\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Capa 2: Linear de hidden_size a 1 (salida)\n",
        "            nn.Linear(hidden_size, 1),\n",
        "\n",
        "            # Activación Sigmoid: 1 / (1 + e^-x)\n",
        "            # Convierte salida a rango [0, 1] (probabilidad)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: define cómo los datos fluyen por la red.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input de shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output de shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Pasamos el input por todas las capas secuencialmente\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# Hiperparámetros de la arquitectura\n",
        "input_size = len(vocabulario)  # 30 features (tamaño del vocabulario)\n",
        "hidden_size = 8                # 8 neuronas en la capa oculta\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = MLP(input_size, hidden_size)\n",
        "\n",
        "print(\"Modelo MLP creado:\")\n",
        "print(\"=\" * 70)\n",
        "print(modelo)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Parámetros del modelo:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Contamos parámetros entrenables\n",
        "total_params = sum(p.numel() for p in modelo.parameters() if p.requires_grad)\n",
        "print(f\"Total de parámetros entrenables: {total_params}\")\n",
        "\n",
        "# Detallamos cada capa\n",
        "for name, param in modelo.named_parameters():\n",
        "    print(f\"  {name:20s} → shape: {str(list(param.shape)):20s} | {param.numel():4d} params\")\n",
        "\n",
        "print(\"\\nNota: Los parámetros se inicializan aleatoriamente.\")\n",
        "print(\"El entrenamiento ajustará estos valores para minimizar el error.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94J_Q-lIb32D"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Función de Pérdida (Loss Function)\n",
        "\n",
        "Para clasificación binaria, usamos **Binary Cross Entropy (BCE)**:\n",
        "\n",
        "$$\\text{BCE} = -\\frac{1}{n}\\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
        "\n",
        "Donde:\n",
        "- $y_i$: Etiqueta real (0 o 1)\n",
        "- $\\hat{y}_i$: Probabilidad predicha por el modelo\n",
        "\n",
        "### Intuición:\n",
        "\n",
        "- Si $y=1$ y $\\hat{y}=0.9$: Pérdida baja (buena predicción)\n",
        "- Si $y=1$ y $\\hat{y}=0.1$: Pérdida alta (mala predicción)\n",
        "- La función es convexa: garantiza convergencia a un mínimo\n",
        "\n",
        "### ¿Por qué no MSE (Mean Squared Error)?\n",
        "\n",
        "MSE funciona para regresión, pero para clasificación:\n",
        "- BCE tiene mejores propiedades de gradiente\n",
        "- Penaliza predicciones muy erróneas más fuerte\n",
        "- Es la elección estándar para problemas binarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jiCKy7ob32D",
        "outputId": "eabf1bd7-045c-4b55-9411-745cecd8935d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función de pérdida: Binary Cross Entropy (BCE)\n",
            "======================================================================\n",
            "BCELoss()\n",
            "\n",
            "Ejemplos de pérdida:\n",
            "----------------------------------------------------------------------\n",
            "Buena predicción: y=1, pred=0.9          → Loss: 0.1054\n",
            "Predicción incierta: y=1, pred=0.5       → Loss: 0.6931\n",
            "Mala predicción: y=1, pred=0.1           → Loss: 2.3026\n",
            "Buena predicción: y=0, pred=0.1          → Loss: 0.1054\n",
            "Mala predicción: y=0, pred=0.9           → Loss: 2.3026\n",
            "\n",
            "Observación: A mayor error, mayor pérdida (penalización).\n"
          ]
        }
      ],
      "source": [
        "# Definimos la función de pérdida\n",
        "# BCELoss: Binary Cross Entropy para clasificación binaria\n",
        "criterio = nn.BCELoss()\n",
        "\n",
        "print(\"Función de pérdida: Binary Cross Entropy (BCE)\")\n",
        "print(\"=\" * 70)\n",
        "print(criterio)\n",
        "\n",
        "# Ejemplo de cómo funciona BCE\n",
        "print(\"\\nEjemplos de pérdida:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Casos de ejemplo\n",
        "casos = [\n",
        "    (1.0, 0.9, \"Buena predicción: y=1, pred=0.9\"),\n",
        "    (1.0, 0.5, \"Predicción incierta: y=1, pred=0.5\"),\n",
        "    (1.0, 0.1, \"Mala predicción: y=1, pred=0.1\"),\n",
        "    (0.0, 0.1, \"Buena predicción: y=0, pred=0.1\"),\n",
        "    (0.0, 0.9, \"Mala predicción: y=0, pred=0.9\"),\n",
        "]\n",
        "\n",
        "for y_true, y_pred, descripcion in casos:\n",
        "    y_tensor = torch.tensor([[y_true]])\n",
        "    pred_tensor = torch.tensor([[y_pred]])\n",
        "    loss = criterio(pred_tensor, y_tensor)\n",
        "    print(f\"{descripcion:40s} → Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nObservación: A mayor error, mayor pérdida (penalización).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLZ9WM2Vb32E"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Optimizador: Adam\n",
        "\n",
        "El optimizador actualiza los parámetros del modelo basándose en los gradientes calculados por backpropagation.\n",
        "\n",
        "### Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Es el optimizador más usado en deep learning moderno porque:\n",
        "1. **Adaptativo**: Ajusta learning rate por parámetro\n",
        "2. **Momento**: Usa promedios móviles de gradientes\n",
        "3. **Robusto**: Funciona bien con hiperparámetros por defecto\n",
        "4. **Eficiente**: Converge rápido\n",
        "\n",
        "### Otros optimizadores comunes:\n",
        "\n",
        "- **SGD (Stochastic Gradient Descent)**: Básico pero efectivo\n",
        "- **RMSprop**: Precursor de Adam\n",
        "- **AdaGrad**: Para sparse data\n",
        "\n",
        "### Learning Rate\n",
        "\n",
        "El parámetro `lr` (learning rate) controla el tamaño del paso:\n",
        "- Típicamente: 0.001 - 0.01 para Adam\n",
        "- Muy alto: Divergencia, no converge\n",
        "- Muy bajo: Convergencia lenta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-xJ-6RIb32E",
        "outputId": "e13545fe-ed06-445f-ed5d-b926ed15a056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizador configurado:\n",
            "======================================================================\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "Parámetros que el optimizador va a actualizar:\n",
            "----------------------------------------------------------------------\n",
            "Grupo 0: 4 tensores\n",
            "  Learning rate: 0.01\n"
          ]
        }
      ],
      "source": [
        "# Definimos el optimizador Adam\n",
        "# modelo.parameters() retorna todos los parámetros entrenables (pesos y biases)\n",
        "# lr: learning rate (tasa de aprendizaje)\n",
        "optimizador = optim.Adam(modelo.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Optimizador configurado:\")\n",
        "print(\"=\" * 70)\n",
        "print(optimizador)\n",
        "print(\"\\nParámetros que el optimizador va a actualizar:\")\n",
        "print(\"-\" * 70)\n",
        "for i, param_group in enumerate(optimizador.param_groups):\n",
        "    print(f\"Grupo {i}: {len(param_group['params'])} tensores\")\n",
        "    print(f\"  Learning rate: {param_group['lr']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFiCyOHHb32F"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Bucle de Entrenamiento\n",
        "\n",
        "El proceso de entrenamiento en PyTorch sigue este patrón:\n",
        "\n",
        "```python\n",
        "for epoca in range(epocas):\n",
        "    # 1. Forward pass: calcular predicciones\n",
        "    salida = modelo(X)\n",
        "    \n",
        "    # 2. Calcular pérdida\n",
        "    loss = criterio(salida, y)\n",
        "    \n",
        "    # 3. Backward pass: calcular gradientes\n",
        "    loss.backward()\n",
        "    \n",
        "    # 4. Actualizar parámetros\n",
        "    optimizador.step()\n",
        "    \n",
        "    # 5. Limpiar gradientes para la próxima iteración\n",
        "    optimizador.zero_grad()\n",
        "```\n",
        "\n",
        "### ¿Qué hace cada paso?\n",
        "\n",
        "1. **Forward pass**: Datos fluyen por la red, genera predicciones\n",
        "2. **Loss**: Compara predicciones con etiquetas reales\n",
        "3. **Backward**: Autograd calcula ∂Loss/∂params (gradientes)\n",
        "4. **Step**: Optimizador actualiza params usando gradientes\n",
        "5. **Zero_grad**: Limpia gradientes (PyTorch los acumula por defecto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsZRaLQxb32F",
        "outputId": "456296c5-4e4f-4110-d7d9-ca878ac38c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando entrenamiento...\n",
            "======================================================================\n",
            "Época  10/200: Loss = 0.6564\n",
            "Época  20/200: Loss = 0.5616\n",
            "Época  30/200: Loss = 0.4274\n",
            "Época  40/200: Loss = 0.2991\n",
            "Época  50/200: Loss = 0.2070\n",
            "Época  60/200: Loss = 0.1464\n",
            "Época  70/200: Loss = 0.1065\n",
            "Época  80/200: Loss = 0.0796\n",
            "Época  90/200: Loss = 0.0609\n",
            "Época 100/200: Loss = 0.0477\n",
            "Época 110/200: Loss = 0.0380\n",
            "Época 120/200: Loss = 0.0309\n",
            "Época 130/200: Loss = 0.0256\n",
            "Época 140/200: Loss = 0.0215\n",
            "Época 150/200: Loss = 0.0183\n",
            "Época 160/200: Loss = 0.0158\n",
            "Época 170/200: Loss = 0.0138\n",
            "Época 180/200: Loss = 0.0122\n",
            "Época 190/200: Loss = 0.0108\n",
            "Época 200/200: Loss = 0.0096\n",
            "\n",
            "======================================================================\n",
            "Entrenamiento completado.\n",
            "Pérdida final: 0.0096\n"
          ]
        }
      ],
      "source": [
        "# Hiperparámetros de entrenamiento\n",
        "epocas = 200  # Número de pasadas completas por el dataset\n",
        "\n",
        "# Listas para guardar el historial\n",
        "historial_loss = []\n",
        "\n",
        "print(\"Iniciando entrenamiento...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Ponemos el modelo en modo entrenamiento\n",
        "# Esto afecta capas como Dropout y BatchNorm (no las usamos aquí, pero es buena práctica)\n",
        "modelo.train()\n",
        "\n",
        "for epoca in range(epocas):\n",
        "    # Paso 1: Forward pass\n",
        "    # Pasamos los datos por la red\n",
        "    salida = modelo(X)\n",
        "\n",
        "    # Paso 2: Calcular pérdida\n",
        "    loss = criterio(salida, y)\n",
        "\n",
        "    # Paso 3: Backward pass\n",
        "    # Limpiamos gradientes de la iteración anterior\n",
        "    optimizador.zero_grad()\n",
        "\n",
        "    # Calculamos gradientes con backpropagation\n",
        "    # PyTorch calcula automáticamente ∂loss/∂w para todos los parámetros\n",
        "    loss.backward()\n",
        "\n",
        "    # Paso 4: Actualizar parámetros\n",
        "    # El optimizador ajusta los pesos usando los gradientes\n",
        "    optimizador.step()\n",
        "\n",
        "    # Guardamos la pérdida para visualización\n",
        "    historial_loss.append(loss.item())\n",
        "\n",
        "    # Imprimimos progreso cada 10 épocas\n",
        "    if (epoca + 1) % 10 == 0:\n",
        "        print(f\"Época {epoca+1:3d}/{epocas}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Entrenamiento completado.\")\n",
        "print(f\"Pérdida final: {historial_loss[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxz07-Qjb32G"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Visualización del Aprendizaje\n",
        "\n",
        "Graficamos la curva de aprendizaje (loss vs. épocas) para visualizar la convergencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "XrhTpsr4b32H",
        "outputId": "784e1256-147f-457f-e7b0-a9395265c076"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpretación de la curva:\n",
            "======================================================================\n",
            "- Curva descendente: El modelo está aprendiendo\n",
            "- Convergencia: La pérdida se estabiliza en un valor bajo\n",
            "- Si oscila mucho: Learning rate probablemente muy alto\n",
            "- Si baja muy lento: Learning rate probablemente muy bajo\n",
            "\n",
            "Esta curva muestra que el modelo convergió correctamente.\n"
          ]
        }
      ],
      "source": [
        "# Gráfico de la pérdida durante el entrenamiento\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(historial_loss, linewidth=2)\n",
        "plt.xlabel('Época', fontsize=12)\n",
        "plt.ylabel('Loss (Binary Cross Entropy)', fontsize=12)\n",
        "plt.title('Curva de Aprendizaje del MLP', fontsize=14, pad=15)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Interpretación de la curva:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"- Curva descendente: El modelo está aprendiendo\")\n",
        "print(\"- Convergencia: La pérdida se estabiliza en un valor bajo\")\n",
        "print(\"- Si oscila mucho: Learning rate probablemente muy alto\")\n",
        "print(\"- Si baja muy lento: Learning rate probablemente muy bajo\")\n",
        "print(\"\\nEsta curva muestra que el modelo convergió correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htseNtyTb32H"
      },
      "source": [
        "---\n",
        "\n",
        "## 11. Evaluación del Modelo\n",
        "\n",
        "Evaluamos el modelo en el conjunto de entrenamiento (en un caso real usaríamos un conjunto de prueba separado)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EOeIAQYb32I",
        "outputId": "a9982c40-e5a5-44e6-8f6b-d2bb50382cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluación en el conjunto de entrenamiento:\n",
            "======================================================================\n",
            "\n",
            "✓ Frase: 'La verdad, este lugar está bárbaro. Muy recomendab...'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.989)\n",
            "\n",
            "✓ Frase: 'Qué buena onda la atención, volvería sin dudarlo....'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.997)\n",
            "\n",
            "✓ Frase: 'Me encantó la comida, aunque la música estaba muy ...'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.986)\n",
            "\n",
            "✓ Frase: 'Todo excelente. Atención de diez....'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.989)\n",
            "\n",
            "✓ Frase: 'Muy conforme con el resultado final....'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.988)\n",
            "\n",
            "✓ Frase: 'Superó mis expectativas, gracias....'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.990)\n",
            "\n",
            "✓ Frase: 'El mejor asado que probé en mucho tiempo....'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.986)\n",
            "\n",
            "✓ Frase: 'Excelente relación precio-calidad, muy recomendabl...'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.990)\n",
            "\n",
            "✓ Frase: 'La atención fue impecable, muy atentos....'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.989)\n",
            "\n",
            "✓ Frase: 'Me gustó mucho el ambiente tranquilo....'\n",
            "  Real: Positivo | Predicción: Positivo (prob=0.980)\n",
            "\n",
            "✓ Frase: 'Una porquería de servicio, nunca más vuelvo....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.000)\n",
            "\n",
            "✓ Frase: 'El envío fue lento y el producto llegó dañado. Qué...'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.030)\n",
            "\n",
            "✓ Frase: 'Qué estafa, me arrepiento de haber comprado....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.000)\n",
            "\n",
            "✓ Frase: 'No me gustó para nada la experiencia....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.012)\n",
            "\n",
            "✓ Frase: 'No lo recomiendo, mala calidad....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.000)\n",
            "\n",
            "✓ Frase: 'Malísima atención, el mozo tenía mala onda....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.000)\n",
            "\n",
            "✓ Frase: 'Tardaron dos horas en entregar, llegó todo frío....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.000)\n",
            "\n",
            "✓ Frase: 'Me cobraron de más y encima se hicieron los giles....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.030)\n",
            "\n",
            "✓ Frase: 'La carne estaba pasada, casi no se podía comer....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.000)\n",
            "\n",
            "✓ Frase: 'Pésima experiencia, no vuelvo más....'\n",
            "  Real: Negativo | Predicción: Negativo (prob=0.000)\n",
            "\n",
            "======================================================================\n",
            "Accuracy: 20/20 = 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Ponemos el modelo en modo evaluación\n",
        "# Esto desactiva Dropout y fija BatchNorm (no los usamos aquí, pero es buena práctica)\n",
        "modelo.eval()\n",
        "\n",
        "# torch.no_grad() desactiva el cálculo de gradientes (ahorra memoria y acelera)\n",
        "with torch.no_grad():\n",
        "    # Forward pass para obtener predicciones\n",
        "    predicciones = modelo(X)\n",
        "\n",
        "    # Convertimos probabilidades a clases (umbral 0.5)\n",
        "    # predicciones >= 0.5 → clase 1, sino → clase 0\n",
        "    clases_predichas = (predicciones >= 0.5).float()\n",
        "\n",
        "# Calculamos accuracy\n",
        "aciertos = (clases_predichas == y).sum().item()\n",
        "total = len(y)\n",
        "accuracy = aciertos / total\n",
        "\n",
        "print(\"Evaluación en el conjunto de entrenamiento:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i in range(len(frases)):\n",
        "    prob = predicciones[i].item()\n",
        "    pred_clase = clases_predichas[i].item()\n",
        "    real_clase = y[i].item()\n",
        "\n",
        "    correcto = \"✓\" if pred_clase == real_clase else \"✗\"\n",
        "    sentimiento_real = \"Positivo\" if real_clase == 1 else \"Negativo\"\n",
        "    sentimiento_pred = \"Positivo\" if pred_clase == 1 else \"Negativo\"\n",
        "\n",
        "    print(f\"\\n{correcto} Frase: '{frases[i][:50]}...'\")  # Truncamos si es muy larga\n",
        "    print(f\"  Real: {sentimiento_real} | Predicción: {sentimiento_pred} (prob={prob:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Accuracy: {aciertos}/{total} = {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhy4Jo_8b32I"
      },
      "source": [
        "---\n",
        "\n",
        "## 12. Predicción sobre Frases Nuevas\n",
        "\n",
        "Probamos el modelo con frases que nunca vio durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A7BNR-hb32I",
        "outputId": "58e2adbb-60ad-4a4a-8833-89821ed3ef18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicciones sobre frases nuevas:\n",
            "======================================================================\n",
            "\n",
            "Frase: 'No me gustó la atención, bastante mala'\n",
            "Predicción: NEGATIVO\n",
            "Probabilidad positivo: 0.000\n",
            "Confianza: 100.0%\n",
            "\n",
            "Frase: 'Muy buena experiencia, todo excelente'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.998\n",
            "Confianza: 99.8%\n",
            "\n",
            "Frase: 'Una estafa total, no lo recomiendo'\n",
            "Predicción: NEGATIVO\n",
            "Probabilidad positivo: 0.000\n",
            "Confianza: 100.0%\n",
            "\n",
            "Frase: 'Súper conforme con el servicio'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.986\n",
            "Confianza: 98.6%\n",
            "\n",
            "Frase: 'Nada que ver con lo prometido, una decepción'\n",
            "Predicción: NEGATIVO\n",
            "Probabilidad positivo: 0.030\n",
            "Confianza: 97.0%\n",
            "\n",
            "Frase: 'La mejor atención que tuve en mucho tiempo'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.998\n",
            "Confianza: 99.8%\n"
          ]
        }
      ],
      "source": [
        "# Frases de prueba\n",
        "frases_prueba = [\n",
        "    \"No me gustó la atención, bastante mala\",\n",
        "    \"Muy buena experiencia, todo excelente\",\n",
        "    \"Una estafa total, no lo recomiendo\",\n",
        "    \"Súper conforme con el servicio\",\n",
        "    \"Nada que ver con lo prometido, una decepción\",\n",
        "    \"La mejor atención que tuve en mucho tiempo\"\n",
        "]\n",
        "\n",
        "# Vectorizamos las frases nuevas\n",
        "X_prueba_np = np.array([vectorizar(frase, vocabulario) for frase in frases_prueba], dtype=np.float32)\n",
        "X_prueba = torch.tensor(X_prueba_np)\n",
        "\n",
        "# Predicción en modo evaluación\n",
        "modelo.eval()\n",
        "with torch.no_grad():\n",
        "    predicciones_prueba = modelo(X_prueba)\n",
        "\n",
        "# Mostramos resultados\n",
        "print(\"Predicciones sobre frases nuevas:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, frase in enumerate(frases_prueba):\n",
        "    prob = predicciones_prueba[i].item()\n",
        "    clase = \"POSITIVO\" if prob >= 0.5 else \"NEGATIVO\"\n",
        "    confianza = prob if prob >= 0.5 else (1 - prob)\n",
        "\n",
        "    print(f\"\\nFrase: '{frase}'\")\n",
        "    print(f\"Predicción: {clase}\")\n",
        "    print(f\"Probabilidad positivo: {prob:.3f}\")\n",
        "    print(f\"Confianza: {confianza:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nOClSG-b32I"
      },
      "source": [
        "---\n",
        "\n",
        "## 13. Comparación: Perceptrón Simple vs. MLP\n",
        "\n",
        "Comparemos conceptualmente lo que ganamos con capas ocultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipM_w_1Xb32J"
      },
      "outputs": [],
      "source": [
        "print(\"COMPARACIÓN: PERCEPTRÓN SIMPLE VS. MLP\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "comparacion = [\n",
        "    (\"Arquitectura\", \"Una capa (entrada → salida)\", \"Múltiples capas (entrada → ocultas → salida)\"),\n",
        "    (\"Parámetros\", \"~30 (30 pesos + 1 bias)\", \"~280 (30×8 + 8 + 8×1 + 1)\"),\n",
        "    (\"Capacidad\", \"Solo problemas lineales\", \"Problemas no lineales complejos\"),\n",
        "    (\"Función activación\", \"Escalón (no diferenciable)\", \"ReLU + Sigmoid (diferenciables)\"),\n",
        "    (\"Aprendizaje\", \"Regla del perceptrón\", \"Backpropagation con Adam\"),\n",
        "    (\"XOR\", \"❌ No puede resolverlo\", \"✓ Sí puede resolverlo\"),\n",
        "    (\"Interacciones\", \"❌ No captura\", \"✓ Aprende features abstractas\"),\n",
        "    (\"Convergencia\", \"Rápida si linealmente separable\", \"Más lenta pero más potente\"),\n",
        "    (\"Overfitting\", \"Poco riesgo (modelo simple)\", \"Mayor riesgo (más parámetros)\"),\n",
        "    (\"Interpretabilidad\", \"Alta (pesos directos)\", \"Media (representaciones ocultas)\"),\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Característica':<20} | {'Perceptrón Simple':<35} | {'MLP':<35}\")\n",
        "print(\"-\" * 95)\n",
        "for caracteristica, perceptron, mlp in comparacion:\n",
        "    print(f\"{caracteristica:<20} | {perceptron:<35} | {mlp:<35}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Conclusión:\")\n",
        "print(\"-\" * 70)\n",
        "print(\"El MLP es más potente pero requiere:\")\n",
        "print(\"  1. Más datos para entrenar (evitar overfitting)\")\n",
        "print(\"  2. Más tiempo de cómputo\")\n",
        "print(\"  3. Ajuste de hiperparámetros (learning rate, hidden size, etc.)\")\n",
        "print(\"  4. Frameworks modernos (PyTorch, TensorFlow) para eficiencia\")\n",
        "print(\"\\nEl perceptrón simple sigue siendo útil como baseline rápido.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hANI8AGb32J"
      },
      "source": [
        "---\n",
        "\n",
        "## 14. Guardar y Cargar el Modelo\n",
        "\n",
        "En aplicaciones reales, necesitamos guardar modelos entrenados para usarlos más tarde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrnTmFy6b32J"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo\n",
        "# state_dict() retorna un diccionario con todos los parámetros\n",
        "torch.save(modelo.state_dict(), 'mlp_sentiment.pth')\n",
        "print(\"Modelo guardado en: mlp_sentiment.pth\")\n",
        "\n",
        "# Cargar el modelo\n",
        "# Primero creamos una instancia nueva del modelo\n",
        "modelo_cargado = MLP(input_size, hidden_size)\n",
        "\n",
        "# Luego cargamos los parámetros guardados\n",
        "modelo_cargado.load_state_dict(torch.load('mlp_sentiment.pth'))\n",
        "modelo_cargado.eval()  # Modo evaluación\n",
        "\n",
        "print(\"Modelo cargado exitosamente.\")\n",
        "\n",
        "# Verificamos que funciona igual\n",
        "with torch.no_grad():\n",
        "    pred_original = modelo(X_prueba)\n",
        "    pred_cargado = modelo_cargado(X_prueba)\n",
        "\n",
        "    # Comprobamos que las predicciones son idénticas\n",
        "    igual = torch.allclose(pred_original, pred_cargado)\n",
        "    print(f\"\\n¿Predicciones idénticas? {igual}\")\n",
        "\n",
        "print(\"\\nEn producción, guardarías el modelo después del entrenamiento\")\n",
        "print(\"y lo cargarías en un servidor/API para hacer predicciones.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qGqwxvQb32K"
      },
      "source": [
        "---\n",
        "\n",
        "## Guía Teórico-Conceptual\n",
        "\n",
        "### 1. Teorema de Aproximación Universal\n",
        "\n",
        "**Enunciado (Cybenko, 1989; Hornik, 1991):**\n",
        "> Una red neuronal feedforward con una sola capa oculta puede aproximar cualquier función continua en un dominio compacto, con precisión arbitraria, siempre que tenga suficientes neuronas ocultas.\n",
        "\n",
        "**Implicaciones:**\n",
        "- Los MLP son aproximadores universales de funciones\n",
        "- En teoría, una capa oculta es suficiente\n",
        "- En práctica, redes profundas (muchas capas) aprenden más eficientemente\n",
        "\n",
        "**Limitaciones:**\n",
        "- No dice cuántas neuronas se necesitan (podría ser exponencial)\n",
        "- No garantiza que el entrenamiento encuentre la aproximación óptima\n",
        "- Redes profundas suelen ser más eficientes que redes anchas\n",
        "\n",
        "### 2. Backpropagation: La Magia de Autograd\n",
        "\n",
        "Backpropagation es la aplicación de la regla de la cadena del cálculo:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial w_1}$$\n",
        "\n",
        "**En PyTorch:**\n",
        "```python\n",
        "loss.backward()  # Calcula TODOS los gradientes automáticamente\n",
        "```\n",
        "\n",
        "**Internamente:**\n",
        "1. PyTorch registra todas las operaciones en un **grafo computacional**\n",
        "2. Cuando llamamos `.backward()`, recorre el grafo en reversa\n",
        "3. Aplica regla de la cadena en cada nodo\n",
        "4. Acumula gradientes en `.grad` de cada tensor con `requires_grad=True`\n",
        "\n",
        "**Ventaja sobre implementación manual:**\n",
        "- No hay que derivar funciones a mano\n",
        "- Funciona con arquitecturas arbitrariamente complejas\n",
        "- Optimizado en C++/CUDA, muy rápido\n",
        "\n",
        "### 3. ReLU vs. Sigmoid vs. Escalón\n",
        "\n",
        "| Característica | Escalón | Sigmoid | ReLU |\n",
        "|----------------|---------|---------|------|\n",
        "| Fórmula | step(x) | 1/(1+e⁻ˣ) | max(0,x) |\n",
        "| Rango | {0, 1} | (0, 1) | [0, ∞) |\n",
        "| Diferenciable | ❌ No | ✓ Sí | ✓ Casi (excepto x=0) |\n",
        "| Vanishing gradient | N/A | ✓ Problema | ❌ No sufre |\n",
        "| Costo computacional | Bajo | Alto (exp) | Muy bajo |\n",
        "| Uso moderno | Obsoleto | Solo salida | Capas ocultas |\n",
        "| Dead neurons | No | No | Sí (si x<0 siempre) |\n",
        "\n",
        "**¿Por qué ReLU domina en capas ocultas?**\n",
        "- Gradiente constante para x>0 (no vanishing)\n",
        "- Computacionalmente trivial (comparación)\n",
        "- Introducida por Hinton et al. en 2010, revolucionó deep learning\n",
        "\n",
        "**Variantes de ReLU:**\n",
        "- **Leaky ReLU**: f(x) = max(0.01x, x) - evita dead neurons\n",
        "- **ELU**: Suave en x<0\n",
        "- **GELU**: Usada en transformers (BERT, GPT)\n",
        "\n",
        "### 4. Vanishing Gradient Problem\n",
        "\n",
        "**Problema:**\n",
        "En redes profundas con sigmoides, los gradientes se multiplican capa por capa:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial h_n} \\cdot \\sigma'(h_{n-1}) \\cdot ... \\cdot \\sigma'(h_2) \\cdot \\sigma'(h_1)$$\n",
        "\n",
        "Como $\\sigma'(x) \\in (0, 0.25)$, el producto de muchos términos < 0.25 tiende a cero.\n",
        "\n",
        "**Consecuencias:**\n",
        "- Capas iniciales no aprenden (gradientes ~0)\n",
        "- Solo capas finales se entrenan\n",
        "- Redes profundas imposibles antes de 2010\n",
        "\n",
        "**Soluciones:**\n",
        "1. **ReLU**: Gradiente = 1 para x>0\n",
        "2. **Batch Normalization**: Normaliza activaciones\n",
        "3. **Residual connections**: Skip connections (ResNet)\n",
        "4. **Inicialización cuidadosa**: Xavier, He initialization\n",
        "\n",
        "### 5. Overfitting en Redes Neuronales\n",
        "\n",
        "**Síntomas:**\n",
        "- Training loss baja pero validation loss sube\n",
        "- Accuracy muy alta en train, baja en test\n",
        "- El modelo memoriza en vez de generalizar\n",
        "\n",
        "**Causas:**\n",
        "1. Demasiados parámetros vs. datos\n",
        "2. Entrenamiento muy largo\n",
        "3. Falta de regularización\n",
        "\n",
        "**Soluciones:**\n",
        "\n",
        "**Regularización L2 (weight decay):**\n",
        "```python\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "```\n",
        "\n",
        "**Dropout:**\n",
        "```python\n",
        "nn.Dropout(p=0.5)  # Desactiva 50% de neuronas aleatoriamente\n",
        "```\n",
        "\n",
        "**Early stopping:**\n",
        "```python\n",
        "if val_loss > best_val_loss:\n",
        "    epochs_without_improvement += 1\n",
        "    if epochs_without_improvement > patience:\n",
        "        break  # Detener entrenamiento\n",
        "```\n",
        "\n",
        "**Data augmentation:**\n",
        "- Sinónimos, paráfrasis, traducciones\n",
        "\n",
        "### 6. Hiperparámetros: Tuning Strategies\n",
        "\n",
        "**Principales hiperparámetros:**\n",
        "1. **Learning rate**: Más importante, probar [1e-5, 1e-1]\n",
        "2. **Hidden size**: Capacidad del modelo\n",
        "3. **Número de capas**: Profundidad\n",
        "4. **Batch size**: Velocidad vs. convergencia\n",
        "5. **Optimizer**: Adam suele ser la mejor opción\n",
        "\n",
        "**Estrategias de búsqueda:**\n",
        "\n",
        "**Grid search:**\n",
        "```python\n",
        "for lr in [0.001, 0.01, 0.1]:\n",
        "    for hidden in [8, 16, 32]:\n",
        "        train_and_evaluate(lr, hidden)\n",
        "```\n",
        "\n",
        "**Random search:** Más eficiente que grid\n",
        "\n",
        "**Bayesian optimization:** Aún más eficiente\n",
        "\n",
        "**Learning rate scheduling:**\n",
        "```python\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "```\n",
        "\n",
        "### 7. Batch Training vs. Online Learning\n",
        "\n",
        "En este notebook entrenamos con **full batch** (todo el dataset a la vez).\n",
        "\n",
        "**Alternativas:**\n",
        "\n",
        "**Stochastic Gradient Descent (SGD):**\n",
        "- Una muestra a la vez\n",
        "- Muy ruidoso pero escapa mínimos locales\n",
        "\n",
        "**Mini-batch:**\n",
        "- Típicamente 32-256 muestras\n",
        "- Balance entre velocidad y estabilidad\n",
        "- Estándar en deep learning\n",
        "\n",
        "**Implementación:**\n",
        "```python\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "for X_batch, y_batch in dataloader:\n",
        "    # Entrenar con el batch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcGknG_Ib32K"
      },
      "source": [
        "---\n",
        "\n",
        "## Preguntas y Respuestas para Estudio\n",
        "\n",
        "### Preguntas Conceptuales\n",
        "\n",
        "**1. ¿Por qué una red multicapa puede resolver XOR y el perceptrón simple no?**\n",
        "\n",
        "*Respuesta:* La capa oculta proyecta los datos a un espacio de mayor dimensión donde SÍ son linealmente separables. Por ejemplo, XOR en 2D no es separable, pero si la capa oculta aprende las features h₁=(x₁ AND x₂) y h₂=(x₁ OR x₂), entonces en el espacio (h₁, h₂) SÍ es separable. La capa oculta aprende representaciones útiles automáticamente.\n",
        "\n",
        "**2. ¿Qué es autograd y cómo funciona en PyTorch?**\n",
        "\n",
        "*Respuesta:* Autograd (automatic differentiation) es el sistema de PyTorch que calcula gradientes automáticamente. Registra todas las operaciones en un grafo computacional y, cuando llamamos `loss.backward()`, recorre el grafo en reversa aplicando la regla de la cadena. Esto nos libera de derivar funciones manualmente.\n",
        "\n",
        "**3. ¿Por qué usamos ReLU en capas ocultas y Sigmoid en la salida?**\n",
        "\n",
        "*Respuesta:*\n",
        "- **ReLU en ocultas**: No sufre vanishing gradient, es computacionalmente eficiente y permite entrenar redes profundas\n",
        "- **Sigmoid en salida**: Convierte la salida a rango [0,1] interpretable como probabilidad, compatible con Binary Cross Entropy Loss\n",
        "\n",
        "**4. ¿Qué significa \"requires_grad=True\" en un tensor?**\n",
        "\n",
        "*Respuesta:* Indica que PyTorch debe calcular y almacenar gradientes para ese tensor. Los parámetros del modelo (pesos, biases) tienen `requires_grad=True` por defecto. Los datos de entrada no, porque no los vamos a optimizar.\n",
        "\n",
        "**5. ¿Por qué llamamos `optimizer.zero_grad()` antes de cada backward pass?**\n",
        "\n",
        "*Respuesta:* Porque PyTorch **acumula** gradientes por defecto (suma los nuevos a los existentes). Si no limpiamos, estaríamos acumulando gradientes de múltiples batches, lo cual es incorrecto. En algunos casos avanzados (gradient accumulation) esta acumulación es intencional, pero normalmente queremos empezar de cero.\n",
        "\n",
        "### Preguntas Técnicas\n",
        "\n",
        "**6. ¿Cuál es la diferencia entre `model.train()` y `model.eval()`?**\n",
        "\n",
        "*Respuesta:*\n",
        "- `model.train()`: Activa modo entrenamiento. Dropout hace drop, BatchNorm actualiza estadísticas\n",
        "- `model.eval()`: Activa modo evaluación. Dropout no hace drop, BatchNorm usa estadísticas fijas\n",
        "\n",
        "Aunque nuestro MLP no usa estas capas, es buena práctica llamarlos para compatibilidad.\n",
        "\n",
        "**7. ¿Qué hace `torch.no_grad()` y cuándo usarlo?**\n",
        "\n",
        "*Respuesta:* Desactiva el tracking de gradientes. Útil durante evaluación/inferencia para:\n",
        "1. Ahorrar memoria (no construir grafo computacional)\n",
        "2. Acelerar ejecución\n",
        "3. Prevenir errores (actualizar parámetros accidentalmente)\n",
        "\n",
        "**8. En el código, ¿por qué reshape(-1, 1) en las etiquetas?**\n",
        "\n",
        "*Respuesta:* `BCELoss` espera que predicciones y etiquetas tengan la misma forma. Nuestro modelo devuelve shape `(n, 1)`, así que las etiquetas también deben ser `(n, 1)` en vez de `(n,)`. El `-1` en reshape significa \"inferir esta dimensión automáticamente\".\n",
        "\n",
        "**9. ¿Qué es un \"forward pass\" y un \"backward pass\"?**\n",
        "\n",
        "*Respuesta:*\n",
        "- **Forward pass**: Los datos fluyen de entrada a salida, calculando predicciones: `y_pred = model(X)`\n",
        "- **Backward pass**: Los gradientes fluyen de salida a entrada, calculando ∂Loss/∂params: `loss.backward()`\n",
        "\n",
        "**10. ¿Por qué `.item()` en `loss.item()`?**\n",
        "\n",
        "*Respuesta:* `.item()` extrae el valor numérico de un tensor de un solo elemento y lo convierte a tipo Python (float). Sin `.item()`, tendríamos un tensor, que consume memoria del grafo computacional. Es buena práctica para logging.\n",
        "\n",
        "### Preguntas de Aplicación\n",
        "\n",
        "**11. Si la curva de loss oscila mucho sin bajar, ¿qué harías?**\n",
        "\n",
        "*Respuesta:*\n",
        "1. **Reducir learning rate**: Probar 0.001 en vez de 0.01\n",
        "2. **Cambiar optimizador**: Adam → SGD with momentum\n",
        "3. **Normalizar datos**: Estandarizar features\n",
        "4. **Revisar implementación**: ¿Está llamando zero_grad()?\n",
        "5. **Aumentar batch size**: Más estabilidad (si usás mini-batches)\n",
        "\n",
        "**12. ¿Cómo adaptarías este código para clasificación multiclase (3+ clases)?**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "# 1. Cambiar salida del modelo\n",
        "nn.Linear(hidden_size, num_classes),  # En vez de 1\n",
        "nn.Softmax(dim=1)  # En vez de Sigmoid\n",
        "\n",
        "# 2. Cambiar loss\n",
        "criterio = nn.CrossEntropyLoss()  # En vez de BCELoss\n",
        "\n",
        "# 3. Etiquetas como enteros\n",
        "y = torch.tensor([0, 1, 2, 0, 1, ...])  # Sin one-hot\n",
        "\n",
        "# 4. Predicción\n",
        "pred_class = torch.argmax(output, dim=1)  # Clase con mayor prob\n",
        "```\n",
        "\n",
        "**13. Si tuvieras 10,000 frases en vez de 20, ¿qué cambios harías al código?**\n",
        "\n",
        "*Respuesta:*\n",
        "1. **Usar DataLoader** para mini-batches (batch_size=32 o 64)\n",
        "2. **Split train/test**: 80/20 para evaluación honesta\n",
        "3. **Más épocas**: Probablemente 50-200 en vez de 200 sobre dataset pequeño\n",
        "4. **Red más grande**: hidden_size=64 o 128\n",
        "5. **Agregar Dropout**: Para prevenir overfitting\n",
        "6. **Usar GPU**: `model.to('cuda')` para acelerar\n",
        "\n",
        "**14. ¿Cómo implementarías early stopping?**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoca in range(max_epocas):\n",
        "    # Entrenar...\n",
        "    val_loss = evaluar_en_validacion()\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "    \n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "```\n",
        "\n",
        "**15. Explicá cómo desplegarías este modelo en una API REST.**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Cargar modelo al inicio\n",
        "modelo = MLP(input_size=30, hidden_size=8)\n",
        "modelo.load_state_dict(torch.load('mlp_sentiment.pth'))\n",
        "modelo.eval()\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(texto: str):\n",
        "    # Vectorizar\n",
        "    x = torch.tensor(vectorizar(texto, vocabulario))\n",
        "    \n",
        "    # Predecir\n",
        "    with torch.no_grad():\n",
        "        prob = modelo(x.unsqueeze(0)).item()\n",
        "    \n",
        "    return {\n",
        "        \"texto\": texto,\n",
        "        \"sentimiento\": \"positivo\" if prob > 0.5 else \"negativo\",\n",
        "        \"probabilidad\": prob\n",
        "    }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9KF9Hjqb32L"
      },
      "source": [
        "---\n",
        "\n",
        "## Ejercicios Propuestos\n",
        "\n",
        "### Ejercicio 1: Experimentación con Arquitecturas\n",
        "Probá diferentes configuraciones:\n",
        "- hidden_size: 4, 8, 16, 32, 64\n",
        "- Dos capas ocultas: Input → 16 → 8 → Output\n",
        "\n",
        "Graficá las curvas de loss. ¿Más neuronas siempre es mejor?\n",
        "\n",
        "### Ejercicio 2: Funciones de Activación\n",
        "Reemplazá ReLU por otras activaciones:\n",
        "- `nn.Tanh()`\n",
        "- `nn.LeakyReLU()`\n",
        "- `nn.ELU()`\n",
        "\n",
        "Compará convergencia y accuracy final.\n",
        "\n",
        "### Ejercicio 3: Regularización con Dropout\n",
        "Agregá dropout después de la capa oculta:\n",
        "```python\n",
        "self.net = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),  # ← Nuevo\n",
        "    nn.Linear(hidden_size, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "¿Cómo afecta al entrenamiento?\n",
        "\n",
        "### Ejercicio 4: Optimizadores\n",
        "Compará Adam vs. SGD:\n",
        "```python\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "```\n",
        "¿Cuál converge más rápido?\n",
        "\n",
        "### Ejercicio 5: Problema XOR\n",
        "Entrená el MLP con el problema XOR:\n",
        "```python\n",
        "X_xor = torch.tensor([[0.,0.], [0.,1.], [1.,0.], [1.,1.]])\n",
        "y_xor = torch.tensor([[0.], [1.], [1.], [0.]])\n",
        "```\n",
        "Verificá que converge. Probá con hidden_size=2, 4, 8. ¿Cuál es el mínimo?\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "En este notebook dimos un salto fundamental: de implementación manual a framework moderno, y de modelos lineales a redes multicapa. Aprendimos:\n",
        "\n",
        "1. **Fundamentos de PyTorch**: Tensores, autograd, nn.Module\n",
        "2. **Arquitectura MLP**: Capas ocultas, ReLU, Sigmoid\n",
        "3. **Entrenamiento con backpropagation**: Forward, loss, backward, step\n",
        "4. **Superación de limitaciones**: XOR y problemas no lineales\n",
        "5. **Prácticas profesionales**: Guardar/cargar modelos, modo train/eval\n",
        "\n",
        "### ¿Qué sigue?\n",
        "\n",
        "Las redes feedforward (MLP) ignoran el **orden** de las palabras. En el próximo notebook vamos a usar **LSTM (Long Short-Term Memory)**, una arquitectura recurrente que procesa texto como secuencia, capturando contexto y dependencias temporales.\n",
        "\n",
        "Con LSTM podremos modelar patrones como:\n",
        "- \"No me gustó\" vs. \"Me gustó\" (negación)\n",
        "- \"Muy muy bueno\" (énfasis por repetición)\n",
        "- Dependencias largas en el texto\n",
        "\n",
        "---\n",
        "\n",
        "*Este material fue desarrollado con fines educativos para la Tecnicatura en Ciencia de Datos del IFTS.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}