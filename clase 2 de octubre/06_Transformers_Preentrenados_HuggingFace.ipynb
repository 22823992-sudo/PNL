{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSliuGhpb2sC"
      },
      "source": [
        "# Transformers Pre-entrenados con HuggingFace\n",
        "\n",
        "**Materiales desarrollados por Matías Barreto, 2025**\n",
        "\n",
        "**Tecnicatura en Ciencia de Datos - IFTS**\n",
        "\n",
        "**Asignatura:** Procesamiento de Lenguaje Natural\n",
        "\n",
        "---\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Este notebook marca un **punto de inflexión** en el curso. Hasta ahora construimos modelos desde cero (perceptrón, MLP, LSTM) para entender los fundamentos. Ahora damos el salto al **paradigma moderno de NLP**: usar modelos transformers pre-entrenados con transfer learning.\n",
        "\n",
        "### El momento \"ajá\" del curso\n",
        "\n",
        "Después de 5 notebooks programando y entrenando modelos, ahora vamos a ver que:\n",
        "- **3 líneas de código** superan todo lo anterior\n",
        "- No necesitamos entrenar desde cero\n",
        "- Transfer learning es el estándar de la industria\n",
        "\n",
        "Esto no invalida lo que aprendimos. Todo ese conocimiento te permite entender **qué está pasando bajo el capó** de HuggingFace.\n",
        "\n",
        "### ¿Qué son los Transformers?\n",
        "\n",
        "Arquitectura revolucionaria introducida en \"Attention is All You Need\" (Vaswani et al., 2017):\n",
        "- **Attention mechanism**: Todas las palabras se ven entre sí simultáneamente\n",
        "- **Paralelizable**: No procesa secuencialmente como LSTM\n",
        "- **Escalable**: Funciona con modelos gigantes (GPT-3: 175B parámetros)\n",
        "- **Transfer learning**: Pre-entrenamiento + fine-tuning\n",
        "\n",
        "### Objetivos de aprendizaje\n",
        "\n",
        "1. Usar HuggingFace Transformers library\n",
        "2. Cargar modelos pre-entrenados (BETO, RoBERTuito)\n",
        "3. Aplicar pipelines para tareas comunes\n",
        "4. Entender el concepto de transfer learning\n",
        "5. Trabajar con modelos en español\n",
        "6. Prepararse para fine-tuning (próximas semanas del curso)\n",
        "\n",
        "### ¿Por qué HuggingFace?\n",
        "\n",
        "- **Model Hub**: Miles de modelos pre-entrenados\n",
        "- **API unificada**: Mismo código para BERT, GPT, T5, etc.\n",
        "- **Comunidad**: Open source, muy activa\n",
        "- **Industria**: Estándar de facto en producción\n",
        "- **Español**: Excelentes modelos para español (BETO, RoBERTuito, MarIA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Eg2131b2sK"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Instalación de HuggingFace Transformers\n",
        "\n",
        "En Google Colab, instalamos la librería (ya viene en algunas versiones, pero mejor asegurarnos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ay8EDK-b2sM"
      },
      "outputs": [],
      "source": [
        "# Instalamos transformers de HuggingFace\n",
        "# -q: quiet mode (menos output)\n",
        "!pip install -q transformers\n",
        "\n",
        "print(\"Librería 'transformers' instalada correctamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lYdAr3Mb2sO"
      },
      "outputs": [],
      "source": [
        "# Importamos librerías\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "\n",
        "# Suprimimos warnings menores para output más limpio\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Librerías importadas.\")\n",
        "print(\"\\nNota: La primera vez que uses un modelo, HuggingFace lo descargará.\")\n",
        "print(\"Esto puede tardar unos minutos dependiendo del tamaño del modelo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw59VnWzb2sP"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Concepto: Transfer Learning en NLP\n",
        "\n",
        "### Paradigma tradicional (lo que hicimos en notebooks anteriores):\n",
        "\n",
        "```\n",
        "1. Recolectar dataset etiquetado\n",
        "2. Inicializar pesos aleatoriamente\n",
        "3. Entrenar desde cero\n",
        "4. Evaluar\n",
        "```\n",
        "\n",
        "**Problemas:**\n",
        "- Requiere muchos datos (>10k muestras)\n",
        "- Tiempo de entrenamiento largo\n",
        "- Recursos computacionales (GPU)\n",
        "- El modelo aprende lenguaje desde cero\n",
        "\n",
        "### Paradigma moderno: Transfer Learning\n",
        "\n",
        "```\n",
        "1. Tomar modelo pre-entrenado en corpus masivo (Wikipedia, Common Crawl)\n",
        "2. Adaptar a tu tarea específica (fine-tuning)\n",
        "3. Requiere pocos datos (<1k muestras)\n",
        "4. Entrenamiento rápido\n",
        "```\n",
        "\n",
        "**Ventajas:**\n",
        "- El modelo ya \"entiende\" lenguaje\n",
        "- Solo ajusta para tu tarea específica\n",
        "- Funciona con datasets pequeños\n",
        "- Estado del arte con menos esfuerzo\n",
        "\n",
        "### Analogía\n",
        "\n",
        "**Entrenar desde cero:** Como enseñar a leer a alguien desde alfabeto y luego pedirle que clasifique sentimientos\n",
        "\n",
        "**Transfer learning:** Como pedirle a alguien que ya lee español que aprenda a distinguir reseñas positivas/negativas (mucho más rápido)\n",
        "\n",
        "### Dos fases del Transfer Learning\n",
        "\n",
        "**Pre-entrenamiento (ya hecho por otros):**\n",
        "- Corpus masivo: Wikipedia (3B palabras), Common Crawl (500B palabras)\n",
        "- Tarea auto-supervisada: Masked Language Modeling (BERT), Next Token Prediction (GPT)\n",
        "- Meses de entrenamiento en clusters de GPUs\n",
        "- Costo: $100k - $1M USD\n",
        "\n",
        "**Fine-tuning (lo que nosotros haremos):**\n",
        "- Dataset específico: Nuestras reseñas etiquetadas\n",
        "- Tarea supervisada: Clasificación, NER, QA, etc.\n",
        "- Horas/días en una GPU\n",
        "- Costo: $10 - $100 USD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3E3wCS5b2sQ"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. HuggingFace Pipelines: La Forma Más Simple\n",
        "\n",
        "Los **pipelines** son abstracciones de alto nivel que encapsulan:\n",
        "1. Tokenización (texto → tokens)\n",
        "2. Modelo (tokens → predicciones)\n",
        "3. Post-procesamiento (predicciones → formato legible)\n",
        "\n",
        "### Pipelines disponibles:\n",
        "\n",
        "- `sentiment-analysis`: Clasificación de sentimientos\n",
        "- `ner`: Named Entity Recognition\n",
        "- `question-answering`: Responder preguntas sobre un contexto\n",
        "- `text-generation`: Generar texto (GPT)\n",
        "- `translation`: Traducción\n",
        "- `summarization`: Resumen de texto\n",
        "- `fill-mask`: Completar texto enmascarado\n",
        "- Y muchos más...\n",
        "\n",
        "### Sintaxis básica:\n",
        "\n",
        "```python\n",
        "# Opción 1: Modelo por defecto (inglés)\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Opción 2: Modelo específico\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X1C3Nwqb2sS"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Cargando BETO: BERT en Español\n",
        "\n",
        "### ¿Qué es BETO?\n",
        "\n",
        "**BETO** (Spanish BERT) es un modelo transformer pre-entrenado en español:\n",
        "- Basado en BERT (Bidirectional Encoder Representations from Transformers)\n",
        "- Pre-entrenado en Wikipedia en español\n",
        "- 110M parámetros\n",
        "- Desarrollado por Universidad de Chile\n",
        "\n",
        "### Variante que usaremos:\n",
        "\n",
        "`finiteautomata/beto-sentiment-analysis`:\n",
        "- BETO fine-tuneado para análisis de sentimientos\n",
        "- Entrenado en tweets en español\n",
        "- 3 clases: POS (positivo), NEU (neutral), NEG (negativo)\n",
        "- Estado del arte para español"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKmQlxhMb2sT"
      },
      "outputs": [],
      "source": [
        "print(\"Cargando modelo BETO...\")\n",
        "print(\"La primera vez descargará ~400MB (modelo + tokenizer).\")\n",
        "print(\"Esto puede tardar 1-2 minutos.\\n\")\n",
        "\n",
        "# Creamos el pipeline de sentiment analysis con BETO\n",
        "# model: Especificamos el modelo del HuggingFace Model Hub\n",
        "clasificador = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"finiteautomata/beto-sentiment-analysis\"\n",
        ")\n",
        "\n",
        "print(\"Modelo BETO cargado correctamente.\")\n",
        "print(\"\\nInformación del modelo:\")\n",
        "print(f\"  Nombre: finiteautomata/beto-sentiment-analysis\")\n",
        "print(f\"  Base: BETO (Spanish BERT)\")\n",
        "print(f\"  Tarea: Sentiment Analysis\")\n",
        "print(f\"  Idioma: Español\")\n",
        "print(f\"  Clases: POS, NEU, NEG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jc6Mfjyb2sV"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Primera Predicción: ¡3 Líneas de Código!\n",
        "\n",
        "Vamos a clasificar una frase. Observá lo simple que es comparado con todo lo que programamos antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXOJduVlb2sW"
      },
      "outputs": [],
      "source": [
        "# Una sola frase de ejemplo\n",
        "texto = \"Si queres morirte de calor, el lugar está bárbaro, muy recomendable.\"\n",
        "\n",
        "# Predicción (una línea)\n",
        "resultado = clasificador(texto)\n",
        "\n",
        "# Mostramos el resultado\n",
        "print(\"=\"*70)\n",
        "print(\"PREDICCIÓN CON TRANSFORMERS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTexto: '{texto}'\")\n",
        "print(f\"\\nResultado:\")\n",
        "print(f\"  Sentimiento: {resultado[0]['label']}\")\n",
        "print(f\"  Confianza: {resultado[0]['score']:.2%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Observación: ¡Esto fue mucho más simple que todos los notebooks\")\n",
        "print(\"anteriores juntos! El modelo ya viene entrenado y listo para usar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URpVOn2Nb2sX"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Corpus de Prueba: Español Rioplatense\n",
        "\n",
        "Usamos el mismo corpus de los notebooks anteriores para comparar resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz7rYyXmb2sY"
      },
      "outputs": [],
      "source": [
        "# Corpus en español rioplatense con expresiones coloquiales\n",
        "frases = [\n",
        "    # Positivas\n",
        "    \"La verdad, este lugar está bárbaro. Muy recomendable.\",\n",
        "    \"Qué buena onda la atención, volvería sin dudarlo.\",\n",
        "    \"Me encantó la comida, aunque la música estaba muy fuerte.\",\n",
        "    \"Todo excelente. Atención de diez.\",\n",
        "    \"Muy conforme con el resultado final.\",\n",
        "    \"Superó mis expectativas, gracias.\",\n",
        "    \"El mejor asado que probé en mucho tiempo.\",\n",
        "    \"Excelente relación precio-calidad, muy recomendable.\",\n",
        "    \"La atención fue impecable, muy atentos.\",\n",
        "    \"Me gustó mucho el ambiente tranquilo.\",\n",
        "\n",
        "    # Negativas\n",
        "    \"Una porquería de servicio, nunca más vuelvo.\",\n",
        "    \"El envío fue lento y el producto llegó dañado. Qué desastre.\",\n",
        "    \"Qué estafa, me arrepiento de haber comprado.\",\n",
        "    \"No me gustó para nada la experiencia.\",\n",
        "    \"No lo recomiendo, mala calidad.\",\n",
        "    \"Malísima atención, el mozo tenía mala onda.\",\n",
        "    \"Tardaron dos horas en entregar, llegó todo frío.\",\n",
        "    \"Me cobraron de más y encima se hicieron los giles.\",\n",
        "    \"La carne estaba pasada, casi no se podía comer.\",\n",
        "    \"Pésima experiencia, no vuelvo más.\",\n",
        "\n",
        "    # Adicionales con expresiones argentinas\n",
        "    \"Zafa, pero nada especial.\",\n",
        "    \"Está piola el lugar, volvería.\",\n",
        "    \"Qué garrón, tardaron una banda.\",\n",
        "    \"Re copado todo, la rompieron.\",\n",
        "    \"Un bodrio total, no vayan.\"\n",
        "]\n",
        "\n",
        "print(f\"Corpus: {len(frases)} frases en español rioplatense\")\n",
        "print(f\"\\nEjemplos:\")\n",
        "print(f\"  [Positiva] {frases[0]}\")\n",
        "print(f\"  [Negativa] {frases[10]}\")\n",
        "print(f\"  [Coloquial] {frases[20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX4u3R1Pb2sZ"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Clasificación Masiva con Pipelines\n",
        "\n",
        "Los pipelines pueden procesar listas de textos de forma eficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXTjPl71b2sa"
      },
      "outputs": [],
      "source": [
        "print(\"Clasificando todas las frases con BETO...\\n\")\n",
        "\n",
        "# Clasificamos todas las frases de una vez\n",
        "# El pipeline automáticamente procesa en batches\n",
        "resultados = clasificador(frases)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RESULTADOS DE CLASIFICACIÓN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Mostramos resultados detallados\n",
        "for i, (frase, resultado) in enumerate(zip(frases, resultados), 1):\n",
        "    label = resultado['label']\n",
        "    score = resultado['score']\n",
        "\n",
        "    # Truncamos frase si es muy larga\n",
        "    frase_corta = frase if len(frase) <= 55 else frase[:52] + \"...\"\n",
        "\n",
        "    # Formato de salida\n",
        "    print(f\"\\n{i:2d}. '{frase_corta}'\")\n",
        "    print(f\"    → {label} (confianza: {score:.2%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "450-ga4-b2sa"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Análisis de Resultados\n",
        "\n",
        "Analicemos qué tan bien BETO maneja expresiones coloquiales argentinas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mkbABQjb2sa"
      },
      "outputs": [],
      "source": [
        "# Extraemos las frases con expresiones muy argentinas\n",
        "frases_argentinas = [\n",
        "    (\"Zafa, pero nada especial.\", \"NEU o POS\"),  # Esperado\n",
        "    (\"Está piola el lugar, volvería.\", \"POS\"),\n",
        "    (\"Qué garrón, tardaron una banda.\", \"NEG\"),\n",
        "    (\"Re copado todo, la rompieron.\", \"POS\"),\n",
        "    (\"Un bodrio total, no vayan.\", \"NEG\"),\n",
        "    (\"Qué buena onda la atención.\", \"POS\"),\n",
        "    (\"Malísima atención, mala onda.\", \"NEG\"),\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ANÁLISIS: EXPRESIONES ARGENTINAS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n¿BETO entiende jerga argentina?\\n\")\n",
        "\n",
        "# Clasificamos estas frases\n",
        "resultados_arg = clasificador([frase for frase, _ in frases_argentinas])\n",
        "\n",
        "aciertos = 0\n",
        "for (frase, esperado), resultado in zip(frases_argentinas, resultados_arg):\n",
        "    predicho = resultado['label']\n",
        "    score = resultado['score']\n",
        "\n",
        "    # Verificamos si coincide\n",
        "    correcto = predicho in esperado\n",
        "    marca = \"✓\" if correcto else \"?\"\n",
        "    if correcto:\n",
        "        aciertos += 1\n",
        "\n",
        "    print(f\"{marca} '{frase}'\")\n",
        "    print(f\"  Esperado: {esperado} | Predicho: {predicho} ({score:.2%})\\n\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Aciertos: {aciertos}/{len(frases_argentinas)}\")\n",
        "print(\"\\nObservación: BETO fue entrenado en tweets en español, por lo que\")\n",
        "print(\"maneja bien expresiones coloquiales. Sin embargo, jerga muy específica\")\n",
        "print(\"de Argentina puede ser un desafío. Fine-tuning con datos locales mejoraría esto.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCvrpJT7b2sb"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Comparación con Modelos Anteriores\n",
        "\n",
        "Comparemos conceptualmente lo que ganamos con transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SikQbD7sb2sc"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPARACIÓN: MODELOS CONSTRUIDOS VS. TRANSFORMERS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparacion = [\n",
        "    (\"Líneas de código\", \"~200-300 líneas\", \"3 líneas\"),\n",
        "    (\"Tiempo de desarrollo\", \"Horas/días\", \"Minutos\"),\n",
        "    (\"Datos requeridos\", \">1000 muestras\", \"0 (modelo ya entrenado)\"),\n",
        "    (\"Tiempo de entrenamiento\", \"Minutos/horas\", \"0 (ya entrenado)\"),\n",
        "    (\"Comprensión lingüística\", \"Limitada\", \"Profunda (pre-entrenado)\"),\n",
        "    (\"Manejo de jerga\", \"Depende del vocabulario\", \"Robusto\"),\n",
        "    (\"Contexto\", \"Limitado (BoW/LSTM)\", \"Bidireccional (BERT)\"),\n",
        "    (\"Parámetros\", \"Decenas/cientos\", \"110 millones\"),\n",
        "    (\"Accuracy esperado\", \"70-85%\", \"90-95%\"),\n",
        "    (\"Fine-tuning\", \"N/A\", \"Posible con pocos datos\"),\n",
        "    (\"Multilingüe\", \"Solo español\", \"Soporte nativo\"),\n",
        "    (\"Interpretabilidad\", \"Alta (pesos directos)\", \"Media (attention)\"),\n",
        "    (\"Recursos computacionales\", \"CPU suficiente\", \"GPU recomendada\"),\n",
        "    (\"Tamaño del modelo\", \"KB\", \"400+ MB\"),\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Aspecto':<30} | {'Nuestros Modelos':<25} | {'Transformers (BETO)':<25}\")\n",
        "print(\"-\"*85)\n",
        "for aspecto, nuestros, transformers in comparacion:\n",
        "    print(f\"{aspecto:<30} | {nuestros:<25} | {transformers:<25}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCLUSIÓN\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nLo que construimos en notebooks anteriores fue FUNDAMENTAL para:\")\n",
        "print(\"  1. Entender cómo funcionan las redes neuronales\")\n",
        "print(\"  2. Apreciar la complejidad que HuggingFace abstrae\")\n",
        "print(\"  3. Saber qué está pasando 'bajo el capó'\")\n",
        "print(\"  4. Debugging cuando algo falla\")\n",
        "print(\"\\nPero para producción y proyectos reales:\")\n",
        "print(\"  → SIEMPRE usar transfer learning con modelos pre-entrenados\")\n",
        "print(\"  → HuggingFace es el estándar de la industria\")\n",
        "print(\"  → No reinventar la rueda (años-persona de investigación)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkJsAYyMb2sc"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Explorando Otros Modelos en Español\n",
        "\n",
        "HuggingFace tiene múltiples modelos para español. Veamos algunos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-sXs3db2sd"
      },
      "source": [
        "### 10.1. RoBERTuito: Especializado en Twitter Español\n",
        "\n",
        "**RoBERTuito** es un modelo basado en RoBERTa entrenado en tweets en español:\n",
        "- Mejor manejo de lenguaje coloquial\n",
        "- Comprende jerga, emojis, hashtags\n",
        "- Ideal para redes sociales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAX0T0D8b2sd"
      },
      "outputs": [],
      "source": [
        "print(\"Cargando RoBERTuito...\\n\")\n",
        "\n",
        "# Cargamos pipeline con RoBERTuito\n",
        "clasificador_twitter = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"pysentimiento/robertuito-sentiment-analysis\"\n",
        ")\n",
        "\n",
        "print(\"RoBERTuito cargado.\")\n",
        "print(\"\\nProbando con frases estilo Twitter/redes sociales:\\n\")\n",
        "\n",
        "# Frases tipo tweet\n",
        "tweets = [\n",
        "    \"Jajaja re copado el lugar 😂👌\",\n",
        "    \"Nooo qué garrón mal servicio 😤\",\n",
        "    \"10/10 recomendadísimo 🔥\",\n",
        "    \"Naaa una estafa total 👎👎\",\n",
        "]\n",
        "\n",
        "resultados_twitter = clasificador_twitter(tweets)\n",
        "\n",
        "for tweet, resultado in zip(tweets, resultados_twitter):\n",
        "    print(f\"Tweet: '{tweet}'\")\n",
        "    print(f\"  → {resultado['label']} ({resultado['score']:.2%})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8M4ldOEb2sd"
      },
      "source": [
        "### 10.2. Comparación BETO vs. RoBERTuito\n",
        "\n",
        "Comparemos ambos modelos en las mismas frases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJr46VSxb2se"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"COMPARACIÓN: BETO vs. RoBERTuito\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "frases_test = [\n",
        "    \"Está piola el lugar, re tranquilo.\",\n",
        "    \"Qué bodrio, nunca más vuelvo.\",\n",
        "    \"La comida estaba bien, nada del otro mundo.\",\n",
        "]\n",
        "\n",
        "resultados_beto = clasificador(frases_test)\n",
        "resultados_robertuito = clasificador_twitter(frases_test)\n",
        "\n",
        "for i, frase in enumerate(frases_test):\n",
        "    print(f\"\\nFrase: '{frase}'\")\n",
        "    print(f\"  BETO:       {resultados_beto[i]['label']:8s} ({resultados_beto[i]['score']:.2%})\")\n",
        "    print(f\"  RoBERTuito: {resultados_robertuito[i]['label']:8s} ({resultados_robertuito[i]['score']:.2%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Observaciones:\")\n",
        "print(\"  - BETO: Más formal, entrenado en Wikipedia\")\n",
        "print(\"  - RoBERTuito: Mejor con jerga y lenguaje informal\")\n",
        "print(\"  - Elección depende del dominio de tu aplicación\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "651q2FiQb2sg"
      },
      "source": [
        "---\n",
        "\n",
        "## 11. Casos Difíciles: Límites de los Modelos\n",
        "\n",
        "Incluso los transformers tienen límites. Veamos casos desafiantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfHIK-X3b2sg"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"CASOS DIFÍCILES: IRONÍA, SARCASMO, AMBIGÜEDAD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "casos_dificiles = [\n",
        "    # Ironía\n",
        "    \"Buenísimo, justo lo que necesitaba, tardar 3 horas.\",\n",
        "\n",
        "    # Sarcasmo\n",
        "    \"Claro, excelente idea cobrarme el doble.\",\n",
        "\n",
        "    # Sentimiento mixto\n",
        "    \"La comida excelente pero el servicio pésimo.\",\n",
        "\n",
        "    # Ambigüedad\n",
        "    \"Interesante experiencia.\",\n",
        "\n",
        "    # Negación doble\n",
        "    \"No estuvo nada mal.\",\n",
        "]\n",
        "\n",
        "print(\"\\nProbando BETO con casos desafiantes:\\n\")\n",
        "\n",
        "resultados_dificiles = clasificador(casos_dificiles)\n",
        "\n",
        "for caso, resultado in zip(casos_dificiles, resultados_dificiles):\n",
        "    print(f\"Frase: '{caso}'\")\n",
        "    print(f\"  BETO dice: {resultado['label']} ({resultado['score']:.2%})\")\n",
        "    print(f\"  Nota: ¿Capturó el matiz?\\n\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Observaciones:\")\n",
        "print(\"  - Ironía/sarcasmo: Muy difícil incluso para humanos sin contexto\")\n",
        "print(\"  - Sentimiento mixto: Los modelos devuelven una sola clase\")\n",
        "print(\"  - Ambigüedad: Score bajo indica incertidumbre del modelo\")\n",
        "print(\"\\nSoluciones:\")\n",
        "print(\"  → Fine-tuning con datos específicos del dominio\")\n",
        "print(\"  → Modelos multiclase para sentimientos mixtos\")\n",
        "print(\"  → Features adicionales (emojis, contexto conversacional)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_friITEcb2sg"
      },
      "source": [
        "---\n",
        "\n",
        "## 12. Próximos Pasos: Fine-tuning\n",
        "\n",
        "Lo que vimos en este notebook es **inferencia** (usar modelo ya entrenado). El siguiente paso es **fine-tuning**: adaptar el modelo a tu dataset específico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYayL9Zb2sh"
      },
      "source": [
        "### ¿Cuándo hacer fine-tuning?\n",
        "\n",
        "**Usar modelo out-of-the-box:**\n",
        "- Tarea general (sentiment analysis estándar)\n",
        "- No tenés datos etiquetados\n",
        "- Prototipado rápido\n",
        "- Precisión suficiente (>85%)\n",
        "\n",
        "**Hacer fine-tuning:**\n",
        "- Dominio muy específico (medicina, legal, finanzas)\n",
        "- Jerga particular (ej: argentinismos)\n",
        "- Necesitás >90% accuracy\n",
        "- Tenés dataset etiquetado (>500 muestras)\n",
        "\n",
        "### Proceso de fine-tuning (próximas semanas del curso):\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "# 1. Cargar modelo pre-entrenado\n",
        "modelo = AutoModelForSequenceClassification.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "\n",
        "# 2. Preparar dataset\n",
        "train_dataset = tokenize_and_encode(train_texts, train_labels)\n",
        "eval_dataset = tokenize_and_encode(eval_texts, eval_labels)\n",
        "\n",
        "# 3. Configurar entrenamiento\n",
        "trainer = Trainer(\n",
        "    model=modelo,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# 4. Fine-tune\n",
        "trainer.train()\n",
        "```\n",
        "\n",
        "**Esto lo verán en detalle en las próximas semanas del curso.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84D182s-b2sh"
      },
      "source": [
        "---\n",
        "\n",
        "## Guía Teórico-Conceptual\n",
        "\n",
        "### 1. Arquitectura Transformer: Conceptos Clave\n",
        "\n",
        "**Attention Mechanism (Self-Attention):**\n",
        "\n",
        "La innovación central de los transformers. Permite que cada palabra \"preste atención\" a todas las demás:\n",
        "\n",
        "```\n",
        "Frase: \"El banco está cerrado\"\n",
        "\n",
        "Self-attention permite que \"banco\" mire:\n",
        "  - \"El\" → Artículo (bajo peso)\n",
        "  - \"está\" → Verbo estado (medio peso)\n",
        "  - \"cerrado\" → ¡Alto peso! Disambigua \"banco\" (institución vs. asiento)\n",
        "```\n",
        "\n",
        "**Fórmula simplificada:**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Donde:\n",
        "- **Q (Query)**: \"¿Qué busco?\"\n",
        "- **K (Key)**: \"¿Qué ofrezco?\"\n",
        "- **V (Value)**: \"¿Qué información tengo?\"\n",
        "\n",
        "**Multi-Head Attention:**\n",
        "- Múltiples attention en paralelo\n",
        "- Cada \"cabeza\" aprende patrones diferentes\n",
        "- BERT tiene 12 capas × 12 cabezas = 144 atenciones\n",
        "\n",
        "**Ventajas sobre LSTM:**\n",
        "1. **Paralelización**: Todas las palabras se procesan simultáneamente\n",
        "2. **Dependencias largas**: Cualquier palabra puede atender a cualquier otra\n",
        "3. **Interpretabilidad**: Los pesos de attention son visualizables\n",
        "\n",
        "### 2. BERT vs. GPT: Encoder vs. Decoder\n",
        "\n",
        "| Aspecto | BERT | GPT |\n",
        "|---------|------|-----|\n",
        "| Arquitectura | Encoder (bidirectional) | Decoder (unidirectional) |\n",
        "| Lectura | Lee toda la oración | Lee izquierda a derecha |\n",
        "| Pre-entrenamiento | Masked LM | Next token prediction |\n",
        "| Mejor para | Clasificación, NER, QA | Generación de texto |\n",
        "| Ejemplos | BERT, RoBERTa, BETO | GPT-2, GPT-3, GPT-4 |\n",
        "\n",
        "**Masked Language Modeling (BERT):**\n",
        "```\n",
        "Input:  \"El [MASK] está cerrado\"\n",
        "Output: \"banco\" (con probabilidad 0.8)\n",
        "```\n",
        "\n",
        "**Next Token Prediction (GPT):**\n",
        "```\n",
        "Input:  \"El banco está\"\n",
        "Output: \"cerrado\" (con probabilidad 0.6)\n",
        "```\n",
        "\n",
        "### 3. Tokenización en Transformers\n",
        "\n",
        "Los transformers usan **sub-word tokenization** (WordPiece, BPE):\n",
        "\n",
        "**Ventajas sobre tokenización por palabra:**\n",
        "```\n",
        "Palabra completa:\n",
        "\"recomendabilísimo\" → <UNK> (desconocida)\n",
        "\n",
        "WordPiece:\n",
        "\"recomendabilísimo\" → [\"recomienda\", \"##bili\", \"##simo\"]\n",
        "```\n",
        "\n",
        "Esto permite:\n",
        "- Vocabulario finito (30k tokens)\n",
        "- Manejar palabras nuevas\n",
        "- Capturar morfología (prefijos, sufijos)\n",
        "\n",
        "### 4. Pre-entrenamiento: La Fase Cara\n",
        "\n",
        "**Corpus de pre-entrenamiento:**\n",
        "- BERT (inglés): Wikipedia (2.5B palabras) + BookCorpus (800M palabras)\n",
        "- BETO (español): Wikipedia español (3B palabras)\n",
        "- GPT-3: Common Crawl (570GB de texto)\n",
        "\n",
        "**Recursos computacionales:**\n",
        "- BERT: 4 días en 16 TPUs (~$7k)\n",
        "- GPT-3: Semanas en clusters masivos (~$12M)\n",
        "\n",
        "**Por esto NO entrenamos desde cero:**\n",
        "- Costo prohibitivo\n",
        "- Expertise técnico alto\n",
        "- Modelos ya disponibles\n",
        "\n",
        "### 5. Fine-tuning: La Fase Accesible\n",
        "\n",
        "**Proceso:**\n",
        "1. **Congelar capas inferiores**: Mantener conocimiento lingüístico general\n",
        "2. **Entrenar capas superiores**: Adaptar a tarea específica\n",
        "3. **Pocos epochs**: 2-5 típicamente\n",
        "4. **Learning rate bajo**: 1e-5 a 5e-5\n",
        "\n",
        "**Datos necesarios:**\n",
        "- Clasificación: 500-1000 muestras\n",
        "- NER: 1000-5000 entidades\n",
        "- QA: 500-2000 pares pregunta-respuesta\n",
        "\n",
        "**Tiempo:**\n",
        "- GPU T4 (Google Colab): 30-60 minutos\n",
        "- GPU A100: 5-10 minutos\n",
        "\n",
        "### 6. HuggingFace Model Hub\n",
        "\n",
        "**Estadísticas (2025):**\n",
        "- 500,000+ modelos\n",
        "- 100+ idiomas\n",
        "- Todas las arquitecturas: BERT, GPT, T5, LLaMA, etc.\n",
        "\n",
        "**Modelos destacados para español:**\n",
        "1. **BETO** (`dccuchile/bert-base-spanish-wwm-cased`)\n",
        "   - BERT base en español\n",
        "   - 110M parámetros\n",
        "   - General purpose\n",
        "\n",
        "2. **RoBERTuito** (`pysentimiento/robertuito-*`)\n",
        "   - RoBERTa en tweets español\n",
        "   - Jerga, emojis, hashtags\n",
        "   - Mejor para redes sociales\n",
        "\n",
        "3. **MarIA** (`PlanTL-GOB-ES/roberta-base-bne`)\n",
        "   - RoBERTa español\n",
        "   - Corpus BNE (Biblioteca Nacional)\n",
        "   - Muy robusto\n",
        "\n",
        "4. **mBERT** (`bert-base-multilingual-cased`)\n",
        "   - 104 idiomas incluyendo español\n",
        "   - Útil para tareas multilingües\n",
        "\n",
        "### 7. Limitaciones de Transformers\n",
        "\n",
        "**1. Longitud de secuencia:**\n",
        "- BERT: Máximo 512 tokens\n",
        "- Documentos largos requieren truncamiento o chunking\n",
        "- Soluciones: Longformer, BigBird (hasta 4096 tokens)\n",
        "\n",
        "**2. Recursos computacionales:**\n",
        "- Modelos grandes (400MB - 2GB)\n",
        "- Inferencia más lenta que modelos simples\n",
        "- GPU recomendada para fine-tuning\n",
        "\n",
        "**3. Interpretabilidad:**\n",
        "- 110M parámetros son difíciles de interpretar\n",
        "- Attention weights ayudan pero no explican todo\n",
        "- \"Caja menos negra\" que antes, pero aún opaco\n",
        "\n",
        "**4. Sesgos del pre-entrenamiento:**\n",
        "- Heredan sesgos del corpus (Wikipedia, internet)\n",
        "- Requiere cuidado en aplicaciones sensibles\n",
        "- Fine-tuning puede amplificar sesgos\n",
        "\n",
        "**5. Overfitting en fine-tuning:**\n",
        "- Con pocos datos, puede sobreajustar\n",
        "- Requiere regularización, early stopping\n",
        "- Data augmentation ayuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6E85KlPb2si"
      },
      "source": [
        "---\n",
        "\n",
        "## Preguntas y Respuestas para Estudio\n",
        "\n",
        "### Preguntas Conceptuales\n",
        "\n",
        "**1. ¿Qué es transfer learning en NLP y por qué es tan importante?**\n",
        "\n",
        "*Respuesta:* Transfer learning es usar un modelo pre-entrenado en un corpus masivo (Wikipedia, Common Crawl) y adaptarlo a una tarea específica con pocos datos. Es importante porque:\n",
        "1. El modelo ya \"entiende\" lenguaje (gramática, semántica)\n",
        "2. Solo necesitás 500-1000 muestras en vez de millones\n",
        "3. Ahorras meses de entrenamiento y miles de dólares\n",
        "4. Obtenés estado del arte sin expertise en deep learning\n",
        "\n",
        "**2. ¿Cuál es la principal innovación de los transformers sobre LSTM?**\n",
        "\n",
        "*Respuesta:* **Self-attention mechanism**. En lugar de procesar secuencialmente (palabra por palabra), los transformers permiten que todas las palabras se \"vean\" entre sí simultáneamente. Esto:\n",
        "- Permite paralelización (mucho más rápido)\n",
        "- Captura dependencias largas mejor\n",
        "- Es más interpretable (visualizar attention weights)\n",
        "\n",
        "**3. ¿Qué es un pipeline en HuggingFace y qué problema resuelve?**\n",
        "\n",
        "*Respuesta:* Un pipeline es una abstracción de alto nivel que encapsula:\n",
        "1. **Tokenización**: Texto → tokens\n",
        "2. **Modelo**: Tokens → embeddings → predicciones\n",
        "3. **Post-procesamiento**: Predicciones → formato legible\n",
        "\n",
        "Resuelve el problema de complejidad: en lugar de manejar manualmente tokenizers, modelos y configuraciones, el pipeline lo hace en una línea de código.\n",
        "\n",
        "**4. ¿Por qué BERT es \"bidirectional\" y GPT es \"unidirectional\"?**\n",
        "\n",
        "*Respuesta:*\n",
        "- **BERT**: Lee toda la oración en ambas direcciones. En \"El banco está cerrado\", \"banco\" ve tanto \"El\" (izquierda) como \"cerrado\" (derecha)\n",
        "- **GPT**: Solo lee de izquierda a derecha. En \"El banco está\", solo puede usar \"El banco\" para predecir \"está\"\n",
        "\n",
        "Por eso BERT es mejor para clasificación/comprensión y GPT para generación.\n",
        "\n",
        "**5. ¿Qué es Masked Language Modeling y por qué es efectivo para pre-entrenamiento?**\n",
        "\n",
        "*Respuesta:* MLM oculta aleatoriamente el 15% de las palabras y el modelo debe predecirlas: \"El [MASK] está cerrado\" → \"banco\". Es efectivo porque:\n",
        "- Fuerza al modelo a entender contexto bidireccional\n",
        "- Es auto-supervisado (no requiere etiquetas humanas)\n",
        "- Aprende representaciones profundas del lenguaje\n",
        "\n",
        "### Preguntas Técnicas\n",
        "\n",
        "**6. En el código, ¿qué hace exactamente `pipeline(\"sentiment-analysis\", model=\"...\")` internamente?**\n",
        "\n",
        "*Respuesta:*\n",
        "1. Descarga el modelo y tokenizer desde HuggingFace Hub (si no está en caché)\n",
        "2. Carga el modelo pre-entrenado en memoria\n",
        "3. Configura el tokenizer apropiado (WordPiece para BERT)\n",
        "4. Crea un pipeline que encadena: tokenización → modelo → argmax → label\n",
        "\n",
        "**7. ¿Por qué los modelos transformer son tan grandes (400MB+)?**\n",
        "\n",
        "*Respuesta:*\n",
        "- BERT base: 110M parámetros × 4 bytes (float32) = 440MB\n",
        "- Cada parámetro es un peso de la red neuronal\n",
        "- 12 capas × 12 attention heads × embeddings × FFN = muchos parámetros\n",
        "\n",
        "Comparado con nuestro MLP (280 parámetros = 1KB), es ~400,000 veces más grande.\n",
        "\n",
        "**8. ¿Qué significa el \"score\" en el resultado del pipeline?**\n",
        "\n",
        "*Respuesta:* Es la probabilidad (softmax) que el modelo asigna a esa clase:\n",
        "- score=0.95 → 95% seguro de la predicción\n",
        "- score=0.55 → Apenas seguro (predicción incierta)\n",
        "\n",
        "Útil para:\n",
        "- Filtrar predicciones inciertas (threshold > 0.7)\n",
        "- Priorizar casos para revisión humana\n",
        "\n",
        "**9. ¿Cuál es la diferencia entre `model` y `pipeline` en HuggingFace?**\n",
        "\n",
        "*Respuesta:*\n",
        "- **Pipeline**: API de alto nivel, maneja todo automáticamente, fácil de usar\n",
        "- **Model**: API de bajo nivel, control total, requiere manejar tokenización manualmente\n",
        "\n",
        "```python\n",
        "# Pipeline (simple)\n",
        "resultado = pipeline(\"sentiment-analysis\")(\"texto\")\n",
        "\n",
        "# Model (control manual)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"beto\")\n",
        "model = AutoModel.from_pretrained(\"beto\")\n",
        "inputs = tokenizer(\"texto\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "**10. ¿Por qué BETO y RoBERTuito dan resultados diferentes en la misma frase?**\n",
        "\n",
        "*Respuesta:*\n",
        "1. **Corpus de pre-entrenamiento diferente**:\n",
        "   - BETO: Wikipedia (formal)\n",
        "   - RoBERTuito: Twitter (informal)\n",
        "2. **Fine-tuning diferente**: Diferentes datos de sentiment analysis\n",
        "3. **Vocabulario**: RoBERTuito conoce mejor jerga, emojis\n",
        "\n",
        "La elección depende del dominio de tu aplicación.\n",
        "\n",
        "### Preguntas de Aplicación\n",
        "\n",
        "**11. Tenés 200 reseñas etiquetadas de un restaurante. ¿Usarías el modelo as-is o harías fine-tuning?**\n",
        "\n",
        "*Respuesta:* **Depende:**\n",
        "- **As-is si**: El modelo out-of-the-box ya da >85% accuracy, el dominio es general\n",
        "- **Fine-tuning si**:\n",
        "  - Necesitás >90% accuracy\n",
        "  - Hay vocabulario muy específico del restaurante\n",
        "  - 200 muestras son borderline (mínimo recomendado es 500), pero podés intentar con data augmentation\n",
        "\n",
        "**12. ¿Cómo manejarías un documento de 2000 palabras con BERT (límite: 512 tokens)?**\n",
        "\n",
        "*Respuesta:* Opciones:\n",
        "1. **Truncamiento**: Tomar primeros 512 tokens (pierde info del final)\n",
        "2. **Chunking + agregación**: Dividir en chunks de 512, clasificar cada uno, agregar (voting, promedio)\n",
        "3. **Extractivo**: Identificar sección clave (ej: resumen, intro) y clasificar eso\n",
        "4. **Modelo especializado**: Usar Longformer o BigBird (hasta 4096 tokens)\n",
        "\n",
        "**13. En producción, necesitás clasificar 10,000 textos/segundo. ¿Transformers son apropiados?**\n",
        "\n",
        "*Respuesta:* Probablemente **no** para latencia ultra-baja:\n",
        "- BERT inference: ~50-100ms/texto en GPU\n",
        "- 10k/seg = 0.1ms/texto (100x más rápido requerido)\n",
        "\n",
        "**Soluciones:**\n",
        "1. **Distillation**: DistilBERT (60% más rápido, 97% accuracy)\n",
        "2. **Quantization**: INT8 en vez de FP32 (4x más rápido)\n",
        "3. **Caching**: Cachear predicciones para textos comunes\n",
        "4. **Ensemble**: Modelo simple (Naive Bayes) para mayoría, BERT solo para casos inciertos\n",
        "\n",
        "**14. ¿Cómo usarías HuggingFace para un chatbot que genera respuestas?**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "# Usar modelo generativo (GPT)\n",
        "from transformers import pipeline\n",
        "\n",
        "generador = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "respuesta = generador(\n",
        "    \"Usuario: Hola, ¿cómo estás?\\nBot:\",\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "```\n",
        "\n",
        "Para español, usar modelos como `DeepESP/gpt2-spanish`.\n",
        "\n",
        "**15. Diseñá un sistema de moderación de contenido con transformers.**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "# Pipeline multimodelo\n",
        "\n",
        "# 1. Clasificación de toxicidad\n",
        "toxicidad = pipeline(\"text-classification\", model=\"modelo-toxicidad-es\")\n",
        "\n",
        "# 2. Detección de spam\n",
        "spam = pipeline(\"text-classification\", model=\"modelo-spam-es\")\n",
        "\n",
        "# 3. NER para detectar info personal\n",
        "ner = pipeline(\"ner\", model=\"modelo-ner-es\")\n",
        "\n",
        "def moderar(texto):\n",
        "    # Análisis paralelo\n",
        "    es_toxico = toxicidad(texto)[0]['score'] > 0.7\n",
        "    es_spam = spam(texto)[0]['score'] > 0.8\n",
        "    tiene_pii = any(ent['entity'] == 'PER' for ent in ner(texto))\n",
        "    \n",
        "    if es_toxico or es_spam:\n",
        "        return \"RECHAZAR\"\n",
        "    elif tiene_pii:\n",
        "        return \"ADVERTIR\"\n",
        "    else:\n",
        "        return \"APROBAR\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6YNhdgpb2sk"
      },
      "source": [
        "---\n",
        "\n",
        "## Ejercicios Propuestos\n",
        "\n",
        "### Ejercicio 1: Explorar el Model Hub\n",
        "Visita https://huggingface.co/models y busca modelos para:\n",
        "- Sentiment analysis en español\n",
        "- NER en español\n",
        "- Traducción es→en\n",
        "\n",
        "Probá al menos 3 modelos diferentes y compará resultados.\n",
        "\n",
        "### Ejercicio 2: Análisis de Confianza\n",
        "Filtrá las predicciones del modelo para mostrar solo aquellas con score > 0.8. ¿Cuántas frases quedan? ¿Las predicciones de baja confianza tienen algo en común?\n",
        "\n",
        "### Ejercicio 3: Comparación Cuantitativa\n",
        "Evalúa BETO vs. RoBERTuito en un conjunto de 50 frases etiquetadas manualmente:\n",
        "- Calcula accuracy, precision, recall\n",
        "- Identifica en qué tipo de frases cada modelo es mejor\n",
        "\n",
        "### Ejercicio 4: NER con Transformers\n",
        "Usa un pipeline de NER para extraer entidades de texto:\n",
        "```python\n",
        "ner = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\")\n",
        "```\n",
        "Probá con textos sobre geografía, personas famosas, organizaciones.\n",
        "\n",
        "### Ejercicio 5: Límites del Modelo\n",
        "Crea 10 frases diseñadas para \"engañar\" al modelo (ironía, sarcasmo, negaciones dobles). ¿Qué porcentaje clasifica incorrectamente? ¿Ves patrones en los errores?\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "Este notebook marca el **fin del módulo introductorio** y el **inicio del programa principal** del curso.\n",
        "\n",
        "### Lo que aprendimos:\n",
        "\n",
        "1. **Transfer learning**: El paradigma moderno de NLP\n",
        "2. **HuggingFace Transformers**: La librería estándar de la industria\n",
        "3. **Pipelines**: Abstracciones de alto nivel para inferencia\n",
        "4. **Modelos en español**: BETO, RoBERTuito, MarIA\n",
        "5. **Limitaciones**: Casos difíciles, sesgos, recursos\n",
        "\n",
        "### El viaje hasta aquí:\n",
        "\n",
        "```\n",
        "Notebook 1-2: Baselines clásicos (sklearn, Naive Bayes)\n",
        "     ↓\n",
        "Notebook 3: Fundamentos (Perceptrón desde cero)\n",
        "     ↓\n",
        "Notebook 4: Redes profundas (MLP con PyTorch)\n",
        "     ↓\n",
        "Notebook 5: Secuencias (LSTM con Keras)\n",
        "     ↓\n",
        "Notebook 6: Estado del arte (Transformers con HuggingFace) ← ESTÁS AQUÍ\n",
        "```\n",
        "\n",
        "### ¿Qué sigue en el curso?\n",
        "\n",
        "**Semanas 1-3: Fundamentos de transformers**\n",
        "- Arquitectura en detalle\n",
        "- Tokenización avanzada\n",
        "- Preparación de datos\n",
        "\n",
        "**Semanas 4-5: Fine-tuning**\n",
        "- Clasificación binaria y multiclase\n",
        "- Optimización de hiperparámetros\n",
        "- Evaluación rigurosa\n",
        "\n",
        "**Semana 6: NER y token classification**\n",
        "\n",
        "**Semanas 7-8: Modelos generativos**\n",
        "- GPT para generación\n",
        "- Summarization\n",
        "- Question Answering\n",
        "\n",
        "**Semanas 9-11: Proyecto integrador**\n",
        "\n",
        "### Reflexión final\n",
        "\n",
        "Los 5 notebooks anteriores fueron **fundamentales** para entender cómo funcionan las redes neuronales. Ahora que vimos la potencia de HuggingFace, apreciás:\n",
        "\n",
        "- El valor de abstracciones bien diseñadas\n",
        "- Por qué transfer learning revolucionó NLP\n",
        "- Que \"hacer deep learning\" hoy es usar herramientas correctamente, no programar todo desde cero\n",
        "\n",
        "Pero también entendés **qué está pasando bajo el capó**, lo cual te diferencia de alguien que solo sabe usar pipelines sin comprensión profunda.\n",
        "\n",
        "**Próximo paso:** El Notebook 7 (Embeddings Semánticos) es material complementario para proyectos avanzados. Luego continúa con el programa principal del curso.\n",
        "\n",
        "---\n",
        "\n",
        "*Este material fue desarrollado con fines educativos para la Tecnicatura en Ciencia de Datos del IFTS.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}