{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF3Uq6zLb3Os"
      },
      "source": [
        "# Redes LSTM con Keras para Procesamiento Secuencial de Texto\n",
        "\n",
        "**Materiales desarrollados por Matías Barreto, 2025**\n",
        "\n",
        "**Tecnicatura en Ciencia de Datos - IFTS**\n",
        "\n",
        "**Asignatura:** Procesamiento de Lenguaje Natural\n",
        "\n",
        "---\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Hasta ahora trabajamos con **Bag of Words**: representamos texto ignorando el orden de las palabras. Esto tiene una limitación crítica: \"No me gusta\" y \"Me gusta no\" tienen la misma representación, pero significados opuestos.\n",
        "\n",
        "Las **redes recurrentes** (RNN) procesan texto como **secuencia ordenada**, capturando contexto y dependencias temporales. Las **LSTM** (Long Short-Term Memory) son una variante de RNN diseñada para recordar información relevante a largo plazo.\n",
        "\n",
        "### ¿Qué cambia con LSTM?\n",
        "\n",
        "1. **Procesa secuencialmente**: Palabra por palabra, manteniendo memoria del contexto\n",
        "2. **Captura orden**: \"No me gusta\" ≠ \"Me gusta no\"\n",
        "3. **Memoria selectiva**: Recuerda información relevante, olvida ruido\n",
        "4. **Dependencias largas**: Puede relacionar palabras distantes en el texto\n",
        "\n",
        "### Objetivos de aprendizaje\n",
        "\n",
        "1. Comprender la arquitectura de redes recurrentes (RNN/LSTM)\n",
        "2. Procesar texto como secuencias con Tokenizer de Keras\n",
        "3. Aplicar padding para uniformizar longitudes\n",
        "4. Implementar embeddings aprendidos (vs. pre-entrenados)\n",
        "5. Entrenar una LSTM para clasificación de sentimientos\n",
        "6. Comparar rendimiento con MLP feedforward\n",
        "\n",
        "### ¿Por qué Keras?\n",
        "\n",
        "Keras (ahora integrado en TensorFlow) es:\n",
        "- Más simple que PyTorch para prototipado rápido\n",
        "- API de alto nivel, menos código\n",
        "- Excelente para RNN/LSTM (Sequential API muy clara)\n",
        "- Ampliamente usado en industria\n",
        "\n",
        "**Nota:** En el ecosistema de transformers (HuggingFace) se usa principalmente PyTorch, pero conocer ambos frameworks es valioso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEoht3o-b3Oy"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Instalación e Importación de Librerías\n",
        "\n",
        "En Google Colab, TensorFlow/Keras ya viene instalado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3wortB2b3Oz",
        "outputId": "90ca4bca-6c35-41c5-a3c0-746823e375c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow versión: 2.19.0\n",
            "Keras versión: 3.10.0\n",
            "GPU disponible: True\n"
          ]
        }
      ],
      "source": [
        "# Si necesitás instalar TensorFlow (descomentá)\n",
        "# !pip install tensorflow\n",
        "\n",
        "# NumPy para operaciones numéricas\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow y Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Keras Sequential: API para apilar capas\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Capas de Keras\n",
        "# Embedding: Capa que aprende representaciones vectoriales de palabras\n",
        "# LSTM: Capa recurrente con memoria a largo plazo\n",
        "# Dense: Capa fully connected (como en MLP)\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Preprocesamiento de texto\n",
        "# Tokenizer: Convierte texto en secuencias de enteros\n",
        "# pad_sequences: Uniformiza longitud de secuencias\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Matplotlib para visualizaciones\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fijamos semillas para reproducibilidad\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow versión: {tf.__version__}\")\n",
        "print(f\"Keras versión: {keras.__version__}\")\n",
        "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbei_z2Yb3O2"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Dataset: Análisis de Sentimiento en Español Rioplatense\n",
        "\n",
        "Usamos el mismo corpus que en el notebook de PyTorch para comparar resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNKzgntbb3O3",
        "outputId": "bf45b589-e9e7-4622-b877-d86752c25436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: 20 frases\n",
            "Distribución: [10 10]\n",
            "\n",
            "Ejemplos:\n",
            "[+] La verdad, este lugar está bárbaro. Muy recomendable.\n",
            "[-] Una porquería de servicio, nunca más vuelvo.\n"
          ]
        }
      ],
      "source": [
        "# Corpus de frases en español rioplatense\n",
        "# Etiqueta: 1 = Positivo, 0 = Negativo\n",
        "frases = [\n",
        "    # Positivas\n",
        "    \"La verdad, este lugar está bárbaro. Muy recomendable.\",\n",
        "    \"Qué buena onda la atención, volvería sin dudarlo.\",\n",
        "    \"Me encantó la comida, aunque la música estaba muy fuerte.\",\n",
        "    \"Todo excelente. Atención de diez.\",\n",
        "    \"Muy conforme con el resultado final.\",\n",
        "    \"Superó mis expectativas, gracias.\",\n",
        "    \"El mejor asado que probé en mucho tiempo.\",\n",
        "    \"Excelente relación precio-calidad, muy recomendable.\",\n",
        "    \"La atención fue impecable, muy atentos.\",\n",
        "    \"Me gustó mucho el ambiente tranquilo.\",\n",
        "\n",
        "    # Negativas\n",
        "    \"Una porquería de servicio, nunca más vuelvo.\",\n",
        "    \"El envío fue lento y el producto llegó dañado. Qué desastre.\",\n",
        "    \"Qué estafa, me arrepiento de haber comprado.\",\n",
        "    \"No me gustó para nada la experiencia.\",\n",
        "    \"No lo recomiendo, mala calidad.\",\n",
        "    \"Malísima atención, el mozo tenía mala onda.\",\n",
        "    \"Tardaron dos horas en entregar, llegó todo frío.\",\n",
        "    \"Me cobraron de más y encima se hicieron los giles.\",\n",
        "    \"La carne estaba pasada, casi no se podía comer.\",\n",
        "    \"Pésima experiencia, no vuelvo más.\"\n",
        "]\n",
        "\n",
        "# Etiquetas: 1=positivo, 0=negativo\n",
        "etiquetas = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # 10 positivas\n",
        "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # 10 negativas\n",
        "\n",
        "print(f\"Corpus: {len(frases)} frases\")\n",
        "print(f\"Distribución: {np.bincount(etiquetas)}\")\n",
        "print(f\"\\nEjemplos:\")\n",
        "print(f\"[+] {frases[0]}\")\n",
        "print(f\"[-] {frases[10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSDHgOiab3O4"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Tokenización: De Texto a Secuencias de Enteros\n",
        "\n",
        "El `Tokenizer` de Keras:\n",
        "1. Construye un **vocabulario** (palabra → índice)\n",
        "2. Convierte cada **texto en secuencia de enteros**\n",
        "3. Maneja palabras fuera del vocabulario (OOV - Out Of Vocabulary)\n",
        "\n",
        "### Ejemplo:\n",
        "```\n",
        "Vocabulario: {\"me\": 1, \"gusta\": 2, \"no\": 3, \"comida\": 4}\n",
        "\n",
        "Texto: \"Me gusta la comida\" → [1, 2, 5, 4]  # 5 = OOV token para \"la\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3XQc5afb3O7",
        "outputId": "52db96cc-1137-438a-b1ce-2d8d90be6650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario: 97 palabras\n",
            "\n",
            "Primeras 15 palabras del vocabulario:\n",
            "{'<OOV>': 1, 'la': 2, 'el': 3, 'muy': 4, 'me': 5, 'atención': 6, 'de': 7, 'no': 8, 'qué': 9, 'más': 10, 'recomendable': 11, 'onda': 12, 'estaba': 13, 'todo': 14, 'excelente': 15}\n",
            "\n",
            "Ejemplo de secuencia:\n",
            "Texto original: 'La verdad, este lugar está bárbaro. Muy recomendable.'\n",
            "Secuencia: [2, 27, 28, 29, 30, 31, 4, 11]\n",
            "Longitud: 8 tokens\n",
            "Reconstrucción: 'la verdad este lugar está bárbaro muy recomendable'\n"
          ]
        }
      ],
      "source": [
        "# Configuración del tokenizer\n",
        "# oov_token: Token especial para palabras no vistas (Out Of Vocabulary)\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "\n",
        "# Paso 1: Construir el vocabulario a partir de las frases\n",
        "# fit_on_texts() analiza el corpus y crea el mapeo palabra→índice\n",
        "tokenizer.fit_on_texts(frases)\n",
        "\n",
        "# Información del vocabulario\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 para el índice 0 (padding)\n",
        "print(f\"Tamaño del vocabulario: {vocab_size} palabras\")\n",
        "print(f\"\\nPrimeras 15 palabras del vocabulario:\")\n",
        "print(dict(list(tokenizer.word_index.items())[:15]))\n",
        "\n",
        "# Paso 2: Convertir textos a secuencias de enteros\n",
        "# texts_to_sequences() reemplaza cada palabra por su índice\n",
        "secuencias = tokenizer.texts_to_sequences(frases)\n",
        "\n",
        "print(f\"\\nEjemplo de secuencia:\")\n",
        "print(f\"Texto original: '{frases[0]}'\")\n",
        "print(f\"Secuencia: {secuencias[0]}\")\n",
        "print(f\"Longitud: {len(secuencias[0])} tokens\")\n",
        "\n",
        "# Reconstrucción inversa (para verificar)\n",
        "# word_index invertido: índice → palabra\n",
        "index_word = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "reconstruido = ' '.join([index_word.get(idx, '<OOV>') for idx in secuencias[0]])\n",
        "print(f\"Reconstrucción: '{reconstruido}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCOouxB7b3O9"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Padding: Uniformizando Longitudes\n",
        "\n",
        "Las redes neuronales requieren inputs de tamaño fijo, pero las frases tienen longitudes variables. **Padding** rellena secuencias cortas con ceros.\n",
        "\n",
        "### Estrategias de padding:\n",
        "\n",
        "**Pre-padding** (por defecto):\n",
        "```\n",
        "[5, 2, 8, 1] → [0, 0, 0, 0, 0, 0, 5, 2, 8, 1]  # Ceros al inicio\n",
        "```\n",
        "\n",
        "**Post-padding**:\n",
        "```\n",
        "[5, 2, 8, 1] → [5, 2, 8, 1, 0, 0, 0, 0, 0, 0]  # Ceros al final\n",
        "```\n",
        "\n",
        "**¿Cuál usar?**\n",
        "- **Pre-padding**: Mejor para RNN/LSTM (las palabras relevantes están al final)\n",
        "- **Post-padding**: Mejor para Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFguW9aDb3O-",
        "outputId": "40491bbf-9964-494e-b06f-4df91013ddbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estadísticas de longitudes:\n",
            "Mínima: 4 tokens\n",
            "Máxima: 11 tokens\n",
            "Promedio: 7.2 tokens\n",
            "\n",
            "Usaremos maxlen = 11\n",
            "\n",
            "Forma de X: (20, 11) (muestras x tokens)\n",
            "\n",
            "Ejemplo de secuencia con padding:\n",
            "Original (8 tokens): [2, 27, 28, 29, 30, 31, 4, 11]\n",
            "Con padding (11 tokens): [ 2 27 28 29 30 31  4 11  0  0  0]\n",
            "\n",
            "Nota: Los ceros al final son padding (relleno).\n"
          ]
        }
      ],
      "source": [
        "# Analizamos las longitudes de las secuencias\n",
        "longitudes = [len(seq) for seq in secuencias]\n",
        "print(\"Estadísticas de longitudes:\")\n",
        "print(f\"Mínima: {min(longitudes)} tokens\")\n",
        "print(f\"Máxima: {max(longitudes)} tokens\")\n",
        "print(f\"Promedio: {np.mean(longitudes):.1f} tokens\")\n",
        "\n",
        "# Definimos la longitud máxima (maxlen)\n",
        "# Opción 1: Usar el máximo observado\n",
        "maxlen = max(longitudes)\n",
        "print(f\"\\nUsaremos maxlen = {maxlen}\")\n",
        "\n",
        "# Aplicamos padding\n",
        "# padding='post': Agrega ceros al final (recomendado para LSTM)\n",
        "# truncating='post': Si una secuencia es > maxlen, trunca al final\n",
        "X = pad_sequences(secuencias, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "print(f\"\\nForma de X: {X.shape} (muestras x tokens)\")\n",
        "print(f\"\\nEjemplo de secuencia con padding:\")\n",
        "print(f\"Original ({len(secuencias[0])} tokens): {secuencias[0]}\")\n",
        "print(f\"Con padding ({maxlen} tokens): {X[0]}\")\n",
        "print(f\"\\nNota: Los ceros al final son padding (relleno).\")\n",
        "\n",
        "# Convertimos etiquetas a formato adecuado\n",
        "y = etiquetas.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX57J0ahb3O-"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Embeddings: Representaciones Vectoriales de Palabras\n",
        "\n",
        "### ¿Qué son los embeddings?\n",
        "\n",
        "Son **representaciones vectoriales densas** de palabras en un espacio continuo de baja dimensión (típicamente 50-300 dimensiones).\n",
        "\n",
        "### Ventajas sobre one-hot encoding:\n",
        "\n",
        "**One-hot** (vocabulario de 10,000 palabras):\n",
        "```\n",
        "\"gato\" → [0, 0, 1, 0, 0, ..., 0]  # Vector de 10,000 elementos\n",
        "\"perro\" → [0, 1, 0, 0, 0, ..., 0]\n",
        "```\n",
        "- Muy disperso (sparse)\n",
        "- No captura similitud semántica\n",
        "- Ineficiente computacionalmente\n",
        "\n",
        "**Embeddings** (dimensión 16):\n",
        "```\n",
        "\"gato\"  → [0.2, -0.5, 0.8, 0.1, ..., -0.3]  # Vector de 16 elementos\n",
        "\"perro\" → [0.3, -0.4, 0.7, 0.2, ..., -0.2]  # Similar a \"gato\"\n",
        "\"casa\"  → [-0.8, 0.6, -0.1, 0.9, ..., 0.5]  # Diferente\n",
        "```\n",
        "- Denso (compact)\n",
        "- Captura relaciones semánticas\n",
        "- Palabras similares tienen vectores similares\n",
        "\n",
        "### Embeddings aprendidos vs. pre-entrenados:\n",
        "\n",
        "**Aprendidos (lo que haremos aquí):**\n",
        "- Se inicializan aleatoriamente\n",
        "- Se ajustan durante el entrenamiento\n",
        "- Específicos para nuestra tarea\n",
        "- Requieren datos suficientes\n",
        "\n",
        "**Pre-entrenados (Word2Vec, GloVe, FastText):**\n",
        "- Entrenados en corpus masivos (Wikipedia, Common Crawl)\n",
        "- Capturan conocimiento lingüístico general\n",
        "- Útiles con pocos datos\n",
        "- Los vieron en módulos anteriores del curso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "harZSMRbb3O_"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Arquitectura LSTM: Conceptos Fundamentales\n",
        "\n",
        "### RNN Simple (Recurrent Neural Network)\n",
        "\n",
        "```\n",
        "t=1:  x₁ → [RNN] → h₁\n",
        "              ↓\n",
        "t=2:  x₂ → [RNN] → h₂\n",
        "              ↓\n",
        "t=3:  x₃ → [RNN] → h₃ → output\n",
        "```\n",
        "\n",
        "Cada paso recibe:\n",
        "- Input actual: $x_t$\n",
        "- Hidden state anterior: $h_{t-1}$\n",
        "\n",
        "Y produce:\n",
        "- Nuevo hidden state: $h_t = tanh(W_x x_t + W_h h_{t-1} + b)$\n",
        "\n",
        "### Problema de las RNN simples: Vanishing Gradient\n",
        "\n",
        "En secuencias largas, el gradiente se propaga multiplicativamente:\n",
        "$$\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\cdot \\frac{\\partial h_T}{\\partial h_{T-1}} \\cdot ... \\cdot \\frac{\\partial h_2}{\\partial h_1}$$\n",
        "\n",
        "Si cada derivada parcial < 1, el producto tiende a 0 (vanishing gradient).\n",
        "\n",
        "**Consecuencia:** La RNN \"olvida\" información temprana, no puede capturar dependencias largas.\n",
        "\n",
        "### LSTM (Long Short-Term Memory)\n",
        "\n",
        "LSTM resuelve esto con **celdas de memoria** y **compuertas** (gates):\n",
        "\n",
        "**Componentes de una celda LSTM:**\n",
        "1. **Forget gate** (f): ¿Qué información olvidar de la memoria?\n",
        "2. **Input gate** (i): ¿Qué información nueva agregar?\n",
        "3. **Cell state** (C): Memoria a largo plazo\n",
        "4. **Output gate** (o): ¿Qué información exponer?\n",
        "\n",
        "**Fórmulas (simplificadas):**\n",
        "```\n",
        "f_t = σ(W_f · [h_{t-1}, x_t] + b_f)    # Forget gate\n",
        "i_t = σ(W_i · [h_{t-1}, x_t] + b_i)    # Input gate\n",
        "C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C) # Candidate values\n",
        "C_t = f_t * C_{t-1} + i_t * C̃_t        # Update cell state\n",
        "o_t = σ(W_o · [h_{t-1}, x_t] + b_o)    # Output gate\n",
        "h_t = o_t * tanh(C_t)                   # Hidden state\n",
        "```\n",
        "\n",
        "**Intuición:**\n",
        "- La celda de memoria $C_t$ fluye directamente sin multiplicaciones repetidas\n",
        "- Esto previene vanishing gradient\n",
        "- Las compuertas aprenden qué recordar y qué olvidar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-9rvLezb3PA"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Construcción del Modelo LSTM con Keras\n",
        "\n",
        "Vamos a construir una red con:\n",
        "1. **Embedding layer**: Aprende representaciones de palabras\n",
        "2. **LSTM layer**: Procesa la secuencia con memoria\n",
        "3. **Dense layer**: Clasificación binaria (positivo/negativo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "R7dSVV1bb3PA",
        "outputId": "0ac0e0f3-c92b-4348-8e05-0679883853cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo LSTM creado:\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Explicación de las capas:\n",
            "----------------------------------------------------------------------\n",
            "1. Embedding: Convierte índices de palabras en vectores densos\n",
            "   - Input: (batch, 11)\n",
            "   - Output: (batch, 11, 16)\n",
            "   - Parámetros: 97 palabras × 16 dims = 1552\n",
            "\n",
            "2. LSTM: Procesa secuencia manteniendo memoria\n",
            "   - Input: (batch, 11, 16)\n",
            "   - Output: (batch, 32)  ← Solo el hidden state final\n",
            "   - Parámetros: 4 × 32 × (16 + 32 + 1) = 6272\n",
            "     (4 matrices: forget, input, output gates + cell state)\n",
            "\n",
            "3. Dense: Clasificación binaria\n",
            "   - Input: (batch, 32)\n",
            "   - Output: (batch, 1)\n",
            "   - Parámetros: 32 pesos + 1 bias = 33\n"
          ]
        }
      ],
      "source": [
        "# Hiperparámetros del modelo\n",
        "embedding_dim = 16   # Dimensión de los vectores de palabras\n",
        "lstm_units = 32      # Número de unidades LSTM (tamaño del hidden state)\n",
        "\n",
        "# Construcción del modelo secuencial\n",
        "modelo = Sequential([\n",
        "    # Capa 1: Embedding\n",
        "    # input_dim: Tamaño del vocabulario\n",
        "    # output_dim: Dimensión del vector de embedding\n",
        "    # input_length: Longitud de las secuencias (después del padding)\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=maxlen,\n",
        "        name='embedding'\n",
        "    ),\n",
        "\n",
        "    # Capa 2: LSTM\n",
        "    # units: Dimensión del hidden state\n",
        "    # La LSTM procesa la secuencia y devuelve el hidden state final\n",
        "    LSTM(\n",
        "        units=lstm_units,\n",
        "        name='lstm'\n",
        "    ),\n",
        "\n",
        "    # Capa 3: Dense (salida)\n",
        "    # 1 neurona con activación sigmoid para clasificación binaria\n",
        "    Dense(\n",
        "        units=1,\n",
        "        activation='sigmoid',\n",
        "        name='output'\n",
        "    )\n",
        "])\n",
        "\n",
        "print(\"Modelo LSTM creado:\")\n",
        "print(\"=\" * 70)\n",
        "modelo.summary()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Explicación de las capas:\")\n",
        "print(\"-\" * 70)\n",
        "print(\"1. Embedding: Convierte índices de palabras en vectores densos\")\n",
        "print(f\"   - Input: (batch, {maxlen})\")\n",
        "print(f\"   - Output: (batch, {maxlen}, {embedding_dim})\")\n",
        "print(f\"   - Parámetros: {vocab_size} palabras × {embedding_dim} dims = {vocab_size * embedding_dim}\")\n",
        "print()\n",
        "print(\"2. LSTM: Procesa secuencia manteniendo memoria\")\n",
        "print(f\"   - Input: (batch, {maxlen}, {embedding_dim})\")\n",
        "print(f\"   - Output: (batch, {lstm_units})  ← Solo el hidden state final\")\n",
        "print(f\"   - Parámetros: 4 × {lstm_units} × ({embedding_dim} + {lstm_units} + 1) = {4 * lstm_units * (embedding_dim + lstm_units + 1)}\")\n",
        "print(\"     (4 matrices: forget, input, output gates + cell state)\")\n",
        "print()\n",
        "print(\"3. Dense: Clasificación binaria\")\n",
        "print(f\"   - Input: (batch, {lstm_units})\")\n",
        "print(f\"   - Output: (batch, 1)\")\n",
        "print(f\"   - Parámetros: {lstm_units} pesos + 1 bias = {lstm_units + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3HCWxiyb3PB"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Compilación del Modelo\n",
        "\n",
        "Especificamos:\n",
        "- **Loss function**: Binary crossentropy (clasificación binaria)\n",
        "- **Optimizer**: Adam (adaptativo, eficiente)\n",
        "- **Metrics**: Accuracy (porcentaje de aciertos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vtYRNAlb3PB",
        "outputId": "436bc5c3-a610-4586-8272-f1aab8d0cc00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo compilado y listo para entrenar.\n",
            "\n",
            "Configuración:\n",
            "  Loss: binary_crossentropy\n",
            "  Optimizer: Adam (lr=0.001 por defecto)\n",
            "  Metrics: accuracy\n"
          ]
        }
      ],
      "source": [
        "# Compilamos el modelo\n",
        "modelo.compile(\n",
        "    # Loss: Binary Cross Entropy para clasificación binaria\n",
        "    loss='binary_crossentropy',\n",
        "\n",
        "    # Optimizer: Adam con learning rate por defecto (0.001)\n",
        "    optimizer='adam',\n",
        "\n",
        "    # Métricas a monitorear durante entrenamiento\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Modelo compilado y listo para entrenar.\")\n",
        "print(\"\\nConfiguración:\")\n",
        "print(f\"  Loss: binary_crossentropy\")\n",
        "print(f\"  Optimizer: Adam (lr=0.001 por defecto)\")\n",
        "print(f\"  Metrics: accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU_c--h4b3PB"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Entrenamiento del Modelo\n",
        "\n",
        "Entrenamos con `.fit()`, que ejecuta el bucle forward-backward-update automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRyiLVSLb3PB",
        "outputId": "3f8427a7-5b63-476d-dee2-bc21419a1752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando entrenamiento...\n",
            "======================================================================\n",
            "Epoch 1/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.6295 - loss: 0.6934\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6295 - loss: 0.6857 \n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6295 - loss: 0.6829 \n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7253 - loss: 0.6783 \n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7797 - loss: 0.6682 \n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8764 - loss: 0.6415 \n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9737 - loss: 0.5617 \n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.3651 \n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1379 \n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0539 \n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0306 \n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0213\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0166\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0136\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0115\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0099\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0086\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0076\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0067\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0060\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0054\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0049\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0045\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0041\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0038\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0035\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0032\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0030 \n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0028 \n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0026 \n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0025 \n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0023 \n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0022 \n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0021 \n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0019 \n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0018 \n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0018 \n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0017 \n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0016 \n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0015 \n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0014 \n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0014 \n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0013 \n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0013 \n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0012 \n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0012 \n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0011 \n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0011 \n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0010 \n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.9040e-04\n",
            "\n",
            "======================================================================\n",
            "Entrenamiento completado.\n",
            "Loss final: 0.0009\n",
            "Accuracy final: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Hiperparámetros de entrenamiento\n",
        "epochs = 50        # Número de épocas (pasadas completas por el dataset)\n",
        "batch_size = 2     # Tamaño del batch (con dataset pequeño, usamos batch chico)\n",
        "\n",
        "print(\"Iniciando entrenamiento...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "# fit() retorna un objeto History con el historial de loss y métricas\n",
        "historia = modelo.fit(\n",
        "    X,                    # Datos de entrada (secuencias con padding)\n",
        "    y,                    # Etiquetas (0 o 1)\n",
        "    epochs=epochs,        # Número de épocas\n",
        "    batch_size=batch_size,# Tamaño del batch\n",
        "    verbose=1             # verbose=1 muestra barra de progreso\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Entrenamiento completado.\")\n",
        "print(f\"Loss final: {historia.history['loss'][-1]:.4f}\")\n",
        "print(f\"Accuracy final: {historia.history['accuracy'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M1KDuQSb3PC"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Visualización del Aprendizaje\n",
        "\n",
        "Graficamos la evolución de loss y accuracy durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "bqY4MO8bb3PC",
        "outputId": "859b4541-4996-40b4-bb30-3ad079603f0c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpretación de las curvas:\n",
            "======================================================================\n",
            "- Loss descendente: El modelo está aprendiendo\n",
            "- Accuracy ascendente: Mejora en predicciones correctas\n",
            "- Convergencia: Estabilización en valores finales\n",
            "\n",
            "Nota: Con más datos, usaríamos un conjunto de validación\n",
            "para detectar overfitting (train accuracy >> val accuracy).\n"
          ]
        }
      ],
      "source": [
        "# Extraemos historial de métricas\n",
        "loss_history = historia.history['loss']\n",
        "acc_history = historia.history['accuracy']\n",
        "\n",
        "# Creamos figura con dos subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Subplot 1: Loss\n",
        "ax1.plot(loss_history, linewidth=2, color='blue')\n",
        "ax1.set_xlabel('Época', fontsize=12)\n",
        "ax1.set_ylabel('Loss (Binary Crossentropy)', fontsize=12)\n",
        "ax1.set_title('Evolución de la Pérdida', fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Accuracy\n",
        "ax2.plot(acc_history, linewidth=2, color='green')\n",
        "ax2.set_xlabel('Época', fontsize=12)\n",
        "ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "ax2.set_title('Evolución de la Precisión', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim([0, 1.05])  # Fijamos rango [0, 1]\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Interpretación de las curvas:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"- Loss descendente: El modelo está aprendiendo\")\n",
        "print(\"- Accuracy ascendente: Mejora en predicciones correctas\")\n",
        "print(\"- Convergencia: Estabilización en valores finales\")\n",
        "print(\"\\nNota: Con más datos, usaríamos un conjunto de validación\")\n",
        "print(\"para detectar overfitting (train accuracy >> val accuracy).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LD6jmV0b3PC"
      },
      "source": [
        "---\n",
        "\n",
        "## 11. Evaluación del Modelo\n",
        "\n",
        "Evaluamos el modelo en el conjunto de entrenamiento y analizamos predicciones individuales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ0kR_Y2b3PC",
        "outputId": "7c4ed9fa-6c21-41c6-949d-953346763afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluación en el conjunto de entrenamiento:\n",
            "======================================================================\n",
            "\n",
            "✓ 'La verdad, este lugar está bárbaro. Muy recomendable.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Qué buena onda la atención, volvería sin dudarlo.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Me encantó la comida, aunque la música estaba muy fu...'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Todo excelente. Atención de diez.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Muy conforme con el resultado final.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Superó mis expectativas, gracias.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'El mejor asado que probé en mucho tiempo.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Excelente relación precio-calidad, muy recomendable.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'La atención fue impecable, muy atentos.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Me gustó mucho el ambiente tranquilo.'\n",
            "  Real: Positivo | Pred: Positivo (prob=0.999)\n",
            "\n",
            "✓ 'Una porquería de servicio, nunca más vuelvo.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'El envío fue lento y el producto llegó dañado. Qué d...'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'Qué estafa, me arrepiento de haber comprado.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'No me gustó para nada la experiencia.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'No lo recomiendo, mala calidad.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'Malísima atención, el mozo tenía mala onda.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'Tardaron dos horas en entregar, llegó todo frío.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'Me cobraron de más y encima se hicieron los giles.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'La carne estaba pasada, casi no se podía comer.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "✓ 'Pésima experiencia, no vuelvo más.'\n",
            "  Real: Negativo | Pred: Negativo (prob=0.001)\n",
            "\n",
            "======================================================================\n",
            "Accuracy: 20/20 = 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Obtenemos predicciones\n",
        "# predict() retorna probabilidades (valores entre 0 y 1)\n",
        "predicciones_prob = modelo.predict(X, verbose=0)\n",
        "\n",
        "# Convertimos probabilidades a clases (umbral 0.5)\n",
        "predicciones_clase = (predicciones_prob >= 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculamos accuracy manualmente\n",
        "aciertos = np.sum(predicciones_clase == y)\n",
        "total = len(y)\n",
        "accuracy = aciertos / total\n",
        "\n",
        "print(\"Evaluación en el conjunto de entrenamiento:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i in range(len(frases)):\n",
        "    prob = predicciones_prob[i][0]\n",
        "    pred_clase = predicciones_clase[i]\n",
        "    real_clase = int(y[i])\n",
        "\n",
        "    correcto = \"✓\" if pred_clase == real_clase else \"✗\"\n",
        "    sent_real = \"Positivo\" if real_clase == 1 else \"Negativo\"\n",
        "    sent_pred = \"Positivo\" if pred_clase == 1 else \"Negativo\"\n",
        "\n",
        "    # Truncamos frase si es muy larga\n",
        "    frase_corta = frases[i] if len(frases[i]) <= 55 else frases[i][:52] + \"...\"\n",
        "\n",
        "    print(f\"\\n{correcto} '{frase_corta}'\")\n",
        "    print(f\"  Real: {sent_real} | Pred: {sent_pred} (prob={prob:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Accuracy: {aciertos}/{total} = {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wum1rXyPb3PD"
      },
      "source": [
        "---\n",
        "\n",
        "## 12. Predicción sobre Frases Nuevas\n",
        "\n",
        "Probamos el modelo con frases que nunca vio durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyfdh4-Mb3PD",
        "outputId": "170637fe-f702-4958-f7fe-fb0e8d1fc670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicciones sobre frases nuevas:\n",
            "======================================================================\n",
            "\n",
            "Frase: 'No me gustó la atención, bastante mala y lenta.'\n",
            "Predicción: NEGATIVO\n",
            "Probabilidad positivo: 0.001\n",
            "Confianza: 99.9%\n",
            "\n",
            "Frase: 'Muy buena experiencia, todo excelente y rápido.'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.999\n",
            "Confianza: 99.9%\n",
            "\n",
            "Frase: 'Una estafa total, no lo recomiendo para nada.'\n",
            "Predicción: NEGATIVO\n",
            "Probabilidad positivo: 0.001\n",
            "Confianza: 99.9%\n",
            "\n",
            "Frase: 'Súper conforme con el servicio, muy atentos.'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.999\n",
            "Confianza: 99.9%\n",
            "\n",
            "Frase: 'Nada que ver con lo prometido, una decepción.'\n",
            "Predicción: NEGATIVO\n",
            "Probabilidad positivo: 0.003\n",
            "Confianza: 99.7%\n",
            "\n",
            "Frase: 'La mejor atención que tuve en mucho tiempo.'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.999\n",
            "Confianza: 99.9%\n",
            "\n",
            "Frase: 'El lugar está bien pero la comida es mala.'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.999\n",
            "Confianza: 99.9%\n",
            "\n",
            "Frase: 'Aunque tardaron mucho, la comida estaba excelente.'\n",
            "Predicción: POSITIVO\n",
            "Probabilidad positivo: 0.999\n",
            "Confianza: 99.9%\n"
          ]
        }
      ],
      "source": [
        "# Frases de prueba\n",
        "frases_prueba = [\n",
        "    \"No me gustó la atención, bastante mala y lenta.\",\n",
        "    \"Muy buena experiencia, todo excelente y rápido.\",\n",
        "    \"Una estafa total, no lo recomiendo para nada.\",\n",
        "    \"Súper conforme con el servicio, muy atentos.\",\n",
        "    \"Nada que ver con lo prometido, una decepción.\",\n",
        "    \"La mejor atención que tuve en mucho tiempo.\",\n",
        "    \"El lugar está bien pero la comida es mala.\",\n",
        "    \"Aunque tardaron mucho, la comida estaba excelente.\"\n",
        "]\n",
        "\n",
        "# Preprocesamiento: texto → secuencias → padding\n",
        "# Usamos el MISMO tokenizer y maxlen que en entrenamiento\n",
        "secuencias_prueba = tokenizer.texts_to_sequences(frases_prueba)\n",
        "X_prueba = pad_sequences(secuencias_prueba, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Predicciones\n",
        "predicciones_prueba = modelo.predict(X_prueba, verbose=0)\n",
        "\n",
        "# Mostramos resultados\n",
        "print(\"Predicciones sobre frases nuevas:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, frase in enumerate(frases_prueba):\n",
        "    prob = predicciones_prueba[i][0]\n",
        "    clase = \"POSITIVO\" if prob >= 0.5 else \"NEGATIVO\"\n",
        "    confianza = prob if prob >= 0.5 else (1 - prob)\n",
        "\n",
        "    print(f\"\\nFrase: '{frase}'\")\n",
        "    print(f\"Predicción: {clase}\")\n",
        "    print(f\"Probabilidad positivo: {prob:.3f}\")\n",
        "    print(f\"Confianza: {confianza:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_mAE0Yb3PE"
      },
      "source": [
        "---\n",
        "\n",
        "## 13. Inspección de Embeddings Aprendidos\n",
        "\n",
        "Los embeddings aprendidos capturan similitud semántica. Veamos algunos ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snL5dtgIb3PE"
      },
      "outputs": [],
      "source": [
        "# Extraemos los pesos de la capa Embedding\n",
        "embedding_layer = modelo.get_layer('embedding')\n",
        "embedding_weights = embedding_layer.get_weights()[0]  # Shape: (vocab_size, embedding_dim)\n",
        "\n",
        "print(f\"Matriz de embeddings: {embedding_weights.shape}\")\n",
        "print(f\"Cada palabra tiene un vector de {embedding_dim} dimensiones.\")\n",
        "\n",
        "# Función para obtener embedding de una palabra\n",
        "def get_embedding(palabra):\n",
        "    idx = tokenizer.word_index.get(palabra.lower())\n",
        "    if idx is None:\n",
        "        return None\n",
        "    return embedding_weights[idx]\n",
        "\n",
        "# Función para calcular similitud coseno\n",
        "def similitud_coseno(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# Analizamos similitudes\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Similitudes semánticas aprendidas:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "pares = [\n",
        "    (\"excelente\", \"buena\"),\n",
        "    (\"excelente\", \"mala\"),\n",
        "    (\"atención\", \"servicio\"),\n",
        "    (\"recomendable\", \"malísima\"),\n",
        "]\n",
        "\n",
        "for palabra1, palabra2 in pares:\n",
        "    emb1 = get_embedding(palabra1)\n",
        "    emb2 = get_embedding(palabra2)\n",
        "\n",
        "    if emb1 is not None and emb2 is not None:\n",
        "        sim = similitud_coseno(emb1, emb2)\n",
        "        print(f\"Similitud('{palabra1}', '{palabra2}'): {sim:.3f}\")\n",
        "    else:\n",
        "        palabras_faltantes = []\n",
        "        if emb1 is None:\n",
        "            palabras_faltantes.append(palabra1)\n",
        "        if emb2 is None:\n",
        "            palabras_faltantes.append(palabra2)\n",
        "        print(f\"Palabra(s) no en vocabulario: {', '.join(palabras_faltantes)}\")\n",
        "\n",
        "print(\"\\nInterpretación:\")\n",
        "print(\"- Similitud cercana a 1: Palabras semánticamente similares\")\n",
        "print(\"- Similitud cercana a 0: Palabras no relacionadas\")\n",
        "print(\"- Similitud negativa: Palabras opuestas (ej: bueno/malo)\")\n",
        "print(\"\\nNota: Con más datos, los embeddings capturarían relaciones más ricas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJXW8m1hb3PF"
      },
      "source": [
        "---\n",
        "\n",
        "## 14. Guardar y Cargar el Modelo\n",
        "\n",
        "Keras permite guardar modelos completos (arquitectura + pesos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3eNUHwXb3PF"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo completo\n",
        "# Formato .h5 (HDF5): Incluye arquitectura, pesos, optimizador\n",
        "modelo.save('lstm_sentiment.h5')\n",
        "print(\"Modelo guardado en: lstm_sentiment.h5\")\n",
        "\n",
        "# Cargar el modelo\n",
        "from tensorflow.keras.models import load_model\n",
        "modelo_cargado = load_model('lstm_sentiment.h5')\n",
        "print(\"Modelo cargado exitosamente.\")\n",
        "\n",
        "# Verificamos que funciona igual\n",
        "pred_original = modelo.predict(X_prueba, verbose=0)\n",
        "pred_cargado = modelo_cargado.predict(X_prueba, verbose=0)\n",
        "\n",
        "# Comprobamos que las predicciones son idénticas\n",
        "igual = np.allclose(pred_original, pred_cargado)\n",
        "print(f\"\\n¿Predicciones idénticas? {igual}\")\n",
        "\n",
        "print(\"\\nEn producción, también necesitarías guardar el tokenizer:\")\n",
        "print(\"import pickle\")\n",
        "print(\"with open('tokenizer.pkl', 'wb') as f:\")\n",
        "print(\"    pickle.dump(tokenizer, f)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ5BGz8zb3PG"
      },
      "source": [
        "---\n",
        "\n",
        "## Guía Teórico-Conceptual\n",
        "\n",
        "### 1. Comparación: RNN Simple vs. LSTM vs. GRU\n",
        "\n",
        "| Característica | RNN Simple | LSTM | GRU |\n",
        "|----------------|------------|------|-----|\n",
        "| Parámetros | Pocos | Muchos | Moderados |\n",
        "| Memoria a largo plazo | ❌ Vanishing gradient | ✓ Cell state | ✓ Reset/update gates |\n",
        "| Complejidad | Baja | Alta | Media |\n",
        "| Velocidad | Rápida | Lenta | Moderada |\n",
        "| Uso moderno | Obsoleto | Estándar | Alternativa popular |\n",
        "\n",
        "**GRU (Gated Recurrent Unit):**\n",
        "- Versión simplificada de LSTM (menos parámetros)\n",
        "- Combina forget e input gates en un solo \"update gate\"\n",
        "- Más rápido de entrenar, rendimiento similar\n",
        "- Popular en NLP cuando velocidad importa\n",
        "\n",
        "### 2. Bidirectional LSTM\n",
        "\n",
        "LSTM procesa secuencia en una dirección:\n",
        "```\n",
        "\"No me gusta\" → [No] → [me] → [gusta]\n",
        "```\n",
        "\n",
        "**Bidirectional LSTM** procesa en ambas direcciones:\n",
        "```\n",
        "Forward:  [No] → [me] → [gusta]\n",
        "Backward: [gusta] → [me] → [No]\n",
        "```\n",
        "\n",
        "Luego concatena ambos hidden states.\n",
        "\n",
        "**Ventaja:** Captura contexto antes y después de cada palabra\n",
        "\n",
        "**Implementación en Keras:**\n",
        "```python\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "modelo = Sequential([\n",
        "    Embedding(...),\n",
        "    Bidirectional(LSTM(32)),  # ← Bidirectional wrapper\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "**Trade-off:** 2x parámetros, 2x tiempo de entrenamiento\n",
        "\n",
        "### 3. Return Sequences vs. Return State\n",
        "\n",
        "**return_sequences=False** (default):\n",
        "```\n",
        "Input:  (batch, timesteps, features)\n",
        "Output: (batch, units)  # Solo hidden state final\n",
        "```\n",
        "Útil para clasificación (como en este notebook).\n",
        "\n",
        "**return_sequences=True**:\n",
        "```\n",
        "Input:  (batch, timesteps, features)\n",
        "Output: (batch, timesteps, units)  # Hidden state en cada timestep\n",
        "```\n",
        "Útil para:\n",
        "- Apilar múltiples LSTMs\n",
        "- Sequence-to-sequence (traducción)\n",
        "- Token classification (NER)\n",
        "\n",
        "**Ejemplo con múltiples LSTMs:**\n",
        "```python\n",
        "modelo = Sequential([\n",
        "    Embedding(...),\n",
        "    LSTM(64, return_sequences=True),  # Primera LSTM\n",
        "    LSTM(32),                         # Segunda LSTM\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "### 4. Embeddings: Aprendidos vs. Pre-entrenados\n",
        "\n",
        "**¿Cuándo usar embeddings aprendidos?**\n",
        "- Vocabulario específico del dominio\n",
        "- Suficientes datos (>10k muestras)\n",
        "- Tarea muy específica\n",
        "\n",
        "**¿Cuándo usar pre-entrenados (Word2Vec, GloVe, FastText)?**\n",
        "- Pocos datos (<1k muestras)\n",
        "- Vocabulario general\n",
        "- Transfer learning\n",
        "\n",
        "**Implementación con pre-entrenados:**\n",
        "```python\n",
        "# 1. Cargar embeddings (ej: GloVe)\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# 2. Crear matriz de embeddings para nuestro vocabulario\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "# 3. Cargar en la capa Embedding\n",
        "embedding_layer = Embedding(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False  # Congelar embeddings\n",
        ")\n",
        "```\n",
        "\n",
        "### 5. Limitaciones de LSTM\n",
        "\n",
        "**1. Procesamiento secuencial:**\n",
        "- No se puede paralelizar (cada paso necesita el anterior)\n",
        "- Lento en secuencias largas\n",
        "\n",
        "**2. Dependencias realmente largas:**\n",
        "- Aunque mejor que RNN simple, LSTM sigue olvidando en textos muy largos (>1000 tokens)\n",
        "\n",
        "**3. Desbalance de información:**\n",
        "- Palabras al final influyen más que al inicio\n",
        "- Bidirectional LSTM mitiga esto\n",
        "\n",
        "**4. Interpretabilidad:**\n",
        "- Hidden state es una \"caja negra\"\n",
        "- Difícil saber qué información retiene\n",
        "\n",
        "**Solución moderna: Transformers**\n",
        "- Attention mechanism: Todas las palabras se ven entre sí\n",
        "- Paralelizable: Mucho más rápido\n",
        "- Mejor captura de dependencias largas\n",
        "- Esto lo veremos en el próximo notebook (BERT, GPT)\n",
        "\n",
        "### 6. Técnicas para Prevenir Overfitting\n",
        "\n",
        "**Dropout:**\n",
        "```python\n",
        "LSTM(32, dropout=0.2, recurrent_dropout=0.2)\n",
        "```\n",
        "- `dropout`: Apaga aleatoriamente inputs\n",
        "- `recurrent_dropout`: Apaga conexiones recurrentes\n",
        "\n",
        "**Regularización L2:**\n",
        "```python\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "LSTM(32, kernel_regularizer=l2(0.01))\n",
        "```\n",
        "\n",
        "**Early Stopping:**\n",
        "```python\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "modelo.fit(..., validation_data=(X_val, y_val), callbacks=[early_stop])\n",
        "```\n",
        "\n",
        "**Data Augmentation para texto:**\n",
        "- Sinónimos (usando WordNet)\n",
        "- Back-translation (traducir y volver a traducir)\n",
        "- Random insertion/deletion/swap de palabras\n",
        "\n",
        "### 7. Sequence-to-Sequence (Seq2Seq)\n",
        "\n",
        "Para tareas como traducción, necesitamos **encoder-decoder**:\n",
        "\n",
        "```\n",
        "Encoder LSTM: \"Hello\" → hidden state h\n",
        "Decoder LSTM: h → \"Hola\"\n",
        "```\n",
        "\n",
        "**Implementación básica:**\n",
        "```python\n",
        "# Encoder\n",
        "encoder = LSTM(128, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder = LSTM(128, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "```\n",
        "\n",
        "**Attention mechanism:** Mejora crucial (base de Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWUkamodb3PG"
      },
      "source": [
        "---\n",
        "\n",
        "## Preguntas y Respuestas para Estudio\n",
        "\n",
        "### Preguntas Conceptuales\n",
        "\n",
        "**1. ¿Por qué LSTM resuelve el problema de vanishing gradient?**\n",
        "\n",
        "*Respuesta:* LSTM usa un **cell state** que fluye directamente a través de los timesteps con suma (no multiplicación). Las compuertas controlan qué agregar/remover del cell state, pero los gradientes fluyen sin multiplicarse repetidamente por matrices de pesos, evitando que se desvanezcan.\n",
        "\n",
        "**2. ¿Qué hace cada compuerta (gate) en una LSTM?**\n",
        "\n",
        "*Respuesta:*\n",
        "- **Forget gate**: Decide qué información del cell state anterior descartar (qué olvidar)\n",
        "- **Input gate**: Decide qué nueva información agregar al cell state\n",
        "- **Output gate**: Decide qué parte del cell state exponer como hidden state\n",
        "\n",
        "Cada una aprende cuándo activarse basándose en el input actual y el hidden state anterior.\n",
        "\n",
        "**3. ¿Por qué necesitamos padding? ¿Qué problema resuelve?**\n",
        "\n",
        "*Respuesta:* Las redes neuronales requieren inputs de tamaño fijo, pero las frases tienen longitudes variables. Padding rellena secuencias cortas con ceros para que todas tengan la misma longitud. Esto permite procesarlas en batches eficientemente.\n",
        "\n",
        "**4. ¿Cuál es la diferencia fundamental entre Bag of Words y procesamiento secuencial?**\n",
        "\n",
        "*Respuesta:*\n",
        "- **BoW**: Ignora orden, solo cuenta ocurrencias. \"No me gusta\" = \"Me gusta no\"\n",
        "- **Secuencial (LSTM)**: Procesa palabra por palabra en orden. Captura que \"no\" antes de \"me gusta\" invierte el sentimiento\n",
        "\n",
        "**5. ¿Qué aprende la capa Embedding?**\n",
        "\n",
        "*Respuesta:* Aprende representaciones vectoriales densas de palabras donde palabras con significados similares tienen vectores cercanos en el espacio de embeddings. Durante el entrenamiento, ajusta estos vectores para que sean útiles para la tarea específica (en nuestro caso, clasificación de sentimientos).\n",
        "\n",
        "### Preguntas Técnicas\n",
        "\n",
        "**6. En el código, ¿por qué vocab_size = len(tokenizer.word_index) + 1?**\n",
        "\n",
        "*Respuesta:* El +1 es porque Keras reserva el índice 0 para padding. El tokenizer asigna índices desde 1 en adelante a las palabras. Entonces si tenemos 100 palabras únicas, word_index va de 1 a 100, y necesitamos 101 posiciones (0-100) en la capa Embedding.\n",
        "\n",
        "**7. ¿Qué pasa si una palabra nueva (no en el vocabulario) aparece en producción?**\n",
        "\n",
        "*Respuesta:* El tokenizer la reemplaza por el token OOV (Out Of Vocabulary) que configuramos con `oov_token=\"<OOV>\"`. Este token tiene su propio embedding aprendido que representa \"palabra desconocida\". Es importante incluir el oov_token durante entrenamiento para que el modelo aprenda a manejarlo.\n",
        "\n",
        "**8. ¿Por qué LSTM devuelve solo el hidden state final y no todos?**\n",
        "\n",
        "*Respuesta:* Porque `return_sequences=False` (default). Para clasificación, solo necesitamos la representación final que \"resume\" toda la secuencia. Si configuramos `return_sequences=True`, devolvería el hidden state en cada timestep, útil para apilar LSTMs o tareas como NER donde necesitamos clasificar cada token.\n",
        "\n",
        "**9. En la capa Embedding, ¿qué significan los parámetros input_dim, output_dim, input_length?**\n",
        "\n",
        "*Respuesta:*\n",
        "- `input_dim`: Tamaño del vocabulario (cuántas palabras diferentes puede manejar)\n",
        "- `output_dim`: Dimensión del vector de embedding (ej: 16, 100, 300)\n",
        "- `input_length`: Longitud de las secuencias de entrada (después del padding)\n",
        "\n",
        "**10. ¿Qué hace padding='post' vs. padding='pre'?**\n",
        "\n",
        "*Respuesta:*\n",
        "- `padding='post'`: Agrega ceros al **final**: [5, 2, 8, 0, 0, 0]\n",
        "- `padding='pre'`: Agrega ceros al **inicio**: [0, 0, 0, 5, 2, 8]\n",
        "\n",
        "Para LSTM, `post` suele ser mejor porque las palabras relevantes quedan al principio de la secuencia (donde la LSTM tiene mejor memoria).\n",
        "\n",
        "### Preguntas de Aplicación\n",
        "\n",
        "**11. Si tuvieras un dataset de 50,000 reseñas, ¿qué cambios harías al código?**\n",
        "\n",
        "*Respuesta:*\n",
        "1. **Train/val/test split**: 70/15/15 para evaluación honesta\n",
        "2. **Batch size mayor**: 32 o 64 (más eficiente)\n",
        "3. **Validation en fit()**: Monitorear overfitting\n",
        "4. **Early stopping**: Callback para detener si val_loss no mejora\n",
        "5. **Modelo más grande**: embedding_dim=100, lstm_units=128\n",
        "6. **Dropout**: Para regularización\n",
        "7. **Learning rate scheduling**: Reducir lr gradualmente\n",
        "\n",
        "**12. ¿Cómo implementarías un modelo Seq2Seq para traducción español→inglés?**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "# Encoder: Procesa español\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(vocab_size_es, 256)(encoder_inputs)\n",
        "encoder_lstm = LSTM(512, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder: Genera inglés\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(vocab_size_en, 256)(decoder_inputs)\n",
        "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(\n",
        "    decoder_embedding,\n",
        "    initial_state=encoder_states  # ← Contexto del encoder\n",
        ")\n",
        "decoder_dense = Dense(vocab_size_en, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "```\n",
        "\n",
        "**13. ¿Cómo cargarías embeddings pre-entrenados de Word2Vec en este modelo?**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# 1. Cargar Word2Vec\n",
        "w2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors.bin', binary=True)\n",
        "\n",
        "# 2. Crear matriz de embeddings\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    if word in w2v:\n",
        "        embedding_matrix[idx] = w2v[word]\n",
        "\n",
        "# 3. Configurar capa Embedding\n",
        "Embedding(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False  # Congelar o permitir fine-tuning\n",
        ")\n",
        "```\n",
        "\n",
        "**14. Si el modelo predice siempre la misma clase, ¿qué revisarías?**\n",
        "\n",
        "*Respuesta:*\n",
        "1. **Dataset desbalanceado**: Verificar distribución de clases\n",
        "2. **Learning rate muy alto**: Probablemente divergió\n",
        "3. **Inicialización de pesos**: Revisar si hay NaNs\n",
        "4. **Preprocesamiento incorrecto**: ¿El tokenizer se aplicó bien?\n",
        "5. **Loss function incorrecta**: Verificar que es binary_crossentropy\n",
        "6. **Modelo muy simple**: Aumentar capacidad (más unidades)\n",
        "\n",
        "**15. Diseñá una arquitectura LSTM profunda con 3 capas LSTM apiladas.**\n",
        "\n",
        "*Respuesta:*\n",
        "```python\n",
        "modelo = Sequential([\n",
        "    Embedding(vocab_size, 64, input_length=maxlen),\n",
        "    \n",
        "    # LSTM 1: return_sequences=True para alimentar siguiente LSTM\n",
        "    LSTM(128, return_sequences=True, dropout=0.2),\n",
        "    \n",
        "    # LSTM 2: También return_sequences=True\n",
        "    LSTM(64, return_sequences=True, dropout=0.2),\n",
        "    \n",
        "    # LSTM 3: return_sequences=False (solo hidden state final)\n",
        "    LSTM(32, dropout=0.2),\n",
        "    \n",
        "    # Clasificación\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "**Trade-off:** Más parámetros = más capacidad pero más riesgo de overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lepnOU9fb3PJ"
      },
      "source": [
        "---\n",
        "\n",
        "## Ejercicios Propuestos\n",
        "\n",
        "### Ejercicio 1: Bidirectional LSTM\n",
        "Modificá el modelo para usar `Bidirectional(LSTM(...))`. Compará:\n",
        "- Tiempo de entrenamiento\n",
        "- Accuracy final\n",
        "- Número de parámetros\n",
        "\n",
        "### Ejercicio 2: LSTM Profunda\n",
        "Apilá 2-3 capas LSTM. Recordá usar `return_sequences=True` en todas menos la última. ¿Mejora el rendimiento?\n",
        "\n",
        "### Ejercicio 3: GRU vs. LSTM\n",
        "Reemplazá `LSTM` por `GRU` (misma sintaxis). Compará:\n",
        "- Velocidad de entrenamiento\n",
        "- Accuracy\n",
        "- Número de parámetros\n",
        "\n",
        "### Ejercicio 4: Embeddings Pre-entrenados\n",
        "Cargá embeddings de FastText o Word2Vec y usálos en lugar de embeddings aprendidos. ¿El modelo converge más rápido?\n",
        "\n",
        "### Ejercicio 5: Análisis de Errores\n",
        "Identificá las frases que el modelo clasifica incorrectamente. ¿Hay patrones comunes? (ej: negaciones, ironía, frases ambiguas)\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "En este notebook dimos el salto de modelos feedforward a **procesamiento secuencial**. Aprendimos:\n",
        "\n",
        "1. **Arquitectura LSTM**: Celdas de memoria, compuertas, manejo de dependencias largas\n",
        "2. **Procesamiento de texto como secuencia**: Tokenización, padding, embeddings\n",
        "3. **Keras API**: Sequential, fit, callbacks, guardar/cargar modelos\n",
        "4. **Embeddings aprendidos**: Representaciones densas específicas de la tarea\n",
        "5. **Comparación con MLP**: Ventajas de capturar orden temporal\n",
        "\n",
        "### Limitaciones de LSTM\n",
        "\n",
        "Aunque LSTM es poderosa, tiene restricciones:\n",
        "- **Procesamiento secuencial**: No paralelizable, lento en secuencias largas\n",
        "- **Dependencias muy largas**: Aún tiene límites (mejor que RNN, pero no perfecto)\n",
        "- **Caja negra**: Difícil interpretar qué recuerda\n",
        "\n",
        "### ¿Qué sigue?\n",
        "\n",
        "En el próximo notebook exploraremos **Transformers con HuggingFace**, la arquitectura que revolucionó NLP:\n",
        "- **Attention mechanism**: Todas las palabras se ven entre sí\n",
        "- **Paralelización**: Mucho más rápido que LSTM\n",
        "- **Transfer learning**: Modelos pre-entrenados (BETO, RoBERTa)\n",
        "- **Estado del arte**: BERT, GPT, T5\n",
        "\n",
        "Los transformers resolvieron las limitaciones de LSTM y dominan NLP moderno. Esto será el puente hacia el programa principal de tu curso.\n",
        "\n",
        "---\n",
        "\n",
        "*Este material fue desarrollado con fines educativos para la Tecnicatura en Ciencia de Datos del IFTS.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}