{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tmO5_TALW3y1"},"outputs":[],"source":["#Trabajo Integrador - An√°lisis de Canciones de la d√©cada de los 80 a los 90 - Vanesa Cabrera."]},{"cell_type":"markdown","metadata":{"id":"JWAgjYTjW2im"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3yMxjkrK3hj"},"outputs":[],"source":["#Secci√≥n 1: Descripci√≥n del corpus elegido\n","# T√≠tulo: \"An√°lisis de NLP: Vocabulario y Tem√°ticas del Rock Nacional Argentino de los 80s: [¬øC√≥mo evolucion√≥ el vocabulario del rock nacional argentino durante la d√©cada de 1980 a 1990?]\n","\n","# 1.1 Descripci√≥n del corpus elegido\n","# - El Corpus est√° conformado por letras de las canciones m√°s reconocidas de la banda Soda Estereo, un √≠cono del rock nacional a nivel internacional. La banda nace en 1982 y las canciones fueron extra√≠das de los √°lbumes mencionados a continuaci√≥n y en los correspondientes a√±os.\n","# - Cu√°ntos documentos: Se conforma por 32 canciones:\n","  #Soda Stereo:\n","  #Album Nada Personal (1985)\n","  #1) Cuando pase el temblor\n","  #2) Nada personal\n","  #3) Juegos de seducci√≥n\n","  #4) Im√°genes retro\n","  #5) Danza rota\n","  #6) Estoy azulado\n","  #Album Signos (1986)\n","  #7) Pr√≥fugos\n","  #8) Persiana americana\n","  #9) Signos\n","  #10) El rito\n","  #11) No esxistes\n","  #Album Soda Estereo (1984)\n","  #12) Un misil en mi placard\n","  #13) Te hacen falta vitaminas\n","  #14) Mi novia tiene biceps\n","  #15) Tr√°tame suavemente\n","  #16) Disco Eterno\n","  #Album Doble Vida (1988)\n","  #17) En la ciudad de la furia\n","  #18) Lo que sangra\n","  #19) Coraz√≥n delator\n","  #20) Sobredosis de TV\n","  #Album Canci√≥n Animal (1990)\n","  #21) El s√©ptimo d√≠a\n","  #22) De m√∫sica ligera\n","  #23) T√© para tres\n","  #24) Un mill√≥n de a√±os luz\n","  #25) Hombre al agua\n","  #26) Sueles dejarme solo\n","  #27) Entre can√≠bales\n","  #Album Dynamo (1992)\n","  #28) Fue\n","  #Album Sue√±o Stereo (1995)\n","  #29) Disco eterno\n","  #30) Paseando por Roma\n","  #31) Zoom\n","  #32) Ella us√≥ mi cabeza como un revolver\n","\n","# - Per√≠odo temporal abarcado:\n","  #Si bien la banda nace en 1982, los √°lbumes pertenecen a los a√±os 1984 (Soda Estereo), 1985 (Nada personal), 1986 (Signos), 1988 (Doble vida), 1990 (Canci√≥n animal), 1992 (Dynamo), y 1995 (Sue√±o stereo)\n","# - Fuentes utilizadas: letras.com, google.com\n","\n","# 1.2 Justificaci√≥n de la elecci√≥n:\n","  #A mi criterio, la d√©cada comprendida entre los a√±os 1980 a 1990 han marcado a la historia argentina en todos los aspectos, cultural, pol√≠tica, social y econ√≥micamente. Es un per√≠odo donde se concentra la represi√≥n, la liberaci√≥n, el caos social, la violaci√≥n de los derechos humanos, la crisis econ√≥mica y pol√≠tica. La dictadura militar en el poder, se desvanec√≠a tras la p√©rdida de la guerra de las malvinas en 1982. Frente a la crisis pol√≠tica, econ√≥mica y social imperante, se convoca a elecciones y se vuelve a restablecer la democracia con Ra√∫l Alfonsin al mando del poder ejecutivo, habiendo ganado las elecciones por balotage en 1983. El pa√≠s atraviesa un per√≠odo de hiperinflaci√≥n en 1989, lo que lleva a la Argentina a atravesar un momento de m√°xima inseguridad social, extrema pobreza social, saqueos, altos niveles de delincuencia llevando a Alfonsin a abandonar el poder antes de lo previsto, y dando lugar a la llegada de la √©poca del menemismo, con Carlos Menem al poder.\n","  #Es un momento de gran apertura cultural, luego de a√±os de represi√≥n social, las artes se desarrollan en su m√°ximo esplendor. Mi pr√≥posito es cotejar la riqueza ling√º√≠stica, jerga porte√±a, met√°foras po√©ticas, cr√≠tica social. Se seleccion√≥ las letras de las canciones m√°s importantes y reconocidas de la banda Soda Stereo para elaborar el presente trabajo dado que ha sido una banda que surgi√≥ a fines del per√≠odo dictatorial de la historia argentina, atraves√≥ el un per√≠odo particular de la historia argentina, fines de la dictadura militar, derrota de la guerra de las malvinas, resurgimiento de la democracia, y la  hiperinflaci√≥n m√°s importante de la historia econ√≥mica. Se considera la banda m√°s importante, popular e influyente del rock latinoamericano. Soda Stereo gan√≥ el Premio Konex de Platino a la mejor banda de rock argentino de la d√©cada.\n","# - Por qu√© elegiste este corpus:\n","  #He seleccionado analizar las letras de las canciones de Soda Stereo ya que es una banda que atraviesa el per√≠odo hist√≥rico transitorio entre la dictadura militar y el resurgimiento de la democracia. Me interesa analizar la riqueza ling√º√≠stica de las canciones y c√≥mo cada album se relaciona con el contexto hist√≥rico imperante. Si bien ser√≠a m√°s enriquecedor poder analizar, asimismo, los recursos musicales que acompa√±an a la letra, ya sea en cuanto a modo, ritmo, armon√≠a y tonalidad, al no ser pertinente a la materia, este trabajo se limitar√° al an√°lisis ling√º√≠stico de las letras de las canciones.\n","# - Qu√© te interesa descubrir:\n","  #La relaci√≥n que hay entre las palabras m√°s importantes de cada album con el contexto hist√≥rico, social y econ√≥mico en el que surge.\n","# - Qu√© hip√≥tesis ten√©s sobre lo que vas a encontrar. Por qu√© este corpus:\n","\n","\n","#Hip√≥tesis:\n","#\"Las letras de las canciones emergen de una interacci√≥n din√°mica y compleja entre el contexto hist√≥rico-social y la experiencia individual del compositor, donde los acontecimientos hist√≥ricos, las tensiones sociales, los movimientos culturales y las condiciones socioecon√≥micas de cada √©poca proporcionan tanto el marco tem√°tico como los recursos expresivos que influencian significativamente el contenido, el tono y los mensajes de las composiciones musicales, la intenci√≥n art√≠stica y la posici√≥n social del creador.\"\n","\n","# 1.3 Proceso de recolecci√≥n\n","# - C√≥mo obtuviste los textos\n","# a trav√©s de c√≥digo que me permiti√≥ bajar las letras de p√°ginas web.\n","# - Criterios de inclusi√≥n/exclusi√≥n\n","#El criterio fue trabajar con el material de un grupo musical de la √©poca de la dictadura militar y comienzos de la democracia de manera de evaluar el vocabulario utilizado en los distintos per√≠odos de la historia argentina.\n","# - Dificultades encontradas y c√≥mo las resolviste\n","\n","# 1.4 Estad√≠sticas b√°sicas\n","# - N√∫mero total de textos\n","#Se utilizaron 32 canciones.\n","# - N√∫mero total de palabras (aproximado)\n","#Se utilizaron 3713 palabras\n","# - Distribuci√≥n de tama√±os de documentos\n","# - Gr√°fico de distribuci√≥n temporal (si aplicable)\n","\n","# 1.5 Exploraci√≥n inicial\n","# - Mostrar fragmentos representativos\n","# - Primeras observaciones cualitativas\n","# - Nube de palabras inicial (opcional)\n","\n","# 2.1 Carga y organizaci√≥n de datos\n","#import pandas as pd\n","#import numpy as np\n","#import pickle\n","#import re\n","#import string\n","#from collections import Counter\n","\n","# - C√≥digo para cargar todos los archivos de texto\n","# - Integraci√≥n con metadatos\n","# - Verificaci√≥n de integridad de datos\n","\n","# 2.2 Limpieza de texto\n","# - Aplicar t√©cnicas vistas en clase:\n","#   * Conversi√≥n a min√∫sculas\n","#   * Eliminaci√≥n de signos de puntuaci√≥n\n","#   * Eliminaci√≥n de n√∫meros (si no son relevantes)\n","#   * Eliminaci√≥n de caracteres especiales\n","# - Justificar cada decisi√≥n de limpieza\n","\n","# 2.3 Tokenizaci√≥n y normalizaci√≥n\n","# - Separar texto en palabras\n","# - Decidir si aplicar stemming o lemmatizaci√≥n (justificar)\n","# - Mostrar ejemplos de antes y despu√©s\n","\n","# 2.4 Manejo de stop words\n","#import nltk\n","#nltk.download('stopwords')\n","# - Decidir qu√© stop words usar (espa√±ol est√°ndar + espec√≠ficas del dominio)\n","# - Mostrar impacto de eliminar stop words\n","# - Justificar decisiones\n","\n","# 2.5 Estad√≠sticas post-procesamiento\n","# - Vocabulario final (n√∫mero de palabras √∫nicas)\n","# - Distribuci√≥n de frecuencias\n","# - Comparaci√≥n antes/despu√©s del preprocesamiento\n","#Secci√≥n 3: An√°lisis con BoW/TF-IDF (25% de la nota)\n","#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","#from sklearn.metrics.pairwise import cosine_similarity\n","#import matplotlib.pyplot as plt\n","#import seaborn as sns\n","\n","# 3.1 Vectorizaci√≥n del corpus\n","# - Crear matriz documento-t√©rmino con CountVectorizer\n","# - Crear matriz TF-IDF\n","# - Mostrar dimensiones y caracter√≠sticas de las matrices\n","# - Explicar qu√© representan los n√∫meros\n","\n","# 3.2 T√©rminos m√°s frecuentes y distintivos\n","# - Top 20 palabras m√°s frecuentes (BoW)\n","# - Top 20 t√©rminos con mayor TF-IDF\n","# - Comparar ambas listas: ¬øqu√© diferencias ves?\n","# - Interpretaci√≥n: ¬øestos t√©rminos caracterizan bien tu corpus?\n","\n","# 3.3 Matriz de similitud entre documentos\n","# - Calcular similitud coseno entre todos los pares de documentos\n","# - Encontrar los 2-3 pares m√°s similares\n","# - Encontrar los 2-3 pares m√°s diferentes\n","# - Analizar: ¬øtiene sentido lo que encontraste?\n","\n","# 3.4 Visualizaci√≥n\n","# - Nube de palabras con t√©rminos m√°s importantes\n","# - Gr√°fico de barras con t√©rminos m√°s frecuentes\n","# - Heatmap de similitud entre documentos (si no son demasiados)\n","\n","# 3.5 Interpretaci√≥n de resultados\n","# - ¬øQu√© patrones encontr√°s en tu corpus?\n","# - ¬øLos documentos similares realmente parecen similares?\n","# - ¬øHay agrupaciones naturales en tus datos?\n","# - ¬øQu√© limitaciones ves en este enfoque?\n","#Secci√≥n 4: An√°lisis con Word Embeddings (25% de la nota)\n","#import spacy\n","# Cargar modelo en espa√±ol\n","#nlp = spacy.load(\"es_core_news_md\")\n","\n","# 4.1 Aplicaci√≥n de embeddings\n","# - Procesar tu corpus con spaCy\n","# - Obtener vectores para documentos (promedio de vectores de palabras)\n","# - Explicar qu√© son los embeddings y por qu√© son diferentes a BoW\n","\n","# 4.2 An√°lisis de similitud sem√°ntica\n","# - Calcular similitud entre documentos usando embeddings\n","# - Comparar con resultados de TF-IDF\n","# - ¬øQu√© documentos son m√°s similares seg√∫n embeddings?\n","# - ¬øCoinciden los resultados con TF-IDF?\n","\n","# 4.3 B√∫squeda de analog√≠as relevantes al corpus\n","# - Encontrar palabras m√°s similares a t√©rminos clave de tu dominio\n","# - Intentar crear 2-3 analog√≠as que funcionen con vocabulario de tu corpus\n","# - Ejemplo: \"rock es a guitarra como tango es a ?\"\n","# - Interpretar: ¬ølas analog√≠as tienen sentido?\n","\n","# 4.4 Comparaci√≥n con resultados de BoW\n","# - Crear tabla comparativa de documentos m√°s similares\n","# - ¬øQu√© m√©todo da resultados m√°s intuitivos?\n","# - ¬øEn qu√© casos embeddings es claramente superior?\n","# - ¬øEn qu√© casos BoW podr√≠a ser suficiente?\n","\n","# 4.5 Visualizaci√≥n de embeddings (si es posible)\n","# - Intentar reducir dimensionalidad (PCA o t-SNE)\n","# - Graficar documentos en 2D\n","# - ¬øSe ven agrupaciones naturales?\n","# (Esta parte es opcional si resulta muy compleja)\n","\n","#Secci√≥n 5: An√°lisis Complementario (10% de la nota)\n","#Eleg√≠ UNA de estas tres opciones seg√∫n lo que hayan visto en clase:\n","\n","#Opci√≥n A: POS Tagging y An√°lisis Gramatical\n","# - Analizar distribuci√≥n de tipos de palabras (sustantivos, verbos, adjetivos)\n","# - ¬øHay diferencias gramaticales entre subcategor√≠as de tu corpus?\n","# - Interpretaci√≥n estil√≠stica de los patrones encontrados\n","#Opci√≥n B: An√°lisis de Sentimientos\n","# - Aplicar an√°lisis de sentimientos a tus textos\n","# - ¬øCu√°les son los m√°s positivos/negativos?\n","# - ¬øHay patrones de sentimiento por categor√≠a/autor/√©poca?\n","#Opci√≥n C: Extracci√≥n B√°sica de Entidades\n","# - Usar spaCy para extraer entidades nombradas (si aplicable a tu corpus)\n","# - ¬øQu√© personas, lugares, organizaciones se mencionan m√°s?\n","# - ¬øHay diferencias entre subcategor√≠as de tu corpus?\n","\n","#Secci√≥n 6: Conclusiones y Reflexiones (5% de la nota)\n","# 6.1 Hallazgos principales sobre el corpus\n","# - ¬øQu√© descubriste sobre tu corpus que no sab√≠as antes?\n","# - ¬øSe confirmaron tus hip√≥tesis iniciales?\n","# - ¬øQu√© te sorprendi√≥ m√°s?\n","\n","# 6.2 Comparaci√≥n de m√©todos utilizados\n","# - ¬øQu√© t√©cnica te pareci√≥ m√°s √∫til para tu tipo de corpus?\n","# - ¬øCu√°ndo usar√≠as BoW/TF-IDF vs embeddings?\n","# - ¬øQu√© ventajas y desventajas encontraste en cada m√©todo?\n","\n","# 6.3 Limitaciones encontradas\n","# - ¬øQu√© no pudiste capturar con las t√©cnicas usadas?\n","# - ¬øQu√© aspectos importantes de tu corpus quedan sin analizar?\n","# - ¬øQu√© mejorar√≠as si tuvieras m√°s tiempo/recursos?\n","\n","# 6.4 Aplicaciones potenciales del an√°lisis\n","# - ¬øC√≥mo se podr√≠a usar este an√°lisis en un contexto real?\n","# - ¬øQu√© valor agregado proporciona?\n","# - ¬øQu√© otros an√°lisis te gustar√≠a hacer en el futuro?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11055,"status":"ok","timestamp":1757683630053,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"},"user_tz":180},"id":"DH0E00z4G2oQ","outputId":"96e14290-9152-44ec-f822-eda2ef59bc18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from wordcloud) (2.0.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"]}],"source":["!pip install spacy\n","!python -m spacy download es_core_news_md\n","!pip install wordcloud"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25884,"status":"ok","timestamp":1758460767830,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"},"user_tz":180},"id":"Ifq_pzKFEj8l","outputId":"a13e8216-0826-4490-eff6-846de218299c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}],"source":["\n","# =============================================================================\n","# 2. CONFIGURACIONES GENERALES\n","# =============================================================================\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# 3. LIBRER√çAS EST√ÅNDAR DE PYTHON\n","# =============================================================================\n","import os\n","import re\n","import time\n","import string\n","import pickle\n","import random\n","import unicodedata\n","from collections import Counter\n","from datetime import datetime\n","from itertools import combinations\n","\n","# =============================================================================\n","# 4. LIBRER√çAS CIENT√çFICAS FUNDAMENTALES\n","# =============================================================================\n","import numpy as np\n","import pandas as pd\n","\n","# =============================================================================\n","# 5. VISUALIZACI√ìN\n","# =============================================================================\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","\n","# =============================================================================\n","# 6. PROCESAMIENTO DE LENGUAJE NATURAL - NLTK\n","# =============================================================================\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.stem import SnowballStemmer\n","\n","# Descargas de NLTK (ejecutar solo una vez)\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","# =============================================================================\n","# 7. PROCESAMIENTO DE LENGUAJE NATURAL - spaCy\n","# =============================================================================\n","import spacy\n","\n","# =============================================================================\n","# 8. MACHINE LEARNING - SCIKIT-LEARN\n","# =============================================================================\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.cluster import KMeans"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1682,"status":"ok","timestamp":1758460769505,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"},"user_tz":180},"id":"PVw_fHsjHo62","outputId":"b77dcab7-561a-47c3-b36c-dd29527ef9ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata loaded: 32 entries\n","                         archivo                  titulo autor_fuente  \\\n","0  01_cuando_pase_el_temblor.txt  Cuando pase el temblor  Soda Stereo   \n","1          02_imagenes_retro.txt          Im√°genes retro  Soda Stereo   \n","2           03_estoy_azulado.txt           Estoy azulado  Soda Stereo   \n","3                04_profugos.txt                Pr√≥fugos  Soda Stereo   \n","4      05_persiana_americana.txt      Persiana americana  Soda Stereo   \n","\n","        fecha categoria          album  palabras_aprox  \n","0  1985-01-01      rock  Nada Personal             144  \n","1  1985-01-01      rock  Nada Personal             119  \n","2  1985-01-01      rock  Nada Personal             117  \n","3  1986-01-01      rock         Signos             101  \n","4  1986-01-01      rock         Signos             121  \n"]}],"source":["# Quick test to see if your files are accessible\n","\n","base_dir = \"/content/drive/MyDrive/Programaci√≥n de lenguaje/Trabajo Integrador PNL/mi_corpus\"\n","metadata_file = os.path.join(base_dir, \"metadata.csv\")\n","\n","if os.path.exists(metadata_file):\n","    df = pd.read_csv(metadata_file)\n","    print(f\"Metadata loaded: {len(df)} entries\")\n","    print(df.head())\n","else:\n","    print(\"Metadata file not found\")"]},{"cell_type":"code","source":["import os\n","import re\n","import unicodedata\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Importaciones con manejo de errores\n","try:\n","    from wordcloud import WordCloud\n","    WORDCLOUD_AVAILABLE = True\n","except ImportError:\n","    print(\"‚ö†Ô∏è WordCloud no disponible - se omitir√°n las nubes de palabras\")\n","    WORDCLOUD_AVAILABLE = False\n","\n","# NLP Libraries\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import SnowballStemmer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","try:\n","    import spacy\n","    SPACY_AVAILABLE = True\n","except ImportError:\n","    print(\"‚ö†Ô∏è spaCy no disponible - an√°lisis de embeddings limitado\")\n","    SPACY_AVAILABLE = False\n","    spacy = None\n","\n","# Download NLTK data if needed\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","class OptimizedCorpusAnalyzer:\n","    \"\"\"Clase principal optimizada para an√°lisis completo de corpus musical\"\"\"\n","\n","    def __init__(self, base_dir=\"/content/drive/MyDrive/Programaci√≥n de lenguaje/Trabajo Integrador PNL/mi_corpus\"):\n","        self.base_dir = base_dir\n","        self.raw_texts_dir = os.path.join(base_dir, \"raw_texts\")\n","        self.metadata_file = os.path.join(base_dir, \"metadata.csv\")\n","        self.viz_dir = os.path.join(base_dir, 'visualizations')\n","\n","        # Initialize components\n","        self.texts = {}\n","        self.metadata_df = None\n","        self.tokenizer = TextTokenizer()\n","        self.nlp = None\n","\n","        # Processed data storage\n","        self.processed_texts = []\n","        self.bow_matrix = None\n","        self.tfidf_matrix = None\n","        self.doc_vectors = None\n","        self.similarities_bow = None\n","        self.similarities_tfidf = None\n","        self.similarities_embeddings = None\n","\n","        self.setup_directories()\n","        self.load_spacy_model()\n","\n","    def setup_directories(self):\n","        \"\"\"Crea directorios necesarios\"\"\"\n","        os.makedirs(self.viz_dir, exist_ok=True)\n","        print(f\"‚úÖ Estructura verificada en: {self.base_dir}\")\n","\n","    def load_spacy_model(self):\n","        \"\"\"Carga modelo spaCy una sola vez\"\"\"\n","        if not SPACY_AVAILABLE:\n","            print(\"‚ö†Ô∏è spaCy no disponible. Funcionalidad de embeddings limitada.\")\n","            self.nlp = None\n","            return\n","\n","        try:\n","            self.nlp = spacy.load('es_core_news_md')\n","            print(\"‚úÖ Modelo spaCy cargado exitosamente\")\n","        except OSError:\n","            print(\"‚ö†Ô∏è Modelo spaCy no encontrado. Instalar con: python -m spacy download es_core_news_md\")\n","            self.nlp = None\n","\n","    def load_corpus(self):\n","        \"\"\"Carga corpus y metadata\"\"\"\n","        if not os.path.exists(self.metadata_file):\n","            print(\"‚ùå Metadata no encontrado\")\n","            return False\n","\n","        self.metadata_df = pd.read_csv(self.metadata_file, encoding='utf-8')\n","\n","        for _, row in self.metadata_df.iterrows():\n","            filepath = os.path.join(self.raw_texts_dir, row['archivo'])\n","            if os.path.exists(filepath):\n","                try:\n","                    with open(filepath, 'r', encoding='utf-8') as f:\n","                        self.texts[row['archivo']] = f.read()\n","                except Exception as e:\n","                    print(f\"‚ö†Ô∏è Error leyendo {row['archivo']}: {e}\")\n","\n","        print(f\"üìö Corpus cargado: {len(self.texts)} documentos\")\n","        return True\n","\n","    def show_corpus_summary(self):\n","        \"\"\"Muestra resumen del corpus sin duplicar an√°lisis\"\"\"\n","        print(\"\\nüìã RESUMEN DEL CORPUS\")\n","        print(\"=\" * 60)\n","\n","        if 'album' in self.metadata_df.columns:\n","            album_counts = self.metadata_df.groupby('album').size()\n","            print(\"üìÄ CANCIONES POR √ÅLBUM:\")\n","            for album, count in album_counts.items():\n","                album_words = self.metadata_df[self.metadata_df['album'] == album]['palabras_aprox'].sum()\n","                print(f\"  ‚Ä¢ {album}: {count} canciones ({album_words:,} palabras)\")\n","\n","        print(f\"\\nüìä ESTAD√çSTICAS GENERALES:\")\n","        print(f\"  ‚Ä¢ Total canciones: {len(self.metadata_df)}\")\n","        print(f\"  ‚Ä¢ Palabras totales: {self.metadata_df['palabras_aprox'].sum():,}\")\n","        print(f\"  ‚Ä¢ Promedio palabras/canci√≥n: {self.metadata_df['palabras_aprox'].mean():.0f}\")\n","\n","    def analyze_all_tokenization(self, apply_stemming=True):\n","        \"\"\"Analiza tokenizaci√≥n de TODAS las canciones\"\"\"\n","        print(f\"\\nüî§ AN√ÅLISIS COMPLETO DE TOKENIZACI√ìN - TODAS LAS CANCIONES\")\n","        print(\"=\" * 80)\n","\n","        if apply_stemming:\n","            print(\"‚úÖ Aplicando stemming para reducir variaciones de palabras\")\n","\n","        all_tokens_original = []\n","        all_tokens_processed = []\n","        total_sentences = 0\n","        song_results = []\n","\n","        print(f\"\\nüìù Procesando {len(self.texts)} canciones...\")\n","\n","        for i, (filename, text) in enumerate(self.texts.items(), 1):\n","            song_info = self.metadata_df[self.metadata_df['archivo'] == filename].iloc[0]\n","\n","            # Procesar canci√≥n\n","            result = self.tokenizer.process_text_complete(text, apply_stemming=apply_stemming)\n","\n","            # Almacenar para uso posterior\n","            if apply_stemming and result['stemmed_tokens']:\n","                processed_text = ' '.join(result['stemmed_tokens'])\n","                all_tokens_processed.extend(result['stemmed_tokens'])\n","            else:\n","                processed_text = ' '.join(result['word_tokens'])\n","                all_tokens_processed.extend(result['word_tokens'])\n","\n","            self.processed_texts.append(processed_text)\n","\n","            # Acumular estad√≠sticas\n","            all_tokens_original.extend(result['word_tokens'])\n","            total_sentences += result['sentence_count']\n","\n","            # Guardar resultado individual\n","            song_results.append({\n","                'titulo': song_info['titulo'],\n","                'album': song_info['album'],\n","                'tokens_originales': len(result['word_tokens']),\n","                'tokens_procesados': len(result['stemmed_tokens']) if apply_stemming else len(result['word_tokens']),\n","                'oraciones': result['sentence_count']\n","            })\n","\n","            if i % 10 == 0:  # Progreso cada 10 canciones\n","                print(f\"  Procesadas: {i}/{len(self.texts)} canciones\")\n","\n","        # Estad√≠sticas completas\n","        print(f\"\\nüìä ESTAD√çSTICAS COMPLETAS DE TOKENIZACI√ìN:\")\n","        print(\"=\" * 60)\n","        print(f\"üìÑ Total oraciones: {total_sentences:,}\")\n","        print(f\"üî§ Total tokens originales: {len(all_tokens_original):,}\")\n","        print(f\"üî§ Total tokens procesados: {len(all_tokens_processed):,}\")\n","        print(f\"üìã Vocabulario √∫nico original: {len(set(all_tokens_original)):,}\")\n","        print(f\"üìã Vocabulario √∫nico procesado: {len(set(all_tokens_processed)):,}\")\n","\n","        if apply_stemming:\n","            reduction = len(set(all_tokens_original)) - len(set(all_tokens_processed))\n","            reduction_pct = (reduction / len(set(all_tokens_original))) * 100\n","            print(f\"üå± Reducci√≥n por stemming: {reduction} palabras ({reduction_pct:.1f}%)\")\n","\n","        # Top palabras m√°s frecuentes\n","        print(f\"\\nüèÜ TOP 20 PALABRAS M√ÅS FRECUENTES:\")\n","        print(\"-\" * 50)\n","\n","        token_freq = Counter(all_tokens_processed)\n","        for i, (token, count) in enumerate(token_freq.most_common(20), 1):\n","            percentage = (count / len(all_tokens_processed)) * 100\n","            print(f\"   {i:2d}. '{token}': {count} veces ({percentage:.1f}%)\")\n","\n","        # Estad√≠sticas por √°lbum\n","        df_results = pd.DataFrame(song_results)\n","        album_stats = df_results.groupby('album').agg({\n","            'tokens_procesados': ['sum', 'mean'],\n","            'oraciones': ['sum', 'mean']\n","        }).round(0)\n","\n","        print(f\"\\nüìÄ ESTAD√çSTICAS POR √ÅLBUM:\")\n","        print(\"-\" * 50)\n","        for album in album_stats.index:\n","            tokens_sum = int(album_stats.loc[album, ('tokens_procesados', 'sum')])\n","            tokens_mean = int(album_stats.loc[album, ('tokens_procesados', 'mean')])\n","            print(f\"  {album}: {tokens_sum:,} tokens ({tokens_mean} promedio/canci√≥n)\")\n","\n","        return {\n","            'all_tokens_processed': all_tokens_processed,\n","            'total_sentences': total_sentences,\n","            'song_results': song_results,\n","            'vocabulary_size': len(set(all_tokens_processed))\n","        }\n","\n","    def create_bow_and_tfidf_matrices(self, max_features=500, min_df=2):\n","        \"\"\"Crea matrices BoW y TF-IDF en una sola operaci√≥n\"\"\"\n","        print(f\"\\nüìä CREANDO MATRICES BOW Y TF-IDF\")\n","        print(\"-\" * 50)\n","\n","        if not self.processed_texts:\n","            print(\"‚ùå Textos no procesados. Ejecutando tokenizaci√≥n primero.\")\n","            self.analyze_all_tokenization()\n","\n","        # BoW Matrix\n","        bow_vectorizer = CountVectorizer(\n","            max_features=max_features,\n","            min_df=min_df,\n","            token_pattern=r'\\b\\w{3,}\\b'\n","        )\n","        self.bow_matrix = bow_vectorizer.fit_transform(self.processed_texts)\n","        bow_features = bow_vectorizer.get_feature_names_out()\n","\n","        # TF-IDF Matrix\n","        tfidf_vectorizer = TfidfVectorizer(\n","            max_features=max_features,\n","            min_df=min_df,\n","            token_pattern=r'\\b\\w{3,}\\b'\n","        )\n","        self.tfidf_matrix = tfidf_vectorizer.fit_transform(self.processed_texts)\n","        tfidf_features = tfidf_vectorizer.get_feature_names_out()\n","\n","        print(f\"‚úÖ Matriz BoW: {self.bow_matrix.shape}\")\n","        print(f\"‚úÖ Matriz TF-IDF: {self.tfidf_matrix.shape}\")\n","        print(f\"üìã Vocabulario: {len(bow_features)} t√©rminos\")\n","\n","        return bow_features, tfidf_features\n","\n","    def analyze_similarities(self):\n","        \"\"\"Calcula todas las similitudes en una operaci√≥n\"\"\"\n","        print(f\"\\nüîç CALCULANDO SIMILITUDES ENTRE DOCUMENTOS\")\n","        print(\"-\" * 50)\n","\n","        if self.bow_matrix is None or self.tfidf_matrix is None:\n","            print(\"‚ùå Matrices no creadas. Creando primero...\")\n","            self.create_bow_and_tfidf_matrices()\n","\n","        # Similitudes BoW y TF-IDF\n","        self.similarities_bow = cosine_similarity(self.bow_matrix)\n","        self.similarities_tfidf = cosine_similarity(self.tfidf_matrix)\n","\n","        print(f\"‚úÖ Similitudes BoW calculadas: {self.similarities_bow.shape}\")\n","        print(f\"‚úÖ Similitudes TF-IDF calculadas: {self.similarities_tfidf.shape}\")\n","\n","        # Similitudes con embeddings si est√° disponible\n","        if self.nlp and self.doc_vectors is not None:\n","            self.similarities_embeddings = cosine_similarity(self.doc_vectors)\n","            print(f\"‚úÖ Similitudes Embeddings calculadas: {self.similarities_embeddings.shape}\")\n","\n","    def process_embeddings(self):\n","        \"\"\"Procesa embeddings de todos los documentos\"\"\"\n","        print(f\"\\nüß† PROCESANDO EMBEDDINGS\")\n","        print(\"-\" * 50)\n","\n","        if not self.nlp:\n","            print(\"‚ùå Modelo spaCy no disponible\")\n","            return False\n","\n","        doc_vectors = []\n","        valid_docs = []\n","\n","        for i, (filename, text) in enumerate(self.texts.items()):\n","            doc = self.nlp(text)\n","\n","            # Extraer vectores v√°lidos\n","            valid_vectors = []\n","            for token in doc:\n","                if (not token.is_stop and\n","                    not token.is_punct and\n","                    not token.is_space and\n","                    len(token.text) > 2 and\n","                    token.has_vector):\n","                    valid_vectors.append(token.vector)\n","\n","            if valid_vectors:\n","                doc_vector = np.mean(valid_vectors, axis=0)\n","                doc_vectors.append(doc_vector)\n","                valid_docs.append(filename)\n","\n","        self.doc_vectors = np.array(doc_vectors)\n","        print(f\"‚úÖ {len(self.doc_vectors)} documentos convertidos a embeddings\")\n","        return True\n","\n","    def find_similar_documents(self, method='tfidf', n_pairs=5):\n","        \"\"\"Encuentra documentos similares usando el m√©todo especificado\"\"\"\n","        print(f\"\\nüéØ DOCUMENTOS M√ÅS SIMILARES ({method.upper()})\")\n","        print(\"-\" * 50)\n","\n","        if method == 'tfidf':\n","            similarity_matrix = self.similarities_tfidf\n","        elif method == 'bow':\n","            similarity_matrix = self.similarities_bow\n","        elif method == 'embeddings':\n","            similarity_matrix = self.similarities_embeddings\n","        else:\n","            print(\"‚ùå M√©todo no v√°lido\")\n","            return\n","\n","        if similarity_matrix is None:\n","            print(\"‚ùå Matriz de similitudes no calculada\")\n","            return\n","\n","        # Encontrar pares m√°s similares\n","        sim_copy = similarity_matrix.copy()\n","        np.fill_diagonal(sim_copy, 0)\n","\n","        flat_indices = np.argsort(sim_copy.flatten())[::-1]\n","        found_pairs = set()\n","\n","        print(f\"üèÜ TOP {n_pairs} PARES M√ÅS SIMILARES:\")\n","\n","        for flat_idx in flat_indices:\n","            row_idx, col_idx = np.unravel_index(flat_idx, sim_copy.shape)\n","\n","            pair = tuple(sorted([row_idx, col_idx]))\n","            if pair in found_pairs or row_idx == col_idx:\n","                continue\n","\n","            found_pairs.add(pair)\n","            similarity_score = similarity_matrix[row_idx, col_idx]\n","\n","            song1 = self.metadata_df.iloc[row_idx]['titulo']\n","            song2 = self.metadata_df.iloc[col_idx]['titulo']\n","            album1 = self.metadata_df.iloc[row_idx]['album']\n","            album2 = self.metadata_df.iloc[col_idx]['album']\n","\n","            print(f\"  {len(found_pairs)}. '{song1}' ({album1})\")\n","            print(f\"     ‚Üî '{song2}' ({album2})\")\n","            print(f\"     üìä Similitud: {similarity_score:.4f}\")\n","            print()\n","\n","            if len(found_pairs) >= n_pairs:\n","                break\n","\n","        return found_pairs\n","\n","    def create_comprehensive_visualizations(self):\n","        \"\"\"Crea todas las visualizaciones en una operaci√≥n\"\"\"\n","        print(f\"\\nüìä CREANDO VISUALIZACIONES COMPLETAS\")\n","        print(\"-\" * 50)\n","\n","        # 1. T√©rminos m√°s frecuentes\n","        self._create_term_frequency_plot()\n","\n","        # 2. Nube de palabras por √°lbum\n","        self._create_wordclouds_by_album()\n","\n","        # 3. Matriz de similitud\n","        if self.similarities_tfidf is not None and len(self.texts) <= 30:\n","            self._create_similarity_heatmap()\n","\n","        # 4. Visualizaci√≥n de embeddings 2D\n","        if self.doc_vectors is not None:\n","            self._create_embeddings_2d_plot()\n","\n","        print(f\"üíæ Todas las visualizaciones guardadas en: {self.viz_dir}\")\n","\n","    def _create_term_frequency_plot(self):\n","        \"\"\"Gr√°fico de t√©rminos m√°s frecuentes\"\"\"\n","        if self.bow_matrix is None or self.tfidf_matrix is None:\n","            return\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n","\n","        # BoW t√©rminos frecuentes\n","        term_frequencies = np.array(self.bow_matrix.sum(axis=0)).flatten()\n","        bow_vectorizer = CountVectorizer(max_features=500, min_df=2, token_pattern=r'\\b\\w{3,}\\b')\n","        bow_vectorizer.fit_transform(self.processed_texts)\n","        feature_names = bow_vectorizer.get_feature_names_out()\n","\n","        top_indices = np.argsort(term_frequencies)[::-1][:15]\n","        terms = [feature_names[i] for i in top_indices]\n","        freqs = [term_frequencies[i] for i in top_indices]\n","\n","        ax1.barh(range(len(terms)), freqs, color='skyblue')\n","        ax1.set_yticks(range(len(terms)))\n","        ax1.set_yticklabels(terms)\n","        ax1.set_title('Top 15 T√©rminos M√°s Frecuentes (BoW)')\n","\n","        # TF-IDF t√©rminos distintivos\n","        tfidf_scores = np.array(self.tfidf_matrix.sum(axis=0)).flatten()\n","        tfidf_vectorizer = TfidfVectorizer(max_features=500, min_df=2, token_pattern=r'\\b\\w{3,}\\b')\n","        tfidf_vectorizer.fit_transform(self.processed_texts)\n","        tfidf_features = tfidf_vectorizer.get_feature_names_out()\n","\n","        top_tfidf_indices = np.argsort(tfidf_scores)[::-1][:15]\n","        tfidf_terms = [tfidf_features[i] for i in top_tfidf_indices]\n","        tfidf_vals = [tfidf_scores[i] for i in top_tfidf_indices]\n","\n","        ax2.barh(range(len(tfidf_terms)), tfidf_vals, color='lightcoral')\n","        ax2.set_yticks(range(len(tfidf_terms)))\n","        ax2.set_yticklabels(tfidf_terms)\n","        ax2.set_title('Top 15 T√©rminos TF-IDF')\n","\n","        plt.tight_layout()\n","        freq_path = os.path.join(self.viz_dir, 'frecuencias_terminos.png')\n","        plt.savefig(freq_path, dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","    def _create_wordclouds_by_album(self):\n","        \"\"\"Nubes de palabras por √°lbum\"\"\"\n","        if not WORDCLOUD_AVAILABLE:\n","            print(\"‚ö†Ô∏è WordCloud no disponible - saltando nubes de palabras\")\n","            return\n","\n","        try:\n","            albums = self.metadata_df['album'].unique()\n","\n","            cols = 2 if len(albums) > 2 else len(albums)\n","            rows = (len(albums) + cols - 1) // cols\n","\n","            fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n","            if len(albums) == 1:\n","                axes = [axes]\n","            elif rows == 1:\n","                axes = axes if len(albums) > 1 else [axes]\n","            else:\n","                axes = axes.flatten()\n","\n","            stop_words_es = {\n","                'que', 'de', 'la', 'el', 'en', 'y', 'a', 'un', 'es', 'se', 'no', 'te',\n","                'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'las', 'del',\n","                'una', 'me', 'al', 'como', 'mi', 'tu', 'si', 'los', 'pero', 'sus'\n","            }\n","\n","            for i, album in enumerate(albums):\n","                album_songs = self.metadata_df[self.metadata_df['album'] == album]\n","                texto_album = \"\"\n","\n","                for _, song in album_songs.iterrows():\n","                    if song['archivo'] in self.texts:\n","                        texto_album += \" \" + self.texts[song['archivo']]\n","\n","                if texto_album.strip():\n","                    wordcloud = WordCloud(\n","                        width=800, height=400,\n","                        background_color='white',\n","                        stopwords=stop_words_es,\n","                        max_words=100\n","                    ).generate(texto_album.lower())\n","\n","                    ax = axes[i] if len(albums) > 1 else axes[0]\n","                    ax.imshow(wordcloud, interpolation='bilinear')\n","                    ax.set_title(f'{album}')\n","                    ax.axis('off')\n","\n","            # Ocultar subplots vac√≠os\n","            for j in range(i + 1, len(axes)):\n","                axes[j].axis('off')\n","\n","            plt.tight_layout()\n","            wordcloud_path = os.path.join(self.viz_dir, 'wordclouds_por_album.png')\n","            plt.savefig(wordcloud_path, dpi=300, bbox_inches='tight')\n","            plt.show()\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error creando nubes de palabras: {e}\")\n","\n","    def _create_similarity_heatmap(self):\n","        \"\"\"Heatmap de similitud entre documentos\"\"\"\n","        plt.figure(figsize=(12, 10))\n","\n","        song_names = [self.metadata_df.iloc[i]['titulo'][:20]\n","                     for i in range(len(self.texts))]\n","\n","        sns.heatmap(self.similarities_tfidf,\n","                   xticklabels=song_names,\n","                   yticklabels=song_names,\n","                   cmap='viridis', fmt='.2f')\n","\n","        plt.title('Matriz de Similitud (TF-IDF)')\n","        plt.xticks(rotation=45, ha='right')\n","        plt.tight_layout()\n","\n","        heatmap_path = os.path.join(self.viz_dir, 'similitud_documentos.png')\n","        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","    def _create_embeddings_2d_plot(self):\n","        \"\"\"Visualizaci√≥n 2D de embeddings\"\"\"\n","        if len(self.doc_vectors) < 4:\n","            return\n","\n","        # PCA y t-SNE\n","        pca = PCA(n_components=2, random_state=42)\n","        vectors_2d_pca = pca.fit_transform(self.doc_vectors)\n","\n","        perplexity = min(5, len(self.doc_vectors) - 1)\n","        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n","        vectors_2d_tsne = tsne.fit_transform(self.doc_vectors)\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n","\n","        # Colores por √°lbum\n","        albums = [self.metadata_df.iloc[i]['album'] for i in range(len(self.doc_vectors))]\n","        unique_albums = list(set(albums))\n","        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_albums)))\n","        album_colors = {album: colors[i] for i, album in enumerate(unique_albums)}\n","\n","        # PCA plot\n","        for i, (x, y) in enumerate(vectors_2d_pca):\n","            album = albums[i]\n","            ax1.scatter(x, y, c=[album_colors[album]], s=50, alpha=0.7)\n","\n","        ax1.set_title('Embeddings - PCA')\n","        ax1.grid(True, alpha=0.3)\n","\n","        # t-SNE plot\n","        for i, (x, y) in enumerate(vectors_2d_tsne):\n","            album = albums[i]\n","            ax2.scatter(x, y, c=[album_colors[album]], s=50, alpha=0.7)\n","\n","        ax2.set_title('Embeddings - t-SNE')\n","        ax2.grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        embeddings_path = os.path.join(self.viz_dir, 'embeddings_2d.png')\n","        plt.savefig(embeddings_path, dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","    def run_complete_analysis(self):\n","        \"\"\"Ejecuta an√°lisis completo optimizado\"\"\"\n","        print(\"üéµ AN√ÅLISIS COMPLETO NLP - CORPUS SODA STEREO\")\n","        print(\"=\" * 80)\n","\n","        # 1. Cargar corpus\n","        if not self.load_corpus():\n","            return None\n","\n","        # 2. Resumen inicial\n","        self.show_corpus_summary()\n","\n","        # 3. Tokenizaci√≥n completa de todas las canciones\n","        tokenization_results = self.analyze_all_tokenization(apply_stemming=True)\n","\n","        # 4. Crear matrices BoW/TF-IDF\n","        bow_features, tfidf_features = self.create_bow_and_tfidf_matrices()\n","\n","        # 5. Procesar embeddings\n","        if self.nlp:\n","            self.process_embeddings()\n","\n","        # 6. Calcular similitudes\n","        self.analyze_similarities()\n","\n","        # 7. Encontrar documentos similares\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"AN√ÅLISIS DE SIMILITUDES\")\n","        print(\"=\"*60)\n","\n","        self.find_similar_documents('tfidf', 5)\n","        if self.similarities_embeddings is not None:\n","            self.find_similar_documents('embeddings', 3)\n","\n","        # 8. Crear visualizaciones\n","        self.create_comprehensive_visualizations()\n","\n","        print(f\"\\n‚úÖ AN√ÅLISIS COMPLETO FINALIZADO\")\n","        print(\"=\" * 80)\n","        print(\"üìã RESUMEN DE AN√ÅLISIS REALIZADOS:\")\n","        print(\"‚úÖ 1. Resumen y estad√≠sticas del corpus\")\n","        print(\"‚úÖ 2. Tokenizaci√≥n y stemming de TODAS las canciones\")\n","        print(\"‚úÖ 3. Matrices BoW/TF-IDF optimizadas\")\n","        print(\"‚úÖ 4. An√°lisis con embeddings\")\n","        print(\"‚úÖ 5. C√°lculo de similitudes\")\n","        print(\"‚úÖ 6. Visualizaciones completas\")\n","        print(f\"üìÅ Archivos en: {self.viz_dir}/\")\n","\n","        return {\n","            'tokenization_results': tokenization_results,\n","            'bow_features': bow_features,\n","            'tfidf_features': tfidf_features,\n","            'similarities_tfidf': self.similarities_tfidf,\n","            'similarities_embeddings': self.similarities_embeddings,\n","            'doc_vectors': self.doc_vectors\n","        }\n","\n","\n","class TextTokenizer:\n","    \"\"\"Clase para tokenizaci√≥n y normalizaci√≥n optimizada\"\"\"\n","\n","    def __init__(self, language='spanish'):\n","        self.language = language\n","        self.stemmer = SnowballStemmer(language)\n","\n","        self.custom_stopwords = {\n","            'que', 'de', 'la', 'el', 'en', 'y', 'a', 'un', 'es', 'se', 'no', 'te',\n","            'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'las', 'del',\n","            'una', 'me', 'al', 'como', 'mi', 'tu', 'si', 'los', 'pero', 'sus',\n","            'ya', 'ni', 'muy', 'tan', 'm√°s', 'vez', 'ser', 'sin', 'otro', 'otra',\n","            'cuando', 'donde', 'quien', 'cual', 'todo', 'todos', 'cada', 'hasta',\n","            'desde', 'entre', 'sobre', 'bajo', 'tras', 'durante', 'mediante',\n","            'esta', 'estos', 'este', 'estas', 'con', 'cabe', 'fue', 'oh', 'ah',\n","            'na', 'eh', 'mm', 'yeah', 'hey', 'wow'\n","        }\n","\n","    def clean_text(self, text):\n","        \"\"\"Limpia el texto b√°sico\"\"\"\n","        text = text.lower()\n","        text = re.sub(r'[^\\w\\s\\n√°√©√≠√≥√∫√º√±]', ' ', text)\n","        text = re.sub(r'\\s+', ' ', text.strip())\n","        return text\n","\n","    def tokenize_words(self, text):\n","        \"\"\"Tokeniza texto en palabras individuales\"\"\"\n","        clean_text = self.clean_text(text)\n","\n","        try:\n","            tokens = word_tokenize(clean_text, language=self.language)\n","        except:\n","            tokens = clean_text.split()\n","\n","        valid_tokens = []\n","        for token in tokens:\n","            if len(token) >= 2 and not token.isdigit() and token not in self.custom_stopwords:\n","                valid_tokens.append(token)\n","\n","        return valid_tokens\n","\n","    def tokenize_sentences(self, text):\n","        \"\"\"Tokeniza texto en oraciones\"\"\"\n","        try:\n","            sentences = sent_tokenize(text, language=self.language)\n","        except:\n","            sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 10]\n","\n","        return [s.strip() for s in sentences if len(s.strip()) > 10]\n","\n","    def apply_stemming(self, tokens):\n","        \"\"\"Aplica stemming a lista de tokens\"\"\"\n","        stemmed_tokens = []\n","        for token in tokens:\n","            try:\n","                stemmed = self.stemmer.stem(token)\n","                stemmed_tokens.append(stemmed)\n","            except:\n","                stemmed_tokens.append(token)\n","\n","        return stemmed_tokens\n","\n","    def process_text_complete(self, text, apply_stemming=True):\n","        \"\"\"Procesa texto completo: tokenizaci√≥n + normalizaci√≥n\"\"\"\n","\n","        # 1. Tokenizaci√≥n por palabras\n","        word_tokens = self.tokenize_words(text)\n","\n","        # 2. Tokenizaci√≥n por oraciones\n","        sentence_tokens = self.tokenize_sentences(text)\n","\n","        # 3. Aplicar stemming si se solicita\n","        if apply_stemming:\n","            stemmed_tokens = self.apply_stemming(word_tokens)\n","        else:\n","            stemmed_tokens = word_tokens\n","\n","        return {\n","            'original_text': text[:200] + '...' if len(text) > 200 else text,\n","            'word_tokens': word_tokens,\n","            'sentence_tokens': sentence_tokens,\n","            'stemmed_tokens': stemmed_tokens if apply_stemming else None,\n","            'token_count': len(word_tokens),\n","            'sentence_count': len(sentence_tokens)\n","        }\n","\n","\n","# FUNCI√ìN PRINCIPAL OPTIMIZADA\n","def ejecutar_analisis_optimizado(corpus_base_dir):\n","    \"\"\"\n","    Funci√≥n principal optimizada que ejecuta todo el an√°lisis sin duplicaciones\n","    \"\"\"\n","    print(\"üéµ INICIANDO AN√ÅLISIS NLP OPTIMIZADO\")\n","    print(\"=\" * 80)\n","\n","    # Crear analizador principal\n","    analyzer = OptimizedCorpusAnalyzer(base_dir=corpus_base_dir)\n","\n","    # Ejecutar an√°lisis completo\n","    results = analyzer.run_complete_analysis()\n","\n","    if results:\n","        print(\"\\nüéØ RESULTADOS DISPONIBLES:\")\n","        print(f\"  ‚Ä¢ Vocabulario procesado: {results['tokenization_results']['vocabulary_size']:,} palabras √∫nicas\")\n","        print(f\"  ‚Ä¢ Caracter√≠sticas BoW: {len(results['bow_features'])}\")\n","        print(f\"  ‚Ä¢ Caracter√≠sticas TF-IDF: {len(results['tfidf_features'])}\")\n","        if results['doc_vectors'] is not None:\n","            print(f\"  ‚Ä¢ Embeddings: {results['doc_vectors'].shape[0]} documentos x {results['doc_vectors'].shape[1]} dimensiones\")\n","\n","    return analyzer, results\n","\n","\n","# EJECUCI√ìN AUTOM√ÅTICA\n","if __name__ == \"__main__\":\n","    corpus_path = \"/content/drive/MyDrive/Programaci√≥n de lenguaje/Trabajo Integrador PNL/mi_corpus\"\n","\n","    print(\"üöÄ Ejecutando an√°lisis completo autom√°tico...\")\n","    analyzer, results = ejecutar_analisis_optimizado(corpus_path)\n","\n","    print(\"\\n‚úÖ ¬°AN√ÅLISIS COMPLETADO!\")\n","    print(\"üìÅ Revisa la carpeta 'visualizations' para ver los gr√°ficos generados\")\n","\n","\n","# Para ejecutar solo el an√°lisis (sin duplicar operaciones)\n","corpus_path = \"/content/drive/MyDrive/Programaci√≥n de lenguaje/Trabajo Integrador PNL/mi_corpus\"\n","analyzer, results = ejecutar_analisis_optimizado(corpus_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1R8jLDD8tgBPcfeCq7Z5L24icXPJXNp6Q"},"id":"62alnjfzsHLA","executionInfo":{"status":"ok","timestamp":1758460810657,"user_tz":180,"elapsed":34877,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"}},"outputId":"75822636-34d7-4a0d-dc9b-a58f41af1265"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"QPAmkJ8ybAXG"},"source":["Conclusi√≥n: La hipotesis planteada afirma que las letras de las canciones emergen de una interacci√≥n din√°mica y compleja entre el contexto hist√≥rico-social y la experiencia individual del compositor, donde los acontecimientos hist√≥ricos, las tensiones sociales, los movimientos culturales y las condiciones socioecon√≥micas de cada √©poca proporcionan tanto el marco tem√°tico como los recursos expresivos que influencian significativamente el contenido, el tono y los mensajes de las composiciones musicales, la intenci√≥n art√≠stica y la posici√≥n social del creador.\n","Si bien, luego de haberse procesado las 32 canciones de Soda Stereo conformando un corpus de 3713 palabras, de acuerdo al an√°lisis de la frecuencia de las 20 palabras que m√°s aparecen y evaluando las nubes de palabras, la hip√≥tesis deber√≠a ser refutada. No obstante, si se profundiza en las letras y las palabras que aparecen en las canciones, se encuentran conceptos que s√≠ tienen una relaci√≥n con el contexto y que el autor pudo haber sido condicionado por este a la hora de pensar en la letra. En la canci√≥n Cuando pase el temblor (1985), aparecen frases como: \"a veces siento temor\", \"un planeta con desiluci√≥n\", \"s√© que te encontrar√© en esas ruinas\"; recordando el contexto, fines de la dictadura militar, de la guerra de las malvinas, son palabras que tienen gran relaci√≥n con el contexto imperante.\n","Se podr√≠a decir que fin y final, son las palabras de las 20 palabras de mayor frecuencia que tendr√≠an una relaci√≥n con el contexto hist√≥rico, el decaimiento de la dictadura, y aparici√≥n de la democracia. Otras palabras que aparecen, pero no con alta frecuencia, son pr√≥fugos, misil, furia, sangra, delator. Profundizando, alguna de estas palabras, como ser misil, sangra, ciudad, furia aparecen en la nube de palabras, empero su tama√±o es muy peque√±o ya que su frecuencia de aparici√≥n es baja. Considero que el estribillo, al repetirse en las letras puede generar un sesgo importante a tener en cuenta a la hora de trabajar con letras de canciones."]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1mN40aw0MjbFdinvMtd1CJfvfiMZrdzY_","authorship_tag":"ABX9TyMLz++rtPkplTkXEXgE6mIp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}