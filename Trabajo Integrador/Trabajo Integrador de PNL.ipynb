{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tmO5_TALW3y1"},"outputs":[],"source":["#Trabajo Integrador - Análisis de Canciones de la década de los 80 a los 90 - Vanesa Cabrera."]},{"cell_type":"markdown","metadata":{"id":"JWAgjYTjW2im"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3yMxjkrK3hj"},"outputs":[],"source":["#Sección 1: Descripción del corpus elegido\n","# Título: \"Análisis de NLP: Vocabulario y Temáticas del Rock Nacional Argentino de los 80s: [¿Cómo evolucionó el vocabulario del rock nacional argentino durante la década de 1980 a 1990?]\n","\n","# 1.1 Descripción del corpus elegido\n","# - El Corpus está conformado por letras de las canciones más reconocidas de la banda Soda Estereo, un ícono del rock nacional a nivel internacional. La banda nace en 1982 y las canciones fueron extraídas de los álbumes mencionados a continuación y en los correspondientes años.\n","# - Cuántos documentos: Se conforma por 32 canciones:\n","  #Soda Stereo:\n","  #Album Nada Personal (1985)\n","  #1) Cuando pase el temblor\n","  #2) Nada personal\n","  #3) Juegos de seducción\n","  #4) Imágenes retro\n","  #5) Danza rota\n","  #6) Estoy azulado\n","  #Album Signos (1986)\n","  #7) Prófugos\n","  #8) Persiana americana\n","  #9) Signos\n","  #10) El rito\n","  #11) No esxistes\n","  #Album Soda Estereo (1984)\n","  #12) Un misil en mi placard\n","  #13) Te hacen falta vitaminas\n","  #14) Mi novia tiene biceps\n","  #15) Trátame suavemente\n","  #16) Disco Eterno\n","  #Album Doble Vida (1988)\n","  #17) En la ciudad de la furia\n","  #18) Lo que sangra\n","  #19) Corazón delator\n","  #20) Sobredosis de TV\n","  #Album Canción Animal (1990)\n","  #21) El séptimo día\n","  #22) De música ligera\n","  #23) Té para tres\n","  #24) Un millón de años luz\n","  #25) Hombre al agua\n","  #26) Sueles dejarme solo\n","  #27) Entre caníbales\n","  #Album Dynamo (1992)\n","  #28) Fue\n","  #Album Sueño Stereo (1995)\n","  #29) Disco eterno\n","  #30) Paseando por Roma\n","  #31) Zoom\n","  #32) Ella usó mi cabeza como un revolver\n","\n","# - Período temporal abarcado:\n","  #Si bien la banda nace en 1982, los álbumes pertenecen a los años 1984 (Soda Estereo), 1985 (Nada personal), 1986 (Signos), 1988 (Doble vida), 1990 (Canción animal), 1992 (Dynamo), y 1995 (Sueño stereo)\n","# - Fuentes utilizadas: letras.com, google.com\n","\n","# 1.2 Justificación de la elección:\n","  #A mi criterio, la década comprendida entre los años 1980 a 1990 han marcado a la historia argentina en todos los aspectos, cultural, política, social y económicamente. Es un período donde se concentra la represión, la liberación, el caos social, la violación de los derechos humanos, la crisis económica y política. La dictadura militar en el poder, se desvanecía tras la pérdida de la guerra de las malvinas en 1982. Frente a la crisis política, económica y social imperante, se convoca a elecciones y se vuelve a restablecer la democracia con Raúl Alfonsin al mando del poder ejecutivo, habiendo ganado las elecciones por balotage en 1983. El país atraviesa un período de hiperinflación en 1989, lo que lleva a la Argentina a atravesar un momento de máxima inseguridad social, extrema pobreza social, saqueos, altos niveles de delincuencia llevando a Alfonsin a abandonar el poder antes de lo previsto, y dando lugar a la llegada de la época del menemismo, con Carlos Menem al poder.\n","  #Es un momento de gran apertura cultural, luego de años de represión social, las artes se desarrollan en su máximo esplendor. Mi próposito es cotejar la riqueza lingüística, jerga porteña, metáforas poéticas, crítica social. Se seleccionó las letras de las canciones más importantes y reconocidas de la banda Soda Stereo para elaborar el presente trabajo dado que ha sido una banda que surgió a fines del período dictatorial de la historia argentina, atravesó el un período particular de la historia argentina, fines de la dictadura militar, derrota de la guerra de las malvinas, resurgimiento de la democracia, y la  hiperinflación más importante de la historia económica. Se considera la banda más importante, popular e influyente del rock latinoamericano. Soda Stereo ganó el Premio Konex de Platino a la mejor banda de rock argentino de la década.\n","# - Por qué elegiste este corpus:\n","  #He seleccionado analizar las letras de las canciones de Soda Stereo ya que es una banda que atraviesa el período histórico transitorio entre la dictadura militar y el resurgimiento de la democracia. Me interesa analizar la riqueza lingüística de las canciones y cómo cada album se relaciona con el contexto histórico imperante. Si bien sería más enriquecedor poder analizar, asimismo, los recursos musicales que acompañan a la letra, ya sea en cuanto a modo, ritmo, armonía y tonalidad, al no ser pertinente a la materia, este trabajo se limitará al análisis lingüístico de las letras de las canciones.\n","# - Qué te interesa descubrir:\n","  #La relación que hay entre las palabras más importantes de cada album con el contexto histórico, social y económico en el que surge.\n","# - Qué hipótesis tenés sobre lo que vas a encontrar. Por qué este corpus:\n","\n","\n","#Hipótesis:\n","#\"Las letras de las canciones emergen de una interacción dinámica y compleja entre el contexto histórico-social y la experiencia individual del compositor, donde los acontecimientos históricos, las tensiones sociales, los movimientos culturales y las condiciones socioeconómicas de cada época proporcionan tanto el marco temático como los recursos expresivos que influencian significativamente el contenido, el tono y los mensajes de las composiciones musicales, la intención artística y la posición social del creador.\"\n","\n","# 1.3 Proceso de recolección\n","# - Cómo obtuviste los textos\n","# a través de código que me permitió bajar las letras de páginas web.\n","# - Criterios de inclusión/exclusión\n","#El criterio fue trabajar con el material de un grupo musical de la época de la dictadura militar y comienzos de la democracia de manera de evaluar el vocabulario utilizado en los distintos períodos de la historia argentina.\n","# - Dificultades encontradas y cómo las resolviste\n","\n","# 1.4 Estadísticas básicas\n","# - Número total de textos\n","#Se utilizaron 32 canciones.\n","# - Número total de palabras (aproximado)\n","#Se utilizaron 3713 palabras\n","# - Distribución de tamaños de documentos\n","# - Gráfico de distribución temporal (si aplicable)\n","\n","# 1.5 Exploración inicial\n","# - Mostrar fragmentos representativos\n","# - Primeras observaciones cualitativas\n","# - Nube de palabras inicial (opcional)\n","\n","# 2.1 Carga y organización de datos\n","#import pandas as pd\n","#import numpy as np\n","#import pickle\n","#import re\n","#import string\n","#from collections import Counter\n","\n","# - Código para cargar todos los archivos de texto\n","# - Integración con metadatos\n","# - Verificación de integridad de datos\n","\n","# 2.2 Limpieza de texto\n","# - Aplicar técnicas vistas en clase:\n","#   * Conversión a minúsculas\n","#   * Eliminación de signos de puntuación\n","#   * Eliminación de números (si no son relevantes)\n","#   * Eliminación de caracteres especiales\n","# - Justificar cada decisión de limpieza\n","\n","# 2.3 Tokenización y normalización\n","# - Separar texto en palabras\n","# - Decidir si aplicar stemming o lemmatización (justificar)\n","# - Mostrar ejemplos de antes y después\n","\n","# 2.4 Manejo de stop words\n","#import nltk\n","#nltk.download('stopwords')\n","# - Decidir qué stop words usar (español estándar + específicas del dominio)\n","# - Mostrar impacto de eliminar stop words\n","# - Justificar decisiones\n","\n","# 2.5 Estadísticas post-procesamiento\n","# - Vocabulario final (número de palabras únicas)\n","# - Distribución de frecuencias\n","# - Comparación antes/después del preprocesamiento\n","#Sección 3: Análisis con BoW/TF-IDF (25% de la nota)\n","#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","#from sklearn.metrics.pairwise import cosine_similarity\n","#import matplotlib.pyplot as plt\n","#import seaborn as sns\n","\n","# 3.1 Vectorización del corpus\n","# - Crear matriz documento-término con CountVectorizer\n","# - Crear matriz TF-IDF\n","# - Mostrar dimensiones y características de las matrices\n","# - Explicar qué representan los números\n","\n","# 3.2 Términos más frecuentes y distintivos\n","# - Top 20 palabras más frecuentes (BoW)\n","# - Top 20 términos con mayor TF-IDF\n","# - Comparar ambas listas: ¿qué diferencias ves?\n","# - Interpretación: ¿estos términos caracterizan bien tu corpus?\n","\n","# 3.3 Matriz de similitud entre documentos\n","# - Calcular similitud coseno entre todos los pares de documentos\n","# - Encontrar los 2-3 pares más similares\n","# - Encontrar los 2-3 pares más diferentes\n","# - Analizar: ¿tiene sentido lo que encontraste?\n","\n","# 3.4 Visualización\n","# - Nube de palabras con términos más importantes\n","# - Gráfico de barras con términos más frecuentes\n","# - Heatmap de similitud entre documentos (si no son demasiados)\n","\n","# 3.5 Interpretación de resultados\n","# - ¿Qué patrones encontrás en tu corpus?\n","# - ¿Los documentos similares realmente parecen similares?\n","# - ¿Hay agrupaciones naturales en tus datos?\n","# - ¿Qué limitaciones ves en este enfoque?\n","#Sección 4: Análisis con Word Embeddings (25% de la nota)\n","#import spacy\n","# Cargar modelo en español\n","#nlp = spacy.load(\"es_core_news_md\")\n","\n","# 4.1 Aplicación de embeddings\n","# - Procesar tu corpus con spaCy\n","# - Obtener vectores para documentos (promedio de vectores de palabras)\n","# - Explicar qué son los embeddings y por qué son diferentes a BoW\n","\n","# 4.2 Análisis de similitud semántica\n","# - Calcular similitud entre documentos usando embeddings\n","# - Comparar con resultados de TF-IDF\n","# - ¿Qué documentos son más similares según embeddings?\n","# - ¿Coinciden los resultados con TF-IDF?\n","\n","# 4.3 Búsqueda de analogías relevantes al corpus\n","# - Encontrar palabras más similares a términos clave de tu dominio\n","# - Intentar crear 2-3 analogías que funcionen con vocabulario de tu corpus\n","# - Ejemplo: \"rock es a guitarra como tango es a ?\"\n","# - Interpretar: ¿las analogías tienen sentido?\n","\n","# 4.4 Comparación con resultados de BoW\n","# - Crear tabla comparativa de documentos más similares\n","# - ¿Qué método da resultados más intuitivos?\n","# - ¿En qué casos embeddings es claramente superior?\n","# - ¿En qué casos BoW podría ser suficiente?\n","\n","# 4.5 Visualización de embeddings (si es posible)\n","# - Intentar reducir dimensionalidad (PCA o t-SNE)\n","# - Graficar documentos en 2D\n","# - ¿Se ven agrupaciones naturales?\n","# (Esta parte es opcional si resulta muy compleja)\n","\n","#Sección 5: Análisis Complementario (10% de la nota)\n","#Elegí UNA de estas tres opciones según lo que hayan visto en clase:\n","\n","#Opción A: POS Tagging y Análisis Gramatical\n","# - Analizar distribución de tipos de palabras (sustantivos, verbos, adjetivos)\n","# - ¿Hay diferencias gramaticales entre subcategorías de tu corpus?\n","# - Interpretación estilística de los patrones encontrados\n","#Opción B: Análisis de Sentimientos\n","# - Aplicar análisis de sentimientos a tus textos\n","# - ¿Cuáles son los más positivos/negativos?\n","# - ¿Hay patrones de sentimiento por categoría/autor/época?\n","#Opción C: Extracción Básica de Entidades\n","# - Usar spaCy para extraer entidades nombradas (si aplicable a tu corpus)\n","# - ¿Qué personas, lugares, organizaciones se mencionan más?\n","# - ¿Hay diferencias entre subcategorías de tu corpus?\n","\n","#Sección 6: Conclusiones y Reflexiones (5% de la nota)\n","# 6.1 Hallazgos principales sobre el corpus\n","# - ¿Qué descubriste sobre tu corpus que no sabías antes?\n","# - ¿Se confirmaron tus hipótesis iniciales?\n","# - ¿Qué te sorprendió más?\n","\n","# 6.2 Comparación de métodos utilizados\n","# - ¿Qué técnica te pareció más útil para tu tipo de corpus?\n","# - ¿Cuándo usarías BoW/TF-IDF vs embeddings?\n","# - ¿Qué ventajas y desventajas encontraste en cada método?\n","\n","# 6.3 Limitaciones encontradas\n","# - ¿Qué no pudiste capturar con las técnicas usadas?\n","# - ¿Qué aspectos importantes de tu corpus quedan sin analizar?\n","# - ¿Qué mejorarías si tuvieras más tiempo/recursos?\n","\n","# 6.4 Aplicaciones potenciales del análisis\n","# - ¿Cómo se podría usar este análisis en un contexto real?\n","# - ¿Qué valor agregado proporciona?\n","# - ¿Qué otros análisis te gustaría hacer en el futuro?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11055,"status":"ok","timestamp":1757683630053,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"},"user_tz":180},"id":"DH0E00z4G2oQ","outputId":"96e14290-9152-44ec-f822-eda2ef59bc18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from wordcloud) (2.0.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"]}],"source":["!pip install spacy\n","!python -m spacy download es_core_news_md\n","!pip install wordcloud"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25884,"status":"ok","timestamp":1758460767830,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"},"user_tz":180},"id":"Ifq_pzKFEj8l","outputId":"a13e8216-0826-4490-eff6-846de218299c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}],"source":["\n","# =============================================================================\n","# 2. CONFIGURACIONES GENERALES\n","# =============================================================================\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# 3. LIBRERÍAS ESTÁNDAR DE PYTHON\n","# =============================================================================\n","import os\n","import re\n","import time\n","import string\n","import pickle\n","import random\n","import unicodedata\n","from collections import Counter\n","from datetime import datetime\n","from itertools import combinations\n","\n","# =============================================================================\n","# 4. LIBRERÍAS CIENTÍFICAS FUNDAMENTALES\n","# =============================================================================\n","import numpy as np\n","import pandas as pd\n","\n","# =============================================================================\n","# 5. VISUALIZACIÓN\n","# =============================================================================\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","\n","# =============================================================================\n","# 6. PROCESAMIENTO DE LENGUAJE NATURAL - NLTK\n","# =============================================================================\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.stem import SnowballStemmer\n","\n","# Descargas de NLTK (ejecutar solo una vez)\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","# =============================================================================\n","# 7. PROCESAMIENTO DE LENGUAJE NATURAL - spaCy\n","# =============================================================================\n","import spacy\n","\n","# =============================================================================\n","# 8. MACHINE LEARNING - SCIKIT-LEARN\n","# =============================================================================\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.cluster import KMeans"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1682,"status":"ok","timestamp":1758460769505,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"},"user_tz":180},"id":"PVw_fHsjHo62","outputId":"b77dcab7-561a-47c3-b36c-dd29527ef9ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata loaded: 32 entries\n","                         archivo                  titulo autor_fuente  \\\n","0  01_cuando_pase_el_temblor.txt  Cuando pase el temblor  Soda Stereo   \n","1          02_imagenes_retro.txt          Imágenes retro  Soda Stereo   \n","2           03_estoy_azulado.txt           Estoy azulado  Soda Stereo   \n","3                04_profugos.txt                Prófugos  Soda Stereo   \n","4      05_persiana_americana.txt      Persiana americana  Soda Stereo   \n","\n","        fecha categoria          album  palabras_aprox  \n","0  1985-01-01      rock  Nada Personal             144  \n","1  1985-01-01      rock  Nada Personal             119  \n","2  1985-01-01      rock  Nada Personal             117  \n","3  1986-01-01      rock         Signos             101  \n","4  1986-01-01      rock         Signos             121  \n"]}],"source":["# Quick test to see if your files are accessible\n","\n","base_dir = \"/content/drive/MyDrive/Programación de lenguaje/Trabajo Integrador PNL/mi_corpus\"\n","metadata_file = os.path.join(base_dir, \"metadata.csv\")\n","\n","if os.path.exists(metadata_file):\n","    df = pd.read_csv(metadata_file)\n","    print(f\"Metadata loaded: {len(df)} entries\")\n","    print(df.head())\n","else:\n","    print(\"Metadata file not found\")"]},{"cell_type":"code","source":["import os\n","import re\n","import unicodedata\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Importaciones con manejo de errores\n","try:\n","    from wordcloud import WordCloud\n","    WORDCLOUD_AVAILABLE = True\n","except ImportError:\n","    print(\"⚠️ WordCloud no disponible - se omitirán las nubes de palabras\")\n","    WORDCLOUD_AVAILABLE = False\n","\n","# NLP Libraries\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import SnowballStemmer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","try:\n","    import spacy\n","    SPACY_AVAILABLE = True\n","except ImportError:\n","    print(\"⚠️ spaCy no disponible - análisis de embeddings limitado\")\n","    SPACY_AVAILABLE = False\n","    spacy = None\n","\n","# Download NLTK data if needed\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","class OptimizedCorpusAnalyzer:\n","    \"\"\"Clase principal optimizada para análisis completo de corpus musical\"\"\"\n","\n","    def __init__(self, base_dir=\"/content/drive/MyDrive/Programación de lenguaje/Trabajo Integrador PNL/mi_corpus\"):\n","        self.base_dir = base_dir\n","        self.raw_texts_dir = os.path.join(base_dir, \"raw_texts\")\n","        self.metadata_file = os.path.join(base_dir, \"metadata.csv\")\n","        self.viz_dir = os.path.join(base_dir, 'visualizations')\n","\n","        # Initialize components\n","        self.texts = {}\n","        self.metadata_df = None\n","        self.tokenizer = TextTokenizer()\n","        self.nlp = None\n","\n","        # Processed data storage\n","        self.processed_texts = []\n","        self.bow_matrix = None\n","        self.tfidf_matrix = None\n","        self.doc_vectors = None\n","        self.similarities_bow = None\n","        self.similarities_tfidf = None\n","        self.similarities_embeddings = None\n","\n","        self.setup_directories()\n","        self.load_spacy_model()\n","\n","    def setup_directories(self):\n","        \"\"\"Crea directorios necesarios\"\"\"\n","        os.makedirs(self.viz_dir, exist_ok=True)\n","        print(f\"✅ Estructura verificada en: {self.base_dir}\")\n","\n","    def load_spacy_model(self):\n","        \"\"\"Carga modelo spaCy una sola vez\"\"\"\n","        if not SPACY_AVAILABLE:\n","            print(\"⚠️ spaCy no disponible. Funcionalidad de embeddings limitada.\")\n","            self.nlp = None\n","            return\n","\n","        try:\n","            self.nlp = spacy.load('es_core_news_md')\n","            print(\"✅ Modelo spaCy cargado exitosamente\")\n","        except OSError:\n","            print(\"⚠️ Modelo spaCy no encontrado. Instalar con: python -m spacy download es_core_news_md\")\n","            self.nlp = None\n","\n","    def load_corpus(self):\n","        \"\"\"Carga corpus y metadata\"\"\"\n","        if not os.path.exists(self.metadata_file):\n","            print(\"❌ Metadata no encontrado\")\n","            return False\n","\n","        self.metadata_df = pd.read_csv(self.metadata_file, encoding='utf-8')\n","\n","        for _, row in self.metadata_df.iterrows():\n","            filepath = os.path.join(self.raw_texts_dir, row['archivo'])\n","            if os.path.exists(filepath):\n","                try:\n","                    with open(filepath, 'r', encoding='utf-8') as f:\n","                        self.texts[row['archivo']] = f.read()\n","                except Exception as e:\n","                    print(f\"⚠️ Error leyendo {row['archivo']}: {e}\")\n","\n","        print(f\"📚 Corpus cargado: {len(self.texts)} documentos\")\n","        return True\n","\n","    def show_corpus_summary(self):\n","        \"\"\"Muestra resumen del corpus sin duplicar análisis\"\"\"\n","        print(\"\\n📋 RESUMEN DEL CORPUS\")\n","        print(\"=\" * 60)\n","\n","        if 'album' in self.metadata_df.columns:\n","            album_counts = self.metadata_df.groupby('album').size()\n","            print(\"📀 CANCIONES POR ÁLBUM:\")\n","            for album, count in album_counts.items():\n","                album_words = self.metadata_df[self.metadata_df['album'] == album]['palabras_aprox'].sum()\n","                print(f\"  • {album}: {count} canciones ({album_words:,} palabras)\")\n","\n","        print(f\"\\n📊 ESTADÍSTICAS GENERALES:\")\n","        print(f\"  • Total canciones: {len(self.metadata_df)}\")\n","        print(f\"  • Palabras totales: {self.metadata_df['palabras_aprox'].sum():,}\")\n","        print(f\"  • Promedio palabras/canción: {self.metadata_df['palabras_aprox'].mean():.0f}\")\n","\n","    def analyze_all_tokenization(self, apply_stemming=True):\n","        \"\"\"Analiza tokenización de TODAS las canciones\"\"\"\n","        print(f\"\\n🔤 ANÁLISIS COMPLETO DE TOKENIZACIÓN - TODAS LAS CANCIONES\")\n","        print(\"=\" * 80)\n","\n","        if apply_stemming:\n","            print(\"✅ Aplicando stemming para reducir variaciones de palabras\")\n","\n","        all_tokens_original = []\n","        all_tokens_processed = []\n","        total_sentences = 0\n","        song_results = []\n","\n","        print(f\"\\n📝 Procesando {len(self.texts)} canciones...\")\n","\n","        for i, (filename, text) in enumerate(self.texts.items(), 1):\n","            song_info = self.metadata_df[self.metadata_df['archivo'] == filename].iloc[0]\n","\n","            # Procesar canción\n","            result = self.tokenizer.process_text_complete(text, apply_stemming=apply_stemming)\n","\n","            # Almacenar para uso posterior\n","            if apply_stemming and result['stemmed_tokens']:\n","                processed_text = ' '.join(result['stemmed_tokens'])\n","                all_tokens_processed.extend(result['stemmed_tokens'])\n","            else:\n","                processed_text = ' '.join(result['word_tokens'])\n","                all_tokens_processed.extend(result['word_tokens'])\n","\n","            self.processed_texts.append(processed_text)\n","\n","            # Acumular estadísticas\n","            all_tokens_original.extend(result['word_tokens'])\n","            total_sentences += result['sentence_count']\n","\n","            # Guardar resultado individual\n","            song_results.append({\n","                'titulo': song_info['titulo'],\n","                'album': song_info['album'],\n","                'tokens_originales': len(result['word_tokens']),\n","                'tokens_procesados': len(result['stemmed_tokens']) if apply_stemming else len(result['word_tokens']),\n","                'oraciones': result['sentence_count']\n","            })\n","\n","            if i % 10 == 0:  # Progreso cada 10 canciones\n","                print(f\"  Procesadas: {i}/{len(self.texts)} canciones\")\n","\n","        # Estadísticas completas\n","        print(f\"\\n📊 ESTADÍSTICAS COMPLETAS DE TOKENIZACIÓN:\")\n","        print(\"=\" * 60)\n","        print(f\"📄 Total oraciones: {total_sentences:,}\")\n","        print(f\"🔤 Total tokens originales: {len(all_tokens_original):,}\")\n","        print(f\"🔤 Total tokens procesados: {len(all_tokens_processed):,}\")\n","        print(f\"📋 Vocabulario único original: {len(set(all_tokens_original)):,}\")\n","        print(f\"📋 Vocabulario único procesado: {len(set(all_tokens_processed)):,}\")\n","\n","        if apply_stemming:\n","            reduction = len(set(all_tokens_original)) - len(set(all_tokens_processed))\n","            reduction_pct = (reduction / len(set(all_tokens_original))) * 100\n","            print(f\"🌱 Reducción por stemming: {reduction} palabras ({reduction_pct:.1f}%)\")\n","\n","        # Top palabras más frecuentes\n","        print(f\"\\n🏆 TOP 20 PALABRAS MÁS FRECUENTES:\")\n","        print(\"-\" * 50)\n","\n","        token_freq = Counter(all_tokens_processed)\n","        for i, (token, count) in enumerate(token_freq.most_common(20), 1):\n","            percentage = (count / len(all_tokens_processed)) * 100\n","            print(f\"   {i:2d}. '{token}': {count} veces ({percentage:.1f}%)\")\n","\n","        # Estadísticas por álbum\n","        df_results = pd.DataFrame(song_results)\n","        album_stats = df_results.groupby('album').agg({\n","            'tokens_procesados': ['sum', 'mean'],\n","            'oraciones': ['sum', 'mean']\n","        }).round(0)\n","\n","        print(f\"\\n📀 ESTADÍSTICAS POR ÁLBUM:\")\n","        print(\"-\" * 50)\n","        for album in album_stats.index:\n","            tokens_sum = int(album_stats.loc[album, ('tokens_procesados', 'sum')])\n","            tokens_mean = int(album_stats.loc[album, ('tokens_procesados', 'mean')])\n","            print(f\"  {album}: {tokens_sum:,} tokens ({tokens_mean} promedio/canción)\")\n","\n","        return {\n","            'all_tokens_processed': all_tokens_processed,\n","            'total_sentences': total_sentences,\n","            'song_results': song_results,\n","            'vocabulary_size': len(set(all_tokens_processed))\n","        }\n","\n","    def create_bow_and_tfidf_matrices(self, max_features=500, min_df=2):\n","        \"\"\"Crea matrices BoW y TF-IDF en una sola operación\"\"\"\n","        print(f\"\\n📊 CREANDO MATRICES BOW Y TF-IDF\")\n","        print(\"-\" * 50)\n","\n","        if not self.processed_texts:\n","            print(\"❌ Textos no procesados. Ejecutando tokenización primero.\")\n","            self.analyze_all_tokenization()\n","\n","        # BoW Matrix\n","        bow_vectorizer = CountVectorizer(\n","            max_features=max_features,\n","            min_df=min_df,\n","            token_pattern=r'\\b\\w{3,}\\b'\n","        )\n","        self.bow_matrix = bow_vectorizer.fit_transform(self.processed_texts)\n","        bow_features = bow_vectorizer.get_feature_names_out()\n","\n","        # TF-IDF Matrix\n","        tfidf_vectorizer = TfidfVectorizer(\n","            max_features=max_features,\n","            min_df=min_df,\n","            token_pattern=r'\\b\\w{3,}\\b'\n","        )\n","        self.tfidf_matrix = tfidf_vectorizer.fit_transform(self.processed_texts)\n","        tfidf_features = tfidf_vectorizer.get_feature_names_out()\n","\n","        print(f\"✅ Matriz BoW: {self.bow_matrix.shape}\")\n","        print(f\"✅ Matriz TF-IDF: {self.tfidf_matrix.shape}\")\n","        print(f\"📋 Vocabulario: {len(bow_features)} términos\")\n","\n","        return bow_features, tfidf_features\n","\n","    def analyze_similarities(self):\n","        \"\"\"Calcula todas las similitudes en una operación\"\"\"\n","        print(f\"\\n🔍 CALCULANDO SIMILITUDES ENTRE DOCUMENTOS\")\n","        print(\"-\" * 50)\n","\n","        if self.bow_matrix is None or self.tfidf_matrix is None:\n","            print(\"❌ Matrices no creadas. Creando primero...\")\n","            self.create_bow_and_tfidf_matrices()\n","\n","        # Similitudes BoW y TF-IDF\n","        self.similarities_bow = cosine_similarity(self.bow_matrix)\n","        self.similarities_tfidf = cosine_similarity(self.tfidf_matrix)\n","\n","        print(f\"✅ Similitudes BoW calculadas: {self.similarities_bow.shape}\")\n","        print(f\"✅ Similitudes TF-IDF calculadas: {self.similarities_tfidf.shape}\")\n","\n","        # Similitudes con embeddings si está disponible\n","        if self.nlp and self.doc_vectors is not None:\n","            self.similarities_embeddings = cosine_similarity(self.doc_vectors)\n","            print(f\"✅ Similitudes Embeddings calculadas: {self.similarities_embeddings.shape}\")\n","\n","    def process_embeddings(self):\n","        \"\"\"Procesa embeddings de todos los documentos\"\"\"\n","        print(f\"\\n🧠 PROCESANDO EMBEDDINGS\")\n","        print(\"-\" * 50)\n","\n","        if not self.nlp:\n","            print(\"❌ Modelo spaCy no disponible\")\n","            return False\n","\n","        doc_vectors = []\n","        valid_docs = []\n","\n","        for i, (filename, text) in enumerate(self.texts.items()):\n","            doc = self.nlp(text)\n","\n","            # Extraer vectores válidos\n","            valid_vectors = []\n","            for token in doc:\n","                if (not token.is_stop and\n","                    not token.is_punct and\n","                    not token.is_space and\n","                    len(token.text) > 2 and\n","                    token.has_vector):\n","                    valid_vectors.append(token.vector)\n","\n","            if valid_vectors:\n","                doc_vector = np.mean(valid_vectors, axis=0)\n","                doc_vectors.append(doc_vector)\n","                valid_docs.append(filename)\n","\n","        self.doc_vectors = np.array(doc_vectors)\n","        print(f\"✅ {len(self.doc_vectors)} documentos convertidos a embeddings\")\n","        return True\n","\n","    def find_similar_documents(self, method='tfidf', n_pairs=5):\n","        \"\"\"Encuentra documentos similares usando el método especificado\"\"\"\n","        print(f\"\\n🎯 DOCUMENTOS MÁS SIMILARES ({method.upper()})\")\n","        print(\"-\" * 50)\n","\n","        if method == 'tfidf':\n","            similarity_matrix = self.similarities_tfidf\n","        elif method == 'bow':\n","            similarity_matrix = self.similarities_bow\n","        elif method == 'embeddings':\n","            similarity_matrix = self.similarities_embeddings\n","        else:\n","            print(\"❌ Método no válido\")\n","            return\n","\n","        if similarity_matrix is None:\n","            print(\"❌ Matriz de similitudes no calculada\")\n","            return\n","\n","        # Encontrar pares más similares\n","        sim_copy = similarity_matrix.copy()\n","        np.fill_diagonal(sim_copy, 0)\n","\n","        flat_indices = np.argsort(sim_copy.flatten())[::-1]\n","        found_pairs = set()\n","\n","        print(f\"🏆 TOP {n_pairs} PARES MÁS SIMILARES:\")\n","\n","        for flat_idx in flat_indices:\n","            row_idx, col_idx = np.unravel_index(flat_idx, sim_copy.shape)\n","\n","            pair = tuple(sorted([row_idx, col_idx]))\n","            if pair in found_pairs or row_idx == col_idx:\n","                continue\n","\n","            found_pairs.add(pair)\n","            similarity_score = similarity_matrix[row_idx, col_idx]\n","\n","            song1 = self.metadata_df.iloc[row_idx]['titulo']\n","            song2 = self.metadata_df.iloc[col_idx]['titulo']\n","            album1 = self.metadata_df.iloc[row_idx]['album']\n","            album2 = self.metadata_df.iloc[col_idx]['album']\n","\n","            print(f\"  {len(found_pairs)}. '{song1}' ({album1})\")\n","            print(f\"     ↔ '{song2}' ({album2})\")\n","            print(f\"     📊 Similitud: {similarity_score:.4f}\")\n","            print()\n","\n","            if len(found_pairs) >= n_pairs:\n","                break\n","\n","        return found_pairs\n","\n","    def create_comprehensive_visualizations(self):\n","        \"\"\"Crea todas las visualizaciones en una operación\"\"\"\n","        print(f\"\\n📊 CREANDO VISUALIZACIONES COMPLETAS\")\n","        print(\"-\" * 50)\n","\n","        # 1. Términos más frecuentes\n","        self._create_term_frequency_plot()\n","\n","        # 2. Nube de palabras por álbum\n","        self._create_wordclouds_by_album()\n","\n","        # 3. Matriz de similitud\n","        if self.similarities_tfidf is not None and len(self.texts) <= 30:\n","            self._create_similarity_heatmap()\n","\n","        # 4. Visualización de embeddings 2D\n","        if self.doc_vectors is not None:\n","            self._create_embeddings_2d_plot()\n","\n","        print(f\"💾 Todas las visualizaciones guardadas en: {self.viz_dir}\")\n","\n","    def _create_term_frequency_plot(self):\n","        \"\"\"Gráfico de términos más frecuentes\"\"\"\n","        if self.bow_matrix is None or self.tfidf_matrix is None:\n","            return\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n","\n","        # BoW términos frecuentes\n","        term_frequencies = np.array(self.bow_matrix.sum(axis=0)).flatten()\n","        bow_vectorizer = CountVectorizer(max_features=500, min_df=2, token_pattern=r'\\b\\w{3,}\\b')\n","        bow_vectorizer.fit_transform(self.processed_texts)\n","        feature_names = bow_vectorizer.get_feature_names_out()\n","\n","        top_indices = np.argsort(term_frequencies)[::-1][:15]\n","        terms = [feature_names[i] for i in top_indices]\n","        freqs = [term_frequencies[i] for i in top_indices]\n","\n","        ax1.barh(range(len(terms)), freqs, color='skyblue')\n","        ax1.set_yticks(range(len(terms)))\n","        ax1.set_yticklabels(terms)\n","        ax1.set_title('Top 15 Términos Más Frecuentes (BoW)')\n","\n","        # TF-IDF términos distintivos\n","        tfidf_scores = np.array(self.tfidf_matrix.sum(axis=0)).flatten()\n","        tfidf_vectorizer = TfidfVectorizer(max_features=500, min_df=2, token_pattern=r'\\b\\w{3,}\\b')\n","        tfidf_vectorizer.fit_transform(self.processed_texts)\n","        tfidf_features = tfidf_vectorizer.get_feature_names_out()\n","\n","        top_tfidf_indices = np.argsort(tfidf_scores)[::-1][:15]\n","        tfidf_terms = [tfidf_features[i] for i in top_tfidf_indices]\n","        tfidf_vals = [tfidf_scores[i] for i in top_tfidf_indices]\n","\n","        ax2.barh(range(len(tfidf_terms)), tfidf_vals, color='lightcoral')\n","        ax2.set_yticks(range(len(tfidf_terms)))\n","        ax2.set_yticklabels(tfidf_terms)\n","        ax2.set_title('Top 15 Términos TF-IDF')\n","\n","        plt.tight_layout()\n","        freq_path = os.path.join(self.viz_dir, 'frecuencias_terminos.png')\n","        plt.savefig(freq_path, dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","    def _create_wordclouds_by_album(self):\n","        \"\"\"Nubes de palabras por álbum\"\"\"\n","        if not WORDCLOUD_AVAILABLE:\n","            print(\"⚠️ WordCloud no disponible - saltando nubes de palabras\")\n","            return\n","\n","        try:\n","            albums = self.metadata_df['album'].unique()\n","\n","            cols = 2 if len(albums) > 2 else len(albums)\n","            rows = (len(albums) + cols - 1) // cols\n","\n","            fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n","            if len(albums) == 1:\n","                axes = [axes]\n","            elif rows == 1:\n","                axes = axes if len(albums) > 1 else [axes]\n","            else:\n","                axes = axes.flatten()\n","\n","            stop_words_es = {\n","                'que', 'de', 'la', 'el', 'en', 'y', 'a', 'un', 'es', 'se', 'no', 'te',\n","                'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'las', 'del',\n","                'una', 'me', 'al', 'como', 'mi', 'tu', 'si', 'los', 'pero', 'sus'\n","            }\n","\n","            for i, album in enumerate(albums):\n","                album_songs = self.metadata_df[self.metadata_df['album'] == album]\n","                texto_album = \"\"\n","\n","                for _, song in album_songs.iterrows():\n","                    if song['archivo'] in self.texts:\n","                        texto_album += \" \" + self.texts[song['archivo']]\n","\n","                if texto_album.strip():\n","                    wordcloud = WordCloud(\n","                        width=800, height=400,\n","                        background_color='white',\n","                        stopwords=stop_words_es,\n","                        max_words=100\n","                    ).generate(texto_album.lower())\n","\n","                    ax = axes[i] if len(albums) > 1 else axes[0]\n","                    ax.imshow(wordcloud, interpolation='bilinear')\n","                    ax.set_title(f'{album}')\n","                    ax.axis('off')\n","\n","            # Ocultar subplots vacíos\n","            for j in range(i + 1, len(axes)):\n","                axes[j].axis('off')\n","\n","            plt.tight_layout()\n","            wordcloud_path = os.path.join(self.viz_dir, 'wordclouds_por_album.png')\n","            plt.savefig(wordcloud_path, dpi=300, bbox_inches='tight')\n","            plt.show()\n","\n","        except Exception as e:\n","            print(f\"❌ Error creando nubes de palabras: {e}\")\n","\n","    def _create_similarity_heatmap(self):\n","        \"\"\"Heatmap de similitud entre documentos\"\"\"\n","        plt.figure(figsize=(12, 10))\n","\n","        song_names = [self.metadata_df.iloc[i]['titulo'][:20]\n","                     for i in range(len(self.texts))]\n","\n","        sns.heatmap(self.similarities_tfidf,\n","                   xticklabels=song_names,\n","                   yticklabels=song_names,\n","                   cmap='viridis', fmt='.2f')\n","\n","        plt.title('Matriz de Similitud (TF-IDF)')\n","        plt.xticks(rotation=45, ha='right')\n","        plt.tight_layout()\n","\n","        heatmap_path = os.path.join(self.viz_dir, 'similitud_documentos.png')\n","        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","    def _create_embeddings_2d_plot(self):\n","        \"\"\"Visualización 2D de embeddings\"\"\"\n","        if len(self.doc_vectors) < 4:\n","            return\n","\n","        # PCA y t-SNE\n","        pca = PCA(n_components=2, random_state=42)\n","        vectors_2d_pca = pca.fit_transform(self.doc_vectors)\n","\n","        perplexity = min(5, len(self.doc_vectors) - 1)\n","        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n","        vectors_2d_tsne = tsne.fit_transform(self.doc_vectors)\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n","\n","        # Colores por álbum\n","        albums = [self.metadata_df.iloc[i]['album'] for i in range(len(self.doc_vectors))]\n","        unique_albums = list(set(albums))\n","        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_albums)))\n","        album_colors = {album: colors[i] for i, album in enumerate(unique_albums)}\n","\n","        # PCA plot\n","        for i, (x, y) in enumerate(vectors_2d_pca):\n","            album = albums[i]\n","            ax1.scatter(x, y, c=[album_colors[album]], s=50, alpha=0.7)\n","\n","        ax1.set_title('Embeddings - PCA')\n","        ax1.grid(True, alpha=0.3)\n","\n","        # t-SNE plot\n","        for i, (x, y) in enumerate(vectors_2d_tsne):\n","            album = albums[i]\n","            ax2.scatter(x, y, c=[album_colors[album]], s=50, alpha=0.7)\n","\n","        ax2.set_title('Embeddings - t-SNE')\n","        ax2.grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        embeddings_path = os.path.join(self.viz_dir, 'embeddings_2d.png')\n","        plt.savefig(embeddings_path, dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","    def run_complete_analysis(self):\n","        \"\"\"Ejecuta análisis completo optimizado\"\"\"\n","        print(\"🎵 ANÁLISIS COMPLETO NLP - CORPUS SODA STEREO\")\n","        print(\"=\" * 80)\n","\n","        # 1. Cargar corpus\n","        if not self.load_corpus():\n","            return None\n","\n","        # 2. Resumen inicial\n","        self.show_corpus_summary()\n","\n","        # 3. Tokenización completa de todas las canciones\n","        tokenization_results = self.analyze_all_tokenization(apply_stemming=True)\n","\n","        # 4. Crear matrices BoW/TF-IDF\n","        bow_features, tfidf_features = self.create_bow_and_tfidf_matrices()\n","\n","        # 5. Procesar embeddings\n","        if self.nlp:\n","            self.process_embeddings()\n","\n","        # 6. Calcular similitudes\n","        self.analyze_similarities()\n","\n","        # 7. Encontrar documentos similares\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"ANÁLISIS DE SIMILITUDES\")\n","        print(\"=\"*60)\n","\n","        self.find_similar_documents('tfidf', 5)\n","        if self.similarities_embeddings is not None:\n","            self.find_similar_documents('embeddings', 3)\n","\n","        # 8. Crear visualizaciones\n","        self.create_comprehensive_visualizations()\n","\n","        print(f\"\\n✅ ANÁLISIS COMPLETO FINALIZADO\")\n","        print(\"=\" * 80)\n","        print(\"📋 RESUMEN DE ANÁLISIS REALIZADOS:\")\n","        print(\"✅ 1. Resumen y estadísticas del corpus\")\n","        print(\"✅ 2. Tokenización y stemming de TODAS las canciones\")\n","        print(\"✅ 3. Matrices BoW/TF-IDF optimizadas\")\n","        print(\"✅ 4. Análisis con embeddings\")\n","        print(\"✅ 5. Cálculo de similitudes\")\n","        print(\"✅ 6. Visualizaciones completas\")\n","        print(f\"📁 Archivos en: {self.viz_dir}/\")\n","\n","        return {\n","            'tokenization_results': tokenization_results,\n","            'bow_features': bow_features,\n","            'tfidf_features': tfidf_features,\n","            'similarities_tfidf': self.similarities_tfidf,\n","            'similarities_embeddings': self.similarities_embeddings,\n","            'doc_vectors': self.doc_vectors\n","        }\n","\n","\n","class TextTokenizer:\n","    \"\"\"Clase para tokenización y normalización optimizada\"\"\"\n","\n","    def __init__(self, language='spanish'):\n","        self.language = language\n","        self.stemmer = SnowballStemmer(language)\n","\n","        self.custom_stopwords = {\n","            'que', 'de', 'la', 'el', 'en', 'y', 'a', 'un', 'es', 'se', 'no', 'te',\n","            'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'las', 'del',\n","            'una', 'me', 'al', 'como', 'mi', 'tu', 'si', 'los', 'pero', 'sus',\n","            'ya', 'ni', 'muy', 'tan', 'más', 'vez', 'ser', 'sin', 'otro', 'otra',\n","            'cuando', 'donde', 'quien', 'cual', 'todo', 'todos', 'cada', 'hasta',\n","            'desde', 'entre', 'sobre', 'bajo', 'tras', 'durante', 'mediante',\n","            'esta', 'estos', 'este', 'estas', 'con', 'cabe', 'fue', 'oh', 'ah',\n","            'na', 'eh', 'mm', 'yeah', 'hey', 'wow'\n","        }\n","\n","    def clean_text(self, text):\n","        \"\"\"Limpia el texto básico\"\"\"\n","        text = text.lower()\n","        text = re.sub(r'[^\\w\\s\\náéíóúüñ]', ' ', text)\n","        text = re.sub(r'\\s+', ' ', text.strip())\n","        return text\n","\n","    def tokenize_words(self, text):\n","        \"\"\"Tokeniza texto en palabras individuales\"\"\"\n","        clean_text = self.clean_text(text)\n","\n","        try:\n","            tokens = word_tokenize(clean_text, language=self.language)\n","        except:\n","            tokens = clean_text.split()\n","\n","        valid_tokens = []\n","        for token in tokens:\n","            if len(token) >= 2 and not token.isdigit() and token not in self.custom_stopwords:\n","                valid_tokens.append(token)\n","\n","        return valid_tokens\n","\n","    def tokenize_sentences(self, text):\n","        \"\"\"Tokeniza texto en oraciones\"\"\"\n","        try:\n","            sentences = sent_tokenize(text, language=self.language)\n","        except:\n","            sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 10]\n","\n","        return [s.strip() for s in sentences if len(s.strip()) > 10]\n","\n","    def apply_stemming(self, tokens):\n","        \"\"\"Aplica stemming a lista de tokens\"\"\"\n","        stemmed_tokens = []\n","        for token in tokens:\n","            try:\n","                stemmed = self.stemmer.stem(token)\n","                stemmed_tokens.append(stemmed)\n","            except:\n","                stemmed_tokens.append(token)\n","\n","        return stemmed_tokens\n","\n","    def process_text_complete(self, text, apply_stemming=True):\n","        \"\"\"Procesa texto completo: tokenización + normalización\"\"\"\n","\n","        # 1. Tokenización por palabras\n","        word_tokens = self.tokenize_words(text)\n","\n","        # 2. Tokenización por oraciones\n","        sentence_tokens = self.tokenize_sentences(text)\n","\n","        # 3. Aplicar stemming si se solicita\n","        if apply_stemming:\n","            stemmed_tokens = self.apply_stemming(word_tokens)\n","        else:\n","            stemmed_tokens = word_tokens\n","\n","        return {\n","            'original_text': text[:200] + '...' if len(text) > 200 else text,\n","            'word_tokens': word_tokens,\n","            'sentence_tokens': sentence_tokens,\n","            'stemmed_tokens': stemmed_tokens if apply_stemming else None,\n","            'token_count': len(word_tokens),\n","            'sentence_count': len(sentence_tokens)\n","        }\n","\n","\n","# FUNCIÓN PRINCIPAL OPTIMIZADA\n","def ejecutar_analisis_optimizado(corpus_base_dir):\n","    \"\"\"\n","    Función principal optimizada que ejecuta todo el análisis sin duplicaciones\n","    \"\"\"\n","    print(\"🎵 INICIANDO ANÁLISIS NLP OPTIMIZADO\")\n","    print(\"=\" * 80)\n","\n","    # Crear analizador principal\n","    analyzer = OptimizedCorpusAnalyzer(base_dir=corpus_base_dir)\n","\n","    # Ejecutar análisis completo\n","    results = analyzer.run_complete_analysis()\n","\n","    if results:\n","        print(\"\\n🎯 RESULTADOS DISPONIBLES:\")\n","        print(f\"  • Vocabulario procesado: {results['tokenization_results']['vocabulary_size']:,} palabras únicas\")\n","        print(f\"  • Características BoW: {len(results['bow_features'])}\")\n","        print(f\"  • Características TF-IDF: {len(results['tfidf_features'])}\")\n","        if results['doc_vectors'] is not None:\n","            print(f\"  • Embeddings: {results['doc_vectors'].shape[0]} documentos x {results['doc_vectors'].shape[1]} dimensiones\")\n","\n","    return analyzer, results\n","\n","\n","# EJECUCIÓN AUTOMÁTICA\n","if __name__ == \"__main__\":\n","    corpus_path = \"/content/drive/MyDrive/Programación de lenguaje/Trabajo Integrador PNL/mi_corpus\"\n","\n","    print(\"🚀 Ejecutando análisis completo automático...\")\n","    analyzer, results = ejecutar_analisis_optimizado(corpus_path)\n","\n","    print(\"\\n✅ ¡ANÁLISIS COMPLETADO!\")\n","    print(\"📁 Revisa la carpeta 'visualizations' para ver los gráficos generados\")\n","\n","\n","# Para ejecutar solo el análisis (sin duplicar operaciones)\n","corpus_path = \"/content/drive/MyDrive/Programación de lenguaje/Trabajo Integrador PNL/mi_corpus\"\n","analyzer, results = ejecutar_analisis_optimizado(corpus_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1R8jLDD8tgBPcfeCq7Z5L24icXPJXNp6Q"},"id":"62alnjfzsHLA","executionInfo":{"status":"ok","timestamp":1758460810657,"user_tz":180,"elapsed":34877,"user":{"displayName":"CABRERA VANESA","userId":"13269838335200626080"}},"outputId":"75822636-34d7-4a0d-dc9b-a58f41af1265"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"QPAmkJ8ybAXG"},"source":["Conclusión: La hipotesis planteada afirma que las letras de las canciones emergen de una interacción dinámica y compleja entre el contexto histórico-social y la experiencia individual del compositor, donde los acontecimientos históricos, las tensiones sociales, los movimientos culturales y las condiciones socioeconómicas de cada época proporcionan tanto el marco temático como los recursos expresivos que influencian significativamente el contenido, el tono y los mensajes de las composiciones musicales, la intención artística y la posición social del creador.\n","Si bien, luego de haberse procesado las 32 canciones de Soda Stereo conformando un corpus de 3713 palabras, de acuerdo al análisis de la frecuencia de las 20 palabras que más aparecen y evaluando las nubes de palabras, la hipótesis debería ser refutada. No obstante, si se profundiza en las letras y las palabras que aparecen en las canciones, se encuentran conceptos que sí tienen una relación con el contexto y que el autor pudo haber sido condicionado por este a la hora de pensar en la letra. En la canción Cuando pase el temblor (1985), aparecen frases como: \"a veces siento temor\", \"un planeta con desilución\", \"sé que te encontraré en esas ruinas\"; recordando el contexto, fines de la dictadura militar, de la guerra de las malvinas, son palabras que tienen gran relación con el contexto imperante.\n","Se podría decir que fin y final, son las palabras de las 20 palabras de mayor frecuencia que tendrían una relación con el contexto histórico, el decaimiento de la dictadura, y aparición de la democracia. Otras palabras que aparecen, pero no con alta frecuencia, son prófugos, misil, furia, sangra, delator. Profundizando, alguna de estas palabras, como ser misil, sangra, ciudad, furia aparecen en la nube de palabras, empero su tamaño es muy pequeño ya que su frecuencia de aparición es baja. Considero que el estribillo, al repetirse en las letras puede generar un sesgo importante a tener en cuenta a la hora de trabajar con letras de canciones."]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1mN40aw0MjbFdinvMtd1CJfvfiMZrdzY_","authorship_tag":"ABX9TyMLz++rtPkplTkXEXgE6mIp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}