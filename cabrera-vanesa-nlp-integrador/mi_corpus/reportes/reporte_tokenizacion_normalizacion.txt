================================================================================
REPORTE ACADÉMICO - TOKENIZACIÓN Y NORMALIZACIÓN
CORPUS SODA STEREO - Procesamiento de Lenguaje Natural
================================================================================

📋 RESUMEN EJECUTIVO
--------------------
• Corpus procesado: 37 canciones
• Total tokens procesados: 3,972
• Promedio tokens por canción: 107.4
• Reducción promedio stemming: 1.35%
• Reducción promedio lemmatización: 2.70%

🔧 METODOLOGÍA APLICADA
-------------------------

1️⃣ TOKENIZACIÓN
   🛠️ Herramienta: NLTK word_tokenize (español)
   📚 Justificación académica:
      • Manejo robusto de puntuación y espacios
      • Reconocimiento de límites de palabras en español
      • Estándar en procesamiento de corpus lingüísticos

   🔍 Filtros aplicados:
      • Longitud mínima: 2 caracteres
      • Solo tokens alfabéticos (excluye números)
      • Conversión a minúsculas para normalización

2️⃣ STEMMING
   🛠️ Herramienta: SnowballStemmer (español)
   📚 Justificación académica:
      • Algoritmo Porter adaptado para español
      • Manejo de morfología española (género, número, verbos)
      • Ampliamente validado en literatura NLP
      • Eficiente computacionalmente

3️⃣ LEMMATIZACIÓN
   🛠️ Implementación: Reglas morfológicas personalizadas
   📚 Justificación académica:
      • Adaptación específica para español
      • Reglas basadas en morfología española estándar
      • Preservación de formas canónicas
      • Validación de resultados para evitar sobre-reducción

📊 RESULTADOS DETALLADOS POR CANCIÓN
----------------------------------------

🎵 Un misil en mi placard (1984)
    📁 Archivo: 01_un_misil_en_mi_placard.txt
    📊 Tokens originales: 119 (40 únicos)
    🌱 Tokens stemmed: 119 (40 únicos)
    📚 Tokens lemmatized: 119 (39 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 2.5%

🎵 Te hacen falta vitaminas (1984)
    📁 Archivo: 02_te_hacen_falta_vitaminas.txt
    📊 Tokens originales: 128 (33 únicos)
    🌱 Tokens stemmed: 128 (33 únicos)
    📚 Tokens lemmatized: 128 (32 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 3.0%

🎵 Mi novia tiene biceps (1984)
    📁 Archivo: 03_mi_novia_tiene_biceps.txt
    📊 Tokens originales: 146 (68 únicos)
    🌱 Tokens stemmed: 146 (67 únicos)
    📚 Tokens lemmatized: 146 (66 únicos)
    📉 Reducción stemming: 1.5%
    📉 Reducción lemmatización: 2.9%

🎵 Trátame suavemente (1984)
    📁 Archivo: 04_trátame_suavemente.txt
    📊 Tokens originales: 126 (63 únicos)
    🌱 Tokens stemmed: 126 (63 únicos)
    📚 Tokens lemmatized: 126 (59 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 6.3%

🎵 Disco Eterno (1984)
    📁 Archivo: 05_disco_eterno.txt
    📊 Tokens originales: 99 (22 únicos)
    🌱 Tokens stemmed: 99 (22 únicos)
    📚 Tokens lemmatized: 99 (22 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Ni un segundo (1984)
    📁 Archivo: 06_ni_un_segundo.txt
    📊 Tokens originales: 102 (21 únicos)
    🌱 Tokens stemmed: 102 (21 únicos)
    📚 Tokens lemmatized: 102 (20 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 4.8%

🎵 Afrodisíacos (1984)
    📁 Archivo: 07_afrodisíacos.txt
    📊 Tokens originales: 85 (33 únicos)
    🌱 Tokens stemmed: 85 (32 únicos)
    📚 Tokens lemmatized: 85 (32 únicos)
    📉 Reducción stemming: 3.0%
    📉 Reducción lemmatización: 3.0%

🎵 Dietético (1984)
    📁 Archivo: 08_dietetico.txt
    📊 Tokens originales: 92 (29 únicos)
    🌱 Tokens stemmed: 92 (29 únicos)
    📚 Tokens lemmatized: 92 (28 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 3.4%

🎵 El tiempo es dinero (1984)
    📁 Archivo: 09_el_tiempo_es_dinero.txt
    📊 Tokens originales: 99 (32 únicos)
    🌱 Tokens stemmed: 99 (32 únicos)
    📚 Tokens lemmatized: 99 (32 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Por qué no puedo ser del jet set? (1984)
    📁 Archivo: 10_por_que_no_puedo_ser_del_jet_set.txt
    📊 Tokens originales: 158 (55 únicos)
    🌱 Tokens stemmed: 158 (55 únicos)
    📚 Tokens lemmatized: 158 (54 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 1.8%

🎵 Cuando pase el temblor (1985)
    📁 Archivo: 11_cuando_pase_el_temblor.txt
    📊 Tokens originales: 113 (47 únicos)
    🌱 Tokens stemmed: 113 (47 únicos)
    📚 Tokens lemmatized: 113 (46 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 2.1%

🎵 Nada personal (1985)
    📁 Archivo: 12_nada_personal.txt
    📊 Tokens originales: 81 (22 únicos)
    🌱 Tokens stemmed: 81 (22 únicos)
    📚 Tokens lemmatized: 81 (22 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Juegos de seducción (1985)
    📁 Archivo: 13_juegos_de_seduccion.txt
    📊 Tokens originales: 116 (57 únicos)
    🌱 Tokens stemmed: 116 (55 únicos)
    📚 Tokens lemmatized: 116 (54 únicos)
    📉 Reducción stemming: 3.5%
    📉 Reducción lemmatización: 5.3%

🎵 Imágenes retro (1985)
    📁 Archivo: 14_imágenes_retro.txt
    📊 Tokens originales: 115 (47 únicos)
    🌱 Tokens stemmed: 115 (46 únicos)
    📚 Tokens lemmatized: 115 (46 únicos)
    📉 Reducción stemming: 2.1%
    📉 Reducción lemmatización: 2.1%

🎵 Danza rota (1985)
    📁 Archivo: 15_danza_rota.txt
    📊 Tokens originales: 107 (31 únicos)
    🌱 Tokens stemmed: 107 (31 únicos)
    📚 Tokens lemmatized: 107 (31 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Estoy azulado (1985)
    📁 Archivo: 16_estoy_azulado.txt
    📊 Tokens originales: 77 (37 únicos)
    🌱 Tokens stemmed: 77 (37 únicos)
    📚 Tokens lemmatized: 77 (37 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Observándonos (1985)
    📁 Archivo: 17_observándonos.txt
    📊 Tokens originales: 95 (46 únicos)
    🌱 Tokens stemmed: 95 (45 únicos)
    📚 Tokens lemmatized: 95 (46 únicos)
    📉 Reducción stemming: 2.2%
    📉 Reducción lemmatización: 0.0%

🎵 Ecos (1985)
    📁 Archivo: 18_ecos.txt
    📊 Tokens originales: 100 (48 únicos)
    🌱 Tokens stemmed: 100 (48 únicos)
    📚 Tokens lemmatized: 100 (45 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 6.2%

🎵 Si no fuera por (1985)
    📁 Archivo: 19_si_no_fuera_por.txt
    📊 Tokens originales: 136 (45 únicos)
    🌱 Tokens stemmed: 136 (44 únicos)
    📚 Tokens lemmatized: 136 (43 únicos)
    📉 Reducción stemming: 2.2%
    📉 Reducción lemmatización: 4.4%

🎵 Sin sobresaltos (1986)
    📁 Archivo: 20_sin_sobresaltos.txt
    📊 Tokens originales: 101 (39 únicos)
    🌱 Tokens stemmed: 101 (39 únicos)
    📚 Tokens lemmatized: 101 (38 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 2.6%

🎵 En camino (1986)
    📁 Archivo: 21_en_camino.txt
    📊 Tokens originales: 117 (46 únicos)
    🌱 Tokens stemmed: 117 (46 únicos)
    📚 Tokens lemmatized: 117 (46 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Final caja negra (1986)
    📁 Archivo: 22_final_caja_negra.txt
    📊 Tokens originales: 92 (45 únicos)
    🌱 Tokens stemmed: 92 (43 únicos)
    📚 Tokens lemmatized: 92 (42 únicos)
    📉 Reducción stemming: 4.4%
    📉 Reducción lemmatización: 6.7%

🎵 Prófugos (1986)
    📁 Archivo: 23_profugos.txt
    📊 Tokens originales: 94 (53 únicos)
    🌱 Tokens stemmed: 94 (52 únicos)
    📚 Tokens lemmatized: 94 (49 únicos)
    📉 Reducción stemming: 1.9%
    📉 Reducción lemmatización: 7.5%

🎵 Persiana americana (1986)
    📁 Archivo: 24_persiana_americana.txt
    📊 Tokens originales: 94 (52 únicos)
    🌱 Tokens stemmed: 94 (50 únicos)
    📚 Tokens lemmatized: 94 (51 únicos)
    📉 Reducción stemming: 3.8%
    📉 Reducción lemmatización: 1.9%

🎵 Signos (1986)
    📁 Archivo: 25_signos.txt
    📊 Tokens originales: 97 (48 únicos)
    🌱 Tokens stemmed: 97 (47 únicos)
    📚 Tokens lemmatized: 97 (45 únicos)
    📉 Reducción stemming: 2.1%
    📉 Reducción lemmatización: 6.2%

🎵 El rito (1986)
    📁 Archivo: 26_el_rito.txt
    📊 Tokens originales: 113 (47 únicos)
    🌱 Tokens stemmed: 113 (45 únicos)
    📚 Tokens lemmatized: 113 (47 únicos)
    📉 Reducción stemming: 4.3%
    📉 Reducción lemmatización: 0.0%

🎵 No existes (1986)
    📁 Archivo: 27_no_existes.txt
    📊 Tokens originales: 97 (45 únicos)
    🌱 Tokens stemmed: 97 (42 únicos)
    📚 Tokens lemmatized: 97 (42 únicos)
    📉 Reducción stemming: 6.7%
    📉 Reducción lemmatización: 6.7%

🎵 Día común (1988)
    📁 Archivo: 28_día_común.txt
    📊 Tokens originales: 104 (52 únicos)
    🌱 Tokens stemmed: 104 (51 únicos)
    📚 Tokens lemmatized: 104 (52 únicos)
    📉 Reducción stemming: 1.9%
    📉 Reducción lemmatización: 0.0%

🎵 En la ciudad de la furia (1988)
    📁 Archivo: 29_en_la_ciudad_de_la_furia.txt
    📊 Tokens originales: 127 (26 únicos)
    🌱 Tokens stemmed: 127 (26 únicos)
    📚 Tokens lemmatized: 127 (25 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 3.8%

🎵 Lo que sangra (1988)
    📁 Archivo: 30_lo_que_sangra.txt
    📊 Tokens originales: 113 (53 únicos)
    🌱 Tokens stemmed: 113 (53 únicos)
    📚 Tokens lemmatized: 113 (53 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Corazón delator (1988)
    📁 Archivo: 31_corazon_delator.txt
    📊 Tokens originales: 96 (49 únicos)
    🌱 Tokens stemmed: 96 (48 únicos)
    📚 Tokens lemmatized: 96 (47 únicos)
    📉 Reducción stemming: 2.0%
    📉 Reducción lemmatización: 4.1%

🎵 Sobredosis de TV (1988)
    📁 Archivo: 32_sobredosis_de_tv.txt
    📊 Tokens originales: 98 (43 únicos)
    🌱 Tokens stemmed: 98 (42 únicos)
    📚 Tokens lemmatized: 98 (42 únicos)
    📉 Reducción stemming: 2.3%
    📉 Reducción lemmatización: 2.3%

🎵 En el borde (1988)
    📁 Archivo: 33_en_el_borde.txt
    📊 Tokens originales: 112 (46 únicos)
    🌱 Tokens stemmed: 112 (46 únicos)
    📚 Tokens lemmatized: 112 (46 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Picnic en el 4to B (1988)
    📁 Archivo: 34_picnic_en_el_4to_b.txt
    📊 Tokens originales: 99 (50 únicos)
    🌱 Tokens stemmed: 99 (49 únicos)
    📚 Tokens lemmatized: 99 (48 únicos)
    📉 Reducción stemming: 2.0%
    📉 Reducción lemmatización: 4.0%

🎵 El ritmo de tus ojos (1988)
    📁 Archivo: 35_el_ritmo_de_tus_ojos.txt
    📊 Tokens originales: 121 (41 únicos)
    🌱 Tokens stemmed: 121 (41 únicos)
    📚 Tokens lemmatized: 121 (41 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 0.0%

🎵 Terapia de amor intensiva (1988)
    📁 Archivo: 36_terapia_de_amor_intensiva.txt
    📊 Tokens originales: 122 (53 únicos)
    🌱 Tokens stemmed: 122 (51 únicos)
    📚 Tokens lemmatized: 122 (51 únicos)
    📉 Reducción stemming: 3.8%
    📉 Reducción lemmatización: 3.8%

🎵 Los languidecientes (1988)
    📁 Archivo: 37_los_languidecientes.txt
    📊 Tokens originales: 81 (46 únicos)
    🌱 Tokens stemmed: 81 (46 únicos)
    📚 Tokens lemmatized: 81 (45 únicos)
    📉 Reducción stemming: 0.0%
    📉 Reducción lemmatización: 2.2%

================================================================================
🎯 CONCLUSIONES ACADÉMICAS
================================================================================
✅ Tokenización robusta aplicada según estándares NLP
✅ Comparación sistemática entre stemming y lemmatización
✅ Adaptación metodológica específica para corpus musical español
✅ Documentación completa para reproducibilidad científica
✅ Fundamentación cuantitativa para selección de técnica
